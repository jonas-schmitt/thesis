\section{Multigrid Methods}
Multigrid methods have been designed to accelerate the convergence of iterative methods by eliminating certain error components on a so-called coarser representation of the original problem.
While the basic iterative methods discussed in the last section are applicable to many PDE-based problems, in practice, their speed of convergence is often insufficient, which means that a large number of iterations is required until an acceptable approximation accuracy can be attained. 
The main reason for this behavior is that these methods are only efficient in the reduction of certain error components, while others remain mostly unaffected~\cite{briggs2000multigrid}.
This can be best understood by considering the effect of a stationary iterative method on oscillatory errors of different frequency.
For this purpose, we consider the one-dimensional Laplace equation
\begin{equation}
		\begin{split}
			- \dv[2]{x} u(x) & = 0 \quad \forall x \in (0, 1) \\
			u(0) = u(1) & = 0,
		\end{split}
		\label{eq:1D-laplace-model}
\end{equation}
which is discretized using the three-point stencil
\begin{equation}
	\Delta_h = \frac{1}{h^2}\begin{bmatrix}
		-1 & 2 & -1
	\end{bmatrix}.
		\label{eq:1D-laplace-stencil}
\end{equation} 
Figure~\ref{fig:different-error-components} shows the impact of applying the Jacobi´and Gauss-Seidel method on different periodic error components.
\begin{figure}
	\centering
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/initial_error_jacobi_4pi.pdf}
		\includegraphics[width=\textwidth]{figures/initial_error_jacobi_16pi.pdf}
		\includegraphics[width=\textwidth]{figures/initial_error_jacobi_32pi.pdf}
		\caption{Initial error}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/final_error_jacobi_4pi.pdf}
		\includegraphics[width=\textwidth]{figures/final_error_jacobi_16pi.pdf}
		\includegraphics[width=\textwidth]{figures/final_error_jacobi_32pi.pdf}
	\caption{Error after applying Jacobi}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/final_error_gauss_seidel_4pi.pdf}
		\includegraphics[width=\textwidth]{figures/final_error_gauss_seidel_16pi.pdf}
		\includegraphics[width=\textwidth]{figures/final_error_gauss_seidel_32pi.pdf}
	\caption{Error after applying Gauss-Seidel}
	\end{subfigure}
	\caption{Different error components on a grid with step size $h = 2^{-9}$ before and after applying either 100 steps of the Jacobi or Gauss-Seidel method.}
	\label{fig:different-error-components}
\end{figure}
Here, the left column shows the initial error discretized on a grid with step size $h = 2^{-9}$ while the middle and right one includes the remaining error after applying 100 Jacobi and Gauss-Seidel steps, respectively.
Note that the frequency of change increases from top to bottom, whereas the amplitude of the error is always the same.
As it becomes apparent from investigating the middle and right columns of the first row of Figure~\ref{fig:different-error-components}, the Jacobi and Gauss-Seidel methods are both not able to achieve a significant reduction of those error components with a low frequency of change within 100 iterations.
In contrast, the third row, which represents a highly-oscillating component, the application of 100 steps of the Jacobi method already reduces the initial error to less than one-fifth of its original value.
The same behavior can also be observed for the Gauss-Seidel method, whereby, compared to the Jacobi method, high-frequency error components are reduced even faster.

We can further illustrate the error reduction properties of basic iterative methods by investigating Figure~\ref{fig:combined-error} which shows the combination of two error components with equal magnitude, one of them with low the other one with high frequency.
Again, the left plot shows the initial while the middle and right ones contain the reduced error after 100 iterations of Jacobi and Gauss-Seidel, respectively.
As it can be seen here, the attained improvement with both methods can be almost fully attributed to the reduction of the highly-oscillating component.
\begin{figure}
	\begin{subfigure}[b]{0.32\textwidth}
	\centering
		\includegraphics[width=\textwidth]{figures/initial_error_jacobi_combined.pdf}
		\caption{Initial error}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
	\centering
		\includegraphics[width=\textwidth]{figures/final_error_jacobi_combined.pdf}
		\caption{Error after applying Jacobi}
\end{subfigure}
\begin{subfigure}[b]{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figures/final_error_gauss_seidel_combined.pdf}
	\caption{Error after applying Gauss-Seidel}
\end{subfigure}
	\caption{Combination of two error components discretized on a grid with step size $h = 2^{-9}$ before and after applying 100 steps of either the Jacobi or Gauss-Seidel method.}
\label{fig:combined-error}
\end{figure}
Because the remaining error is more smooth than initially, basic iterative methods are often denoted as \emph{smoothers} and their effectiveness is measured in terms of their capability to reduce the high-frequency components of a given error. 

Now observe what happens if we represent the same low-frequency error component shown in the first row of Figure~\ref{fig:different-error-components} on a grid with larger step size $h = 2^{-6}$, hence a smaller number of grid points.
Because the number of (inner) grid points $n$ is inversely proportional to the step size with $n = 1/h - 1$, usually the term \emph{coarser} grid is used.
The resulting error reduction, again after 100 iterations of each of both iterative methods, is shown in Figure~\ref{fig:low-frequency-error-component-coarse}. 
\begin{figure}
	\begin{subfigure}[b]{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figures/initial_error_jacobi_4pi_coarse.pdf}
	\caption{Initial error}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.32\textwidth}
	\centering
	\includegraphics[width=\textwidth]{figures/final_error_jacobi_4pi_coarse.pdf}
	\caption{Error after applying Jacobi}
\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/final_error_gauss_seidel_4pi_coarse.pdf}
		\caption{Error after applying Gauss-Seidel}
	\end{subfigure}
	\caption{Low-frequency error component discretized on a coarse grid with step size $h = 2^{-6}$ before and after applying 100 steps of either the Jacobi or Gauss-Seidel method.}
	\label{fig:low-frequency-error-component-coarse}
\end{figure}
As it can be seen, the amount of low-frequency error reduction is significantly higher for both methods than on the \emph{finer} grid with a step size of $h = 2^{-9}$.
While basic iterative methods, like the Jacobi and Gauss-Seidel method are only capable of reducing the high-frequency components of a given error, we can, hence, employ the same methods to effectively reduce the remaining low-frequency components when they can be represented on a coarser grid.
\emph{Multigrid} methods build up on this idea by automating the process of recursively obtaining a coarser representation of a given problem, which can then be effectively reduced by employing only a few \emph{smoothing} iterations.
These methods, therefore, operate on a so-called hierarchy of grids, where on each subsequent discretization level certain error components are reduced, which is then used to correct the remaining error on the next-finer grid.
In the following, we first introduce the basic components of a multigrid method, such as the smoothing, restriction, prolongation and coarse grid correction operations.
Based on this definition, we then develop a formal language for the representation of multigrid solvers.
\subsection{Smoothing}
\label{subsec:smoothing}
One of the central elements of multigrid methods is the utilization of a smoothing procedure, that is able to quickly reduce the oscillatory components of a given error.
We have already shown that the Jacobi and Gauss-Seidel method behave in such a way for the considered one-dimensional model problem.
To further improve the smoothing property of a basic iterative methods, it is often beneficial to introduce an additional relaxation factor $\omega$ which yields the iteration 
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + \omega M^{-1}(\bm b - A \bm{x}^{(i)}).
	\label{eq:general-weighted-stationary-iterative-method}
\end{equation}
Again, a weighted Jacobi or Gauss-Seidel can then be obtained by replacing the matrix $M$ by the respective term.
\subsubsection{Red-Black Gauss-Seidel}
While the Gauss-Seidel method exhibits superior smoothing properties compared to the Jacobi method, each iteration requires solving a lower triangular system of the form
\begin{equation*}
	(D - L) (\bm{x}^{(k+1)} - \bm{x}^{(k)}) = \bm{b} - A \bm{x}^{(k)},
\end{equation*}
with $D - L$ as the lower triangular part of the system matrix $A$.
Since $U = A - (D - L)$, we can rewrite this equation to obtain
\begin{equation*}
	(D - L) \bm{x}^{(k+1)} = \bm{b} - U \bm{x}^{(k)}.
\end{equation*}  
which can then be solved using forward substitution by applying
\begin{equation}
	x_{i}^{(k+1)}={\frac {1}{a_{ii}}}\left(b_{i}-\sum _{j=1}^{i-1}a_{ij}x_{j}^{(k+1)}-\sum _{j=i+1}^{n}a_{ij}x_{j}^{(k)}\right),
	\label{eq:gauss-seidel-element-wise}
\end{equation}
where $x_{i}^{(k)}$ represents the $i$th element of the vector $\bm{x}^{(k)}$.
However, because $x_{i+1}^{(k)}$ depends on $x_{i}^{(k)}$ this computation can must be performed sequentially, which means that the individual components of $\bm{x}^{(k+1)}$ must be computed one after another. 
Modern compute architectures exhibit an ever increasing degree of parallelism and, therefore, rely on the fact that all computations can be performed fully in parallel.
For comparison, consider the Jacobi method as defined by 
\begin{equation*}
	\bm{x}^{(k+1)} = \bm{x}^{(k)} + D^{-1}(\bm b - A \bm{x}^{(k)}).
\end{equation*}
Similar to the Gauss-Seidel method, we can first rewrite this equation to
\begin{equation*}
	\bm{x}^{(k+1)} = D^{-1}(\bm b - (A - D)\bm{x}^{(k)}),
\end{equation*}
which then yields the following element-wise formulation of the Jacobi method:
\begin{equation}
x_{i}^{(k+1)}={\frac {1}{a_{ii}}}\left(b_{i}-\sum _{j\neq i}a_{ij}x_{j}^{(k)}\right).
	\label{eq:jacobi-element-wise}
\end{equation}
In contrast to Equation~\eqref{eq:gauss-seidel-element-wise} the computation of each subsequent element of the vector $\bm{x}^{(k+1)}$ does not depend on any of its previous ones.
Consequently, the computation of a new approximate solution $\bm{x}^{(k+1)}$ using the Jacobi method can be performed in parallel for each individual element of the vector.

One possibility to enable a parallel computation of the approximate solution $\bm{x}^{(k+1)}$ that is similar to the Gauss-Seidel method but does not rely on a sequential computation, is to partition the grid points into multiple subsets.
The computation of each subset is then performed in a Jacobi-like fashion, however, in case the values of grid points from other subsets are needed, the updated values are used.
A common variant of this approach is the red-black Gauss-Seidel method.
Here, the grid points are assigned to two distinct sets, where the first represents the red and the second one the black points.
We can define the red-black Gauss-Seidel method in the following way:
\begin{equation}
	\begin{split}
		\bm{x}^{(k+1/2)} & = \bm{x}^{(k)} + P_r D^{-1} (\bm{b} - A \bm{x}^{k}) \\
		\bm{x}^{(k+1)} & = \bm{x}^{(k+1/2)} + P_b D^{-1} (\bm{b} - A \bm{x}^{k+1/2})
	\end{split}
\end{equation}
The entries of the matrices $P_R$ and $P_B$ are then defined as
\begin{equation}
	P_{R,ij} = \begin{cases}
	1 & \text{if} \; i = j \; \text{and} \; i,j \in R \\
	0 & \text{otherwise},  
	\end{cases}
\end{equation}
\begin{equation}
	P_{B,ij} = \begin{cases}
		1 & \text{if} \; i = j \; \text{and} \; i,j \in B \\
		0 & \text{otherwise}  
	\end{cases}
\end{equation}
Here, $R$ and $B$ are the sets of grid indices that correspond to the red and black points, respectively.
Now consider again the three-point stencil defined in Equation~\eqref{eq:1D-laplace-stencil}.
If we define the Gauss-Seidel method with respect to this stencil we obtain
\begin{equation*}
	x_{i}^{(k+1)}=\frac {1}{2}\left(b_{i} + x_{i-1}^{(k+1)} + x_{i+1}^{(k)}\right).
\end{equation*}
Here the update of each individual grid point is exclusively based on its neighbors, whereas the left neighbor $x_{i-1}^{(k+1)}$ has already been updated in the previous step of the method.
We can achieve a similar behavior by assigning neighboring grid points to different partitions, for instance in case of a red-black partitioning by defining
\begin{equation}
		R = \{ \bm{i} : \bm{i} \in \mathbb{N}^d, \sum_{k=1}^d i_k \; \text{even} \}, \;
		B = \{ \bm{i} : \bm{i} \in \mathbb{N}^d, \sum_{k=1}^d i_k \; \text{odd} \}.
\end{equation}
In many cases, the resulting method has better smoothing properties than the Jacobi method without sacrificing much of its parallelism, since the computations on each partition can be performed in parallel~\cite{trottenberg2000multigrid}.
\subsubsection{Block Smoothing}
In Section~\ref{subsec:smoothing}, we have only discussed smoothers that operate in a pointwise manner.
For instance, in the Jacobi method, Equation~\eqref{eq:jacobi-element-wise} is computed for each individual grid point.
The idea of block smoothing is to reorder the original system in such a way, that the same operations can be defined on blocks of grid points.
As a consequence each scalar operation in the original pointwise method is then replaced by a matrix or vector operation, whose dimensionality corresponds to the chosen block size.

For example, we can rearrange our original system of the form of Equation~\eqref{eq:general-system-of-linear-equations} in the following way:
\begin{equation}
\underbrace{
\begin{pmatrix}A_{11}&A_{12}&\cdots &A_{1m}\\A_{21}&A_{22}&\cdots &A_{2m}\\\vdots &\vdots &\ddots &\vdots \\A_{m1}&A_{m2}&\cdots &A_{mm}\end{pmatrix}}_{A}
 = 
\underbrace{
\begin{pmatrix}
\bm{x}_1 \\ \bm{x}_2 \\ \vdots \\ \bm{x_m} 
\end{pmatrix}}_{\bm{x}} =
\underbrace{
\begin{pmatrix}
	\bm{b}_1 \\ \bm{b}_2 \\ \vdots \\ \bm{b_m} 
\end{pmatrix}}_{\bm{b}}
\end{equation}
with $m = n / n_b$ if $n_b$ is the size of each block.
A block Jacobi method can then be defined as
\begin{equation}
	\bm{x}_{i}^{(k+1)}=A_{ii}^{-1}\left(\bm{b}_{i}-\sum _{j\neq i}A_{ij}\bm{x}_{j}^{(k)}\right),
	\label{eq:jacobi-block-wise}
\end{equation}
For example we can define a block consisting of two elements for the one-dimensional Laplace equation and obtain the respective block Jacobi for each inner block with index $i$ by computing
\begin{equation}
	\begin{pmatrix}
		2 & -1 \\
		-1 & 2
	\end{pmatrix}
	\begin{pmatrix}
		x_{j}^{(k+1)} \\ x_{j+1}^{(k+1)} 
	\end{pmatrix}
= 	-  \begin{pmatrix}
	0 & 0 \\
	-1 & 0
\end{pmatrix} 	
\begin{pmatrix}
x_{j+2}^{(k)} \\ x_{j+3}^{(k)}
\end{pmatrix} -
\begin{pmatrix}
	0 & -1 \\
	0 & 0
\end{pmatrix} 	
\begin{pmatrix}
	x_{j-2}^{(k)} \\ x_{j-1}^{(k)} 
\end{pmatrix},
\end{equation}
where $j = n_b (i - 1) + 1$.
This method can be defined in a similar way using our previously introduced stencil notation
\begin{equation}
\begin{split}
	& \begin{pmatrix}
		\left[0 \quad 2 \quad -1 \right]_{h} \cdot x_{j}^{(k+1)} \\ \left[ -1 \quad 2 \quad 0 \right]_{h} \cdot x_{j+1}^{(k+1)} 
	\end{pmatrix}
	= \\ - & 
	\begin{pmatrix}
		\left[ 0 \right]_{h} \cdot x_{j+2}^{(k)} \\ \left[-1 \quad 0 \quad 0 \right]_{h} \cdot x_{j + 3}^{(k)}
	\end{pmatrix} -
	\begin{pmatrix}
		\left[0 \quad 0 \quad -1 \right]_{h} \cdot x_{j-2}^{(k)} \\ \left[ 0 \right]_{h} \cdot x_{j-1}^{(k)} 
	\end{pmatrix}.
\end{split}
\end{equation}
In both cases we obtain a system of two linear equations of the form
\begin{equation}
	\begin{pmatrix}
		2 x_{j}^{(k+1)} - x_{j+1}^{(k+1)} \\ 2 x_{j+1}^{(k+1)} - x_{j}^{(k+1)} 
	\end{pmatrix}
	=
	\begin{pmatrix}
		x_{j - 1}^{(k)} \\ x_{j + 2}^{(k)}
	\end{pmatrix}.
\end{equation}
that in each step of the method must be solved for each block, for instance, using a direct solver such as Gaussian elimination.
As in the pointwise Jacobi method, the system of each block can be solved independently and, therefore, operations on different blocks can be performed in parallel. 
Considering the fact that direct solvers in general require $\mathcal{O}(n_b^3)$ operations to solve each block, the overall computational cost of applying a block smoother can be estimated with $\mathcal{O}(n_b^3 \frac{n}{n_b}) = \mathcal{O}(n_b^2 n)$.
Note that choosing $n_b = n$ means that we treat the whole matrix $A$ as a single block and, therefore, solve the original system using Gaussian elimination.
In contrast, the choice of $n_b = 1$ corresponds to the pointwise Jacobi method, which, for a constant stencil, can be computed with a constant number of operations per grid point.
While we here only provide a brief introductions to block smoothers, it must be mentioned that it is also possible to define block variants for other smoothers, such as the Gauss-Seidel and red-black Gauss-Seidel method.
Finally, one can also define overlapping block smoothers needs, which means that multiple blocks contain the same grid point as an unknown to be solved.

\subsection{The coarse grid correction scheme}
The core idea behind multigrid methods is to reduce the oscillatory components of an error by obtaining an approximation of the same problem on a coarser grid.
As we have illustrated in Figure~\ref{fig:low-frequency-error-component-coarse} these components can then be reduced quickly using similar relaxation techniques that are effective to reduce oscillatory components on the finer grid.
Before defining this procedure, observe that our original system
\begin{equation}
	A_h \bm{x}_h = \bm{b}_h
\end{equation}
is equivalent to the error equation
\begin{equation}
	A_h \left(\bm{x}_h - \bm {x}^{(0)_h}\right) = \bm{b}_h - A_h \bm{x}^{(0)}_h
\end{equation}
with an arbitrary initial guess $\bm{x}^{(0)}_h$.
The subscripts $A_h$ and $\bm{x}_h$ indicate a discretization with step size $\bm{h}$.
Therefore, each entry of the vector $\bm{x}_h$ represents a grid function value $u_h(\bm{i} \cdot \bm{h})$ at the respective position within the grid.
We can further introduce the error $\bm{e}_h = \bm{x}_h - \bm {x}^{(0)}_h$ to obtain the equation
\begin{equation}
	A \bm{e} = \bm{b} - A \bm{x}^{(0)}.
	\label{eq:linear-system-error-equation}
\end{equation}
The solution $\bm{e}_h$ of this system, which depends on the initial solution $\bm{x}^{(0)}_h$, can then be employed to obtain the solution of the original system by computing
\begin{equation}
	\bm{x}_h = \bm{x}^{(0)}_h + \bm{e}_h.
\end{equation}
Consequently, each approximate solution computed for Equation~\eqref{eq:linear-system-error-equation} can be used to obtain an approximation for the solution of the original system of linear equations.
Now, assume there exists the prolongation and restriction operators, $I_h^{2h}$ and $I_{2h}^h$, that enable us to obtain an approximation for a given variable $\bm{x}_h$ on a coarser and finer grid, respectively, such that
\begin{equation}
	\bm{x}_{2h} \approx I_h^{2h} \bm{x}_{h}, \;
	\bm{x}_{h} \approx I_{2h}^{h} \bm{x}_{2h}. 
\end{equation}
In general, this approximation can obviously never be exact.
However, we have already observed that a certain error only consists of smooth components, we can approximate the same components on a coarser grid without a significant loss of accuracy.
This is illustrated in Figure~\ref{fig:error-on-multiple-levels}, where two error components with different frequency are discretized on a one-dimensional hierarchy of grids with decreasing steps size.
Here, the first error component, which possesses a higher frequency, can not be accurately represented on the coarsest grid with step size $h = 2^{-5}$.
In contrast, the characteristics of the second error component, which is relatively smooth, is still clearly visible on the same grid. 
\begin{figure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/initial_error_16pi_level7.pdf}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/initial_error_16pi_level6.pdf}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/initial_error_16pi_level5.pdf}
	\end{subfigure}
		\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/initial_error_2pi_level7.pdf}
		\caption{$h = 2^{-7}$}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/initial_error_2pi_level6.pdf}
		\caption{$h = 2^{-6}$}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/initial_error_2pi_level5.pdf}
		\caption{$h = 2^{-5}$}
	\end{subfigure}
	\caption{Oscillatory and smooth error components discretized on a hierarchy of grids with decreasing step size.}
	\label{fig:error-on-multiple-levels}
\end{figure}
Assuming that the error $\bm{e}_{2h}$ on the coarse grid consists exclusively of smooth components and, hence, $\bm{e}_{h} \approx I_{2h}^{h} \bm{e}_{2h}$, we can define the coarse grid correction 
\begin{equation}
	\bm{x}^{k+1}_h = \bm{x}^{k}_h + I_{2h}^h \bm{e}_{2h}.
\end{equation} 
Now the question remains to be answered how we can compute the error $\bm{e}_{2h}$ on the coarse grid.
As we have shown Equation~\eqref{eq:linear-system-error-equation} is equivalent to the original system.
Therefore, we can apply our previously defined inter-grid transfer operators to define the error equation on a coarser grid with step size $2\bm{h}$
\begin{equation}
	\underbrace{I_{h}^{2h} A_h I_{2h}^h}_{A_{2h}} \bm{e}_{2h} = I_{h}^{2h} \left(\bm{b}_h - A_h \bm{x}^{(0)}_h\right).
	\label{eq:coarse-grid-error-equation}
\end{equation}
Note that we have again made use of the fact that $\bm{e}_{h} \approx I_{2h}^{h} \bm{e}_{2h}$ is a reasonably accurate approximation in case $\bm{e}_{2h}$ is smooth.
Note that in Equation~\eqref{eq:coarse-grid-error-equation} the coarse operator $A_{2h}$ is directly obtained from the original operator $A_{2h}$. 
This process is usually called \emph{Galerkin coarsening}.
However, in cases where the operator $A_h$ can be directly discretized on a coarser grid with step size $2h$, it is often also possible to use the resulting operator $A_{2h}$ instead.
We can therefore derive a two-level-method which is summarized in Algorithm~\ref{alg:two-grid-method}.
\begin{algorithm}
	\caption{Two-Grid Method}
	\label{alg:two-grid-method}
	\begin{algorithmic}
		\State{Relax on $A_h \bm{x}_h = \bm{b}_h$ to obtain an approximation $\tilde{\bm{x}}_h$}
		\State{Compute the residual $\bm{r}_h = \bm{b}_h - A_h \tilde{\bm {x}}_h$}
		\State{\hskip1em Obtain $A_{2h}$ through Galerkin coarsening or rediscretization}
		\State{\hskip1em Restrict the residual $\bm{b}_{2h} = I_h^{2h} \bm{r}_h$}
		\State{\hskip1em Solve $A_{2h} \bm{x}_{2h} = \bm{b}_{2h}$ for $\bm{x}_{2h}$}
		\State{Perform the correction $\tilde{\bm{x}}_h = \tilde{\bm{x}}_h + I_{2h}^h \bm{x}_{2h}$}
		\State{Relax again $A_h \bm{x}_h = \bm{b}_h$ with an initial guess of $\tilde{\bm{x}}_h$}
	\end{algorithmic}
\end{algorithm}
Note that even though we are actually solving Equation~\eqref{eq:coarse-grid-error-equation} on the coarse grid, we have now redefined it as $A_{2h} \bm{x}_{2h} = \bm{b}_{2h}$.
Based on this notation, we could similarly define a two-level method starting from a grid with step size $2h$.
However, we have not covered the question which initial guess should then be used for the error equation on the coarser level.
For this purpose, note that one step of smoothing on Equation~\eqref{eq:coarse-grid-error-equation} with an initial guess of zero corresponds to
\begin{equation}
	\bm{e}_{2h}^{(1)} = M_{2h}^{-1} I_{h}^{2h} \left(\bm{b}_h - A_h \bm{x}^{(0)}_h\right).
	\label{eq:initial-coarse-grid-relaxation}
\end{equation}
Since the error $\bm{e}_{2h}^{(k+1)}$ in step $k+1$ is defined as
\begin{equation*}
	\bm{e}_{2h}^{(k+1)} = \bm{x}_{2h}^{(k+1)} - \bm{x}_{2h}^{(k)},
\end{equation*}
and assuming that $\bm{x}_{h}^{(0)}$ is smooth, we can choose an initial guess of $\bm{x}_{2h}^{(0)} = I_{2h}^{h} \bm{x}_{h}^{(0)}$ and, hence, obtain the iteration
\begin{equation}
	\bm{x}_{2h}^{(1)} = I_{h}^{2h} \bm{x}_{h}^{(0)} + M_{2h}^{-1} ( I_{h}^{2h} \bm{b}_h - \underbrace{I_{h}^{2h} A_h I_{2h}^{h}}_{A_{2h}} I_{h}^{2h} \bm{x}_{h}^{(0)} ).
\end{equation}
Consequently, performing one step of relaxation with a zero initial guess on the coarse-grid error equation corresponds to one step of relaxation on the equation
\begin{equation}
	I_{h}^{2h} A_h I_{2h}^h \bm{x}_{2h} = I_{h}^{2h} \bm{b}_h,
\end{equation}
with an initial guess of $\bm{x}_{2h}^{(0)} = I_{h}^{2h} \bm{x}_{h}^{(0)}$.
Because smoothing aims to remove the remaining oscillatory components of the error obtained from the fine grid, performing a relaxation of the form of Equation~\eqref{eq:initial-coarse-grid-relaxation} precisely serves this purpose.

\subsection{Restriction and Prolongation}
%TODO fix stencil notation in this section
Before we can define an actual multigrid method, which recursively applies the techniques introduced here to a hierarchy of discretizations consisting of more than two levels, we need to define suitable inter-grid operators, $I_{2h}^{h}$ and $I_{h}^{2h}$.
Here, the restriction operator $I_{2h}^{h}$ must be able to compute an approximation of the current residual on a coarser grid, while the goal of the prolongation operator $I_{h}^{2h}$ is to transfer a approximation for the solution of the error equation back to a finer grid.
In both cases, we are, first and foremost, interested in preserving the low-frequency components, as the remaining ones will be quickly reduced by smoothing.
In general, the definition of such operators depends on the chosen method of discretization. 
Since, as mentioned in Section~\ref{sec:discretization}, this work focuses on the discretizations of PDEs on regular grids, we do not consider inter-grid operators defined on other grid types, such as unstructured grids.
For a detailed treatment of these cases, the reader is referred to~\cite{trottenberg2000multigrid,ruge1987algebraic,stuben2001introduction}.%TODO mehr Referenzen einfügen
First, to define the restriction operator, we can make the observation, that on a regular grid, the set coarse grid points is always contained in the set of fine grid points.
The easiest way to define such an operator is to simply take the values present at the respective points of the fine grid over to the coarse grid, which leads to the so-called \emph{injection} restriction operator.
While this approach can lead to a functioning multigrid method in case the residual is sufficiently smooth, it neglects the information contained within all fine grid points that do not coincide with a coarse grid point.
In most cases, it is beneficial to incorporate this information into the coarse grid, by computing a weighted average over all neighboring fine grid points.
This idea leads to the so-called \emph{full-weighting} and \emph{half-weighting} restriction operators.
Using our previously defined stencil notation, the one-dimensional full-weighting restriction operator is given by
\begin{equation}
	I_{h_x}^{2 h_x} = \frac{1}{4}\{((-1), 1), ((0), 2), ((1), 1)\}_{h_x}^{2h_x}
\end{equation} 
or equivalently using the matrix notation
\begin{equation}
	I_{h_x}^{2h_x} =  \frac{1}{4} \begin{bmatrix}
			1 & 2 & 1
		\end{bmatrix}_{h_x}^{2h_x}.
	\label{eq:full-weighting-restriction}
\end{equation} 
If we treat Equation~\eqref{eq:full-weighting-restriction} as a row vector, we can define the two- and three-dimensional full-weighting restriction operator as a tensor product with the corresponding column vector, such that
\begin{equation}
	I^{2h_x, 2h_y}_{h_x, h_y} = \frac{1}{4} \begin{bmatrix}
		1 \\ 2 \\ 1
	\end{bmatrix}_{h_y}^{2h_y} \otimes \frac{1}{4} \begin{bmatrix}
		1 & 2 & 1
	\end{bmatrix}_{h_x}^{2h_x} =
\frac{1}{16} 
\begin{bmatrix}
	1 & 2 & 1 \\
	2 & 4 & 2 \\
	1 & 2 & 1
\end{bmatrix}^{2h_x, 2h_y}_{h_x, h_y},
\end{equation} 
\begin{equation}
\begin{split}
	& I^{2h_x, 2h_y, 2h_z}_{h_x, h_y, h_z} = \frac{1}{4} \begin{bmatrix}
		1 & 2 & 1
	\end{bmatrix}_{h_z}^{2h_z} \otimes 
	\frac{1}{16} 
	\begin{bmatrix}
		1 & 2 & 1 \\
		2 & 4 & 2 \\
		1 & 2 & 1
	\end{bmatrix}^{2h_x, 2h_y}_{h_x, h_y} \\
& = \frac{1}{64} \begin{bmatrix}
\begin{bmatrix}
	1 & 2 & 1 \\
	2 & 4 & 2 \\
	1 & 2 & 1
\end{bmatrix} &	\begin{bmatrix}
2 & 4 & 2 \\
4 & 8 & 4 \\
2 & 4 & 2
\end{bmatrix} &
\begin{bmatrix}
	1 & 2 & 1 \\
	2 & 4 & 2 \\
	1 & 2 & 1
\end{bmatrix}
\end{bmatrix}^{2h_x, 2h_y, 2h_z}_{h_x, h_y, h_z}.
\end{split}
\end{equation}
A second restriction operator based on the idea of averaging neighboring fine grid point is the half-weighting restriction operator, whose two- and three-dimensional versions are defined as
\begin{equation}
	I^{2h_x,2h_y}_{h_x, h_y} = \frac{1}{8}
	\begin{bmatrix}
		0 & 1 & 0 \\
		1 & 4 & 1 \\
		0 & 1 & 0
	\end{bmatrix}^{2h_x}_{h_x},
\end{equation} 
\begin{equation}
\begin{split}
	& I^{2h_x, 2h_y, 2h_z}_{h_x, h_y, h_z} = 
\frac{1}{4} \begin{bmatrix}
	1 & 2 & 1
\end{bmatrix}_{h_z}^{2h_z}
\otimes 
\frac{1}{8}
	\begin{bmatrix}
	0 & 1 & 0 \\
	1 & 4 & 1 \\
	0 & 1 & 0 
\end{bmatrix}^{2h_x,2h_y}_{h_x, h_y} \\
& =
\frac{1}{32} \begin{bmatrix}
	\begin{bmatrix}
		& 1 &  \\
		1 & 4 & 1 \\
		& 1 & 
	\end{bmatrix}
 &		\begin{bmatrix}
 	& 2 &  \\
 	2 & 6 & 2 \\
 	& 2 & 
 \end{bmatrix} &
	\begin{bmatrix}
	& 1 &  \\
	1 & 4 & 1 \\
	& 1 & 
\end{bmatrix}
\end{bmatrix}^{2h_x, 2h_y, 2h_z}_{h_x, h_y, h_z}
\end{split}
\end{equation} 
Note that in contrast to our original definition of the stencil operator in Equation~\eqref{eq:stencil-application}, which applies the stencil to each grid point on the current grid, the restriction stencils defined here only have to be applied on each point on the fine grid that coincides with a coarse grid point.
We can therefore replace it with the following slightly adapted definition of stencil application
\begin{equation}
	\begin{split}
		& I_{h}^{2h} \cdot u_h(\bm{x}) = \sum_{k=1}^m b_k u_h({\bm x + \bm{a}_k} \circ \bm{h}) \quad 
		\text{with} \; \bm{x} \in G_{2h}, m \in \mathbb{N} \\ & (\bm{a}_k, b_k) \in I_{h}^{2h} \; \forall \, k \in \{ 1, 2, \dots, m \}
	\end{split}
	\label{eq:stencil-restriction-application}
\end{equation}
where $u_h(\bm{x})$ with $\bm{x} \in G_{2h}$ represents an arbitrary point on the fine grid for which there exists a unique coarse-grid point defined at the spatial position within the computational domain, which is an immediate consequence of the fact that the set of coarse-grid points represents a subset of the set of fine-grid points.

Conversely, for prolongation our goal is to define an operator that transfers an approximation of the error computed on a certain discretization level to a grid of higher resolution.
In contrast to restriction, where the coarse grid can be considered a subset of the fine grid, we must be able to restore a higher number of grid points from the given values on the coarse grid, which can be considered as a distribution process.
The application of this operator to a given coarse grid point $u_{2h}(\bm{x})$ with $\bm{x} \in G_{2h} \supset G_h$ can be defined as
\begin{equation}
	I_{h}^{2h} \cdot u_{2h}(\bm{x}) \rightarrow
	\begin{cases}
		& \forall \, (\bm{a}_k, b_k) \in I_{2h}^{h} \; \text{with} \; k \in \{ 1, 2, \dots, m \} \; \wedge \; \bm{x} \in G_{2h} : \\
		& u_{h}(\bm{x} + \bm{a}_k \circ \bm{h}) = u_{h}(\bm{x} + \bm{a}_k \circ \bm{h}) + b_k u_{2h}(\bm{x}) 
	\end{cases},
	\label{eq:stencil-prolongation application}
\end{equation}
whereby we assume that initially $\forall u_h(\bm{x}) \; \text{with} \; \bm{x} \in \mathcal G_h : u_h(\bm{x}) = 0$.
A common choice for one-dimensional prolongation is the linear interpolation operator~\cite{trottenberg2000multigrid}, which can be defined as
\begin{equation}
	I_{h_x}^{2h_x} =  \frac{1}{2} \begin{bmatrix}
		1 & 2 & 1
	\end{bmatrix}_{h_x}^{2h_x}.
	\label{eq:linear-interpolation}
\end{equation}
Similar to full-weighting restriction we can define the two- and three-dimensional versions of this operator as tensor products of the form
\begin{equation}
	I_{h_x, h_y}^{2h_x, 2h_y} = \frac{1}{2} \begin{bmatrix}
		1 \\ 2 \\ 1
	\end{bmatrix}_{h_y}^{2h_y} \otimes \frac{1}{2} \begin{bmatrix}
		1 & 2 & 1
	\end{bmatrix}_{h_x}^{2h_x} =
	\frac{1}{4} 
	\begin{bmatrix}
		1 & 2 & 1 \\
		2 & 4 & 2 \\
		1 & 2 & 1
	\end{bmatrix}_{h_x, h_y}^{2h_x, 2h_y},
\end{equation}
\begin{equation}
	\begin{split}
		I_{h_x, h_y, h_z}^{2h_x, 2h_y, 2h_z} = & \frac{1}{2} \begin{bmatrix}
			1 & 2 & 1
		\end{bmatrix}_{2h}^{h} \otimes 
		\frac{1}{4} 
		\begin{bmatrix}
			1 & 2 & 1 \\
			2 & 4 & 2 \\
			1 & 2 & 1
		\end{bmatrix}_{2h}^{h} \\
		= & \frac{1}{8} \begin{bmatrix}
			\begin{bmatrix}
				1 & 2 & 1 \\
				2 & 4 & 2 \\
				1 & 2 & 1
			\end{bmatrix}&	\begin{bmatrix}
				2 & 4 & 2 \\
				4 & 8 & 4 \\
				2 & 4 & 2
			\end{bmatrix} &
			\begin{bmatrix}
				1 & 2 & 1 \\
				2 & 4 & 2 \\
				1 & 2 & 1
			\end{bmatrix}
		\end{bmatrix}_{2h}^{h} .
	\end{split}
\end{equation}
Finally, we want to emphasize again that the prolongation and restriction operators presented here

\subsection{The Multigrid Cycle}
By putting this all together we can formulate a recursive version of Algorithm~\ref{alg:two-grid-method}, that enables us to perform an arbitrary number of coarsening steps until we solve the resulting error equation precisely on a certain discretization level, which can be seen in Algorithm~\ref{alg:multigrid-cycle}.
Note that the function \textsc{mg-cycle} has a number of additional parameters compared to our original two-grid method.
First, of all $l$ represents the number of recursive coarsening steps until the respective system of linear equations is solved directly.
Furthermore, since in certain cases a single recursive application of this function is not sufficient to obtain a reasonably accurate approximation on the coarse grid, the procedure can be repeated for a certain number of times, which is controlled by the parameter $\gamma$.
\begin{algorithm}
	\caption{Multigrid Cycle}
	\label{alg:multigrid-cycle}
	\begin{algorithmic}
		\Function{mg-cycle}{$\tilde{\bm{x}}_h$, $A_h$, $\bm{b}_h$, $l$, $\gamma$, $\nu_1$, $\nu_2$, $\omega$}
		\For{$i = 1, \dots, \nu_1$}
		\State{$\tilde{\bm{x}}_h = \tilde{\bm{x}}_h + \omega M_h^{-1} \left( \bm{b}_h - A_h \tilde{\bm{x}}_h \right)$ where $A_h = M_h + N_h$}
		\EndFor
		\State{$\bm{r}_h = \bm{b}_h - A_h \tilde{\bm {x}}_h$}
		\State{$\bm{b}_{2h} = I_h^{2h} \bm{r}_h$}
		\If{$l = 0$}
		\State{ Solve $A_{2h} \bm{x}_{2h} = \bm{b}_{2h}$ for $\bm{x}_{2h}$}
		\Else
		\State{$\tilde{\bm{x}}_{2h} = \bm{0}_{2h}$}
		\For{$i = 1, \dots, \gamma$}
		\State{$\tilde{\bm{x}}_{2h} =  \textsc{mg-cycle}(\tilde{\bm{x}}_{2h}, A_{2h}, \bm{b}_{2h}, l-1, \gamma, \nu_1, \nu_2)$}
		\EndFor
		\EndIf
		\State{$\tilde{\bm{x}}_h = \tilde{\bm{x}}_h + I_{2h}^h \tilde{\bm{x}}_{2h}$}
		\For{$i = 1, \dots, \nu_2$}
		\State{$\tilde{\bm{x}}_h = \tilde{\bm{x}}_h + \omega M_h^{-1} \left( \bm{b}_h - A_h \tilde{\bm{x}}_h \right)$ where $A_h = M_h + N_h$}
		\EndFor
		\State \Return{$\tilde{\bm{x}}_h$}
		\EndFunction
	\end{algorithmic}
\end{algorithm}
Finally, the parameters $\nu_1$ and $\nu_2$ specify the number of smoothing steps before and after the coarse grid correction is performed, respectively.
Note that when performing multiple sweeps of relaxation or coarse grid correction, the initial guess is, as usual, replaced by the approximation obtained in the previous step.
While in principle the parameters of Algorithm~\ref{alg:multigrid-cycle} can be freely chosen, one usually classifies multigrid cycles according to the choice of $\gamma$, as each value corresponds to a distinct computational pattern.
For instance, choosing $\gamma = 1$ means that only a single recursive descent is performed on each discretization level.
Figure~\ref{fig:three-grid-cycles} and~\ref{fig:four-grid-cycles} show the computational patterns resulting from different values of $\gamma$ on a hierarchy of three and four grids, respectively.
\begin{figure}
	\captionsetup{justification=centering}
	\begin{subfigure}{0.1\textwidth}
		\begin{tikzpicture}
			\node   (h) at (-0.75, 4){$h$};
			\node   (2h) at (-0.75, 3){$2h$};
			\node   (4h) at (-0.75, 2){$4h$};
		\end{tikzpicture}
		\caption*{}
	\end{subfigure}
	\begin{subfigure}{0.22\textwidth}
		\begin{tikzpicture}
			\node	(a) at (0,4) [draw, circle,scale=0.8] {};
			\node	(b) at (0.5,3) [draw, circle,scale=0.8] {};
			\node	(c) at (1,2) [draw, circle,,fill=black,scale=0.8] {};
			\node	(d) at (1.5,3) [draw, circle,scale=0.8] {};
			\node	(e) at (2,4) [draw, circle, scale=0.8] {};
			
			\draw 
			(a) edge[->] (b) 
			(b) edge[->] (c)
			(c) edge[->] (d)
			(d) edge[->] (e)   
			;
		\end{tikzpicture}
		\caption*{$\gamma = 1$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\begin{tikzpicture}
			\node	(a) at (0,4) [draw, circle,scale=0.8] {};
			\node	(b) at (0.5,3) [draw, circle,scale=0.8] {};
			\node	(c) at (1,2) [draw, circle,fill=black,scale=0.8] {};
			\node	(d) at (1.5,3) [draw, circle,scale=0.8] {};
			\node	(e) at (2,2) [draw, circle, fill=black, scale=0.8] {};
			\node	(f) at (2.5,3) [draw, circle, scale=0.8] {};
			\node	(g) at (3,4) [draw, circle, scale=0.8] {};
			
			\draw 
			(a) edge[->] (b) 
			(b) edge[->] (c)
			(c) edge[->] (d)
			(d) edge[->] (e)   
			(e) edge[->] (f)
			(f) edge[->] (g)
			;
		\end{tikzpicture}
		\caption*{$\gamma = 2$}
	\end{subfigure}
	\begin{subfigure}{0.32\textwidth}
		\begin{tikzpicture}
			\node	(a) at (0,4) [draw, circle,scale=0.8] {};
			\node	(b) at (0.5,3) [draw, circle,scale=0.8] {};
			\node	(c) at (1,2) [draw, circle,fill=black,scale=0.8] {};
			\node	(d) at (1.5,3) [draw, circle,scale=0.8] {};
			\node	(e) at (2,2) [draw, circle, fill=black, scale=0.8] {};
			\node	(f) at (2.5,3) [draw, circle, scale=0.8] {};
			\node	(g) at (3,2) [draw, circle, fill=black,scale=0.8] {};
			\node	(h) at (3.5,3) [draw, circle, scale=0.8] {};	
			\node	(i) at (4,4) [draw, circle, scale=0.8] {};	
			\draw 
			(a) edge[->] (b) 
			(b) edge[->] (c)
			(c) edge[->] (d)
			(d) edge[->] (e)   
			(e) edge[->] (f)
			(f) edge[->] (g)
			(g) edge[->] (h)
			(h) edge[->] (i)
			;
		\end{tikzpicture}
		\caption*{$\gamma = 3$}
	\end{subfigure}
	\caption{Three-grid cycles ($l = 2$).}
	\label{fig:three-grid-cycles}
\end{figure}

\begin{figure}
	\captionsetup{justification=centering}
	\begin{subfigure}{0.1\textwidth}
		\begin{tikzpicture}
			\node   (h) at (-0.75, 4){$h$};
			\node   (2h) at (-0.75, 3){$2h$};
			\node   (4h) at (-0.75, 2){$4h$};
			\node   (8h) at (-0.75, 1){$8h$};
		\end{tikzpicture}
		\caption*{}
	\end{subfigure}
	\begin{subfigure}{0.3\textwidth}
		\begin{tikzpicture}
			\node	(a) at (0,4) [draw, circle,scale=0.8] {};
			\node	(b) at (0.5,3) [draw, circle,scale=0.8] {};
			\node	(c) at (1,2) [draw, circle,scale=0.8] {};
			\node	(d) at (1.5,1) [draw, circle,fill=black, scale=0.8] {};
			\node	(e) at (2,2) [draw, circle, scale=0.8] {};
			\node	(f) at (2.5,3) [draw, circle,scale=0.8] {};
			\node	(g) at (3,4) [draw, circle,scale=0.8] {};
			\draw 
			(a) edge[->] (b) 
			(b) edge[->] (c)
			(c) edge[->] (d)
			(d) edge[->] (e)   
			(e) edge[->] (f)
			(f) edge[->] (g)
			
			;
		\end{tikzpicture}
		\caption*{$\gamma = 1$}
	\end{subfigure}
	\begin{subfigure}{0.6\textwidth}
		\begin{tikzpicture}
			\node	(a) at (0,4) [draw, circle,scale=0.8] {};
			\node	(b) at (0.5,3) [draw, circle,scale=0.8] {};
			\node	(c) at (1,2) [draw, circle,scale=0.8] {};
			\node	(d) at (1.5,1) [draw, circle,fill=black, scale=0.8] {};
			\node	(e) at (2,2) [draw, circle, scale=0.8] {};
			\node	(f) at (2.5,1) [draw, circle,fill=black,scale=0.8] {};
			\node	(g) at (3,2) [draw, circle,scale=0.8] {};
			\node	(h) at (3.5,3) [draw, circle,scale=0.8] {};
			\node	(i) at (4,2) [draw, circle,scale=0.8] {};
			\node	(j) at (4.5,1) [draw, circle,fill=black, scale=0.8] {};
			\node	(k) at (5,2) [draw, circle, scale=0.8] {};
			\node	(l) at (5.5,1) [draw, circle,fill=black,scale=0.8] {};
			\node	(m) at (6,2) [draw, circle,scale=0.8] {};
			\node	(n) at (6.5,3) [draw, circle, scale=0.8] {};
			\node	(o) at (7,4) [draw, circle, scale=0.8] {};
			
			\draw 
			(a) edge[->] (b) 
			(b) edge[->] (c)
			(c) edge[->] (d)
			(d) edge[->] (e)   
			(e) edge[->] (f)
			(f) edge[->] (g)
			(g) edge[->] (h)
			(h) edge[->] (i)
			(i) edge[->] (j)
			(j) edge[->] (k)
			(k) edge[->] (l)
			(l) edge[->] (m)
			(m) edge[->] (n)
			(n) edge[->] (o)
			;
		\end{tikzpicture}
		\caption*{$\gamma = 2$}
	\end{subfigure}
	\caption{Four-grid cycles ($l = 3$).}
	\label{fig:four-grid-cycles}
\end{figure}
Here each white node corresponds to one or multiple step of smoothing on the respective level, while a black node represents the direct solution of the resulting error equation on the coarsest level.
Since, as it can be seen in Figure~\ref{fig:three-grid-cycles}, the choice of $\gamma = 1$ results in a V-like computational pattern, the corresponding multigrid method is usually called a V-cycle.
Similarly, the computational pattern of a method with $\gamma = 2$ on a three-grid hierarchy looks like a W, and, hence, the name W-cycle is chosen.
As it can be seen in Figure~\ref{fig:four-grid-cycles} the amount of computation of a W-cycle drastically increases with the number of coarsening steps.
In general the choice of a V-cycle always results in the least amount of computations.
However, in many cases a single coarse grid correction step is not sufficient to reduce the oscillatory components of the initial error on the finest grid and more computationally expensive methods, such as W-cycles are required~\cite{trottenberg2000multigrid}.
Due to the resulting drastic increase in the number of computations, larger values of $\gamma$ are usually impractical for multigrid methods with more than two coarsening steps~\cite{trottenberg2000multigrid}.
While Algorithm~\ref{alg:multigrid-cycle} enables the construction of different multigrid methods with the choice of different values for the parameters $l$, $\gamma$, $\nu_1$, $\nu_2$ and $\omega$, the structural composition of the resulting methods is limited.
For instance, in Algorithm~\ref{alg:multigrid-cycle} assumes that the same value of $\gamma$ is chosen on each level, which restricts possible computational pattern to those depicted in Figure~\ref{fig:three-grid-cycles} and~\ref{fig:four-grid-cycles}.
One way to surpass this limitation is to combine different cycle types in a single method.
For instance employing a combination of W- and V-cycles on each level results in a so-called F-cycle, whose computational pattern is shown in Figure~\ref{fig:f-cycle}.
\begin{figure}
	\begin{subfigure}{0.4\textwidth}

		\begin{tikzpicture}
			\node   (h) at (-0.75, 4){$h$};
			\node   (2h) at (-0.75, 3){$2h$};
			\node   (4h) at (-0.75, 2){$4h$};
			\node   (8h) at (-0.75, 1){};
			\node	(a) at (0,4) [draw, circle,scale=0.8] {};
			\node	(b) at (0.5,3) [draw, circle,scale=0.8] {};
			\node	(c) at (1,2) [draw, circle,fill=black,scale=0.8] {};
			\node	(d) at (1.5,3) [draw, circle,scale=0.8] {};
			\node	(e) at (2,2) [draw, circle, fill=black, scale=0.8] {};
			\node	(f) at (2.5,3) [draw, circle, scale=0.8] {};
			\node	(g) at (3,4) [draw, circle, scale=0.8] {};
			
			\draw 
			(a) edge[->] (b) 
			(b) edge[->] (c)
			(c) edge[->] (d)
			(d) edge[->] (e)   
			(e) edge[->] (f)
			(f) edge[->] (g)
			;
		\end{tikzpicture}
	\end{subfigure}
	\begin{subfigure}{0.6\textwidth}
		\begin{tikzpicture}
				\node   (h) at (-0.75, 4){$h$};
				\node   (2h) at (-0.75, 3){$2h$};
				\node   (4h) at (-0.75, 2){$4h$};
				\node   (8h) at (-0.75, 1){$8h$};
			\node	(a) at (0,4) [draw, circle,scale=0.8] {};
			\node	(b) at (0.5,3) [draw, circle,scale=0.8] {};
			\node	(c) at (1,2) [draw, circle,scale=0.8] {};
			\node	(d) at (1.5,1) [draw, circle,fill=black, scale=0.8] {};
			\node	(e) at (2,2) [draw, circle, scale=0.8] {};
			\node	(f) at (2.5,1) [draw, circle,fill=black,scale=0.8] {};
			\node	(g) at (3,2) [draw, circle,scale=0.8] {};
			\node	(h) at (3.5,3) [draw, circle,scale=0.8] {};
			\node	(i) at (4,2) [draw, circle,scale=0.8] {};
			\node	(j) at (4.5,1) [draw, circle,fill=black, scale=0.8] {};
			\node	(k) at (5,2) [draw, circle, scale=0.8] {};
			\node	(l) at (5.5,3) [draw, circle,scale=0.8] {};
			\node	(m) at (6,4) [draw, circle,scale=0.8] {};
			
			\draw 
			(a) edge[->] (b) 
			(b) edge[->] (c)
			(c) edge[->] (d)
			(d) edge[->] (e)   
			(e) edge[->] (f)
			(f) edge[->] (g)
			(g) edge[->] (h)
			(h) edge[->] (i)
			(i) edge[->] (j)
			(j) edge[->] (k)
			(k) edge[->] (l)
			(l) edge[->] (m)
			;
		\end{tikzpicture}
	\end{subfigure}
	\caption{Computational pattern of the F-cycle with a different numbers of coarsening steps.}
	\label{fig:f-cycle}
\end{figure}
Except for the case of $l = 2$, where F-cycle and W-cycle are equivalent, it represents a compromise between applying a pure V- or W-cycle.
While the combination of different cycle types, as within the F-cycle, represents an additional degree of freedom, the resulting methods can be still formulated within the rules of Algorithm~\ref{alg:multigrid-cycle} and, hence, are fully described by the aforementioned parameters.
In the classical formulation of a multigrid method as in~\cite{brandt1977multi,hackbusch2013multi,trottenberg2000multigrid,briggs2000multigrid} these parameters are considered global and, hence, represent a fixed choice throughout the method.
Therefore, within this framework, the same smoothing procedure and cycle type is assumed on every discretization level of the method.
While for many applications this approach leads to efficient multigrid methods~\cite{trottenberg2000multigrid} there are still a number of cases, where the full potential of multigrid methods could not yet be exploited, see for instance~\cite{ernst2012difficult,benzi2005numerical}.
While to date there exists a rich amount of research on the optimization of multigrid cycles based on its classical formulation, the variation of its components on an individual level has not been considered yet.
Such an approach would grant us the flexibility to generate multigrid cycles that consist of arbitrary restriction and coarse-grid correction steps, each with a different combination of smoothers and relaxation factors.
In Algorithm~\ref{alg:multigrid-cycle} the space of possible computational patterns is restricted to a finite number of choices. However, we can overcome this constraint by expressing each multigrid cycle in form of a finite stream of instructions, where the order of instructions is restricted by the rules of linear algebra and multigrid theory.
To specify these rules in a mathematically formal way, we can make use of programming language theory, such that the formulation of a multigrid method is treated as a program construction task.
Therefore, in the next section we give a brief overview about programming language theory and introduce the concept of formal grammars.
We then combine this formalism together with our knowledge about multigrid methods to derive a novel representation of multigrid solvers that enables us to construct these methods in the form of a program design and optimization task.


%TODO computational complexity
%\subsection{Systems of PDEs}
%TODO write section
  


