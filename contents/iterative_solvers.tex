\section{Iterative Methods}
In general, the methods for solving systems of linear equations can be assigned to two categories: Direct and iterative methods.
Direct methods are characterized by the fact that they are able to compute the exact solution of a linear system in a finite number of steps.
In contrast, iterative methods are based on computing a series of approximations for the solution of the linear system.
Even though this series often converges to the exact solution, there is usually no guarantee that the approximations will ever reach the accuracy of a solution computed by a direct method.
Unfortunately, for many problems applying direct methods infeasible, due to their high computational complexity and memory storage requirements.
For instance, Gaussian elimination, in general, requires $\mathcal O(n^3)$ operations for solving a system of linear equations with $n$ unknowns.
Moreover, since the goal of Gaussian elimination is to transform a given matrix into upper-triangular form, it based on the direct manipulation of the input matrix and, hence, usually requires storing it explicitly.
In contrast, many iterative methods are exclusively based on the computation of matrix-vector products and, hence, do not require to manipulate the input matrix directly.
As we have shown in Section~\ref{subsec:stencil-codes}, the discretization of many partial differential equations enables the representation of matrix-vector products as the application of a stencil code, whereas each stencil is directly derived from the discretization of a continuous differential operator.
If we assume that each stencil includes only a finite number of entries and that we need to store at most one stencil per grid point, we can reduce the storage requirements to $\mathcal{O}(n)$, where $n$ is the total number of grid points.
In case the same stencil applies to the whole domain, we even only need to store a single stencil, whose memory requirements are negligible compared to those for storing each grid point.
Furthermore, due to the approximation of real numbers by floating-point numbers on a computer, even the solution of a direct method is prone to numerical errors.
As a consequence, in practice, the exactness of direct methods is often undermined by these effects and, hence, the approximations computed by an iterative method can achieve the same degree of numerical accuracy.%TODO include ref
Furthermore, assuming that we can compute an acceptable approximation using only a finite number of $m$ matrix-vector multiplications, the application of an iterative method reduces the computational complexity for solving a system of linear equations to $O(mn)$.
In the following, we first introduce stationary iterative methods, as the Jacobi, Gauss-Seidel and SOR method, and then give an outlook to more advanced methods, like Krylov Subspace Methods.
Finally, as multigrid methods represent the fundamental basis for this work, we discuss these methods in greater detail.   
\subsection{Stationary Iterative Methods} 
We begin our introduction of stationary iterative method by considering the general system of linear equations
\begin{equation}
	A \bm{x} = \bm{b},
	\label{eq:general-system-of-linear-equations}
\end{equation}
where $A$ is the coefficient matrix, $\bm x$ the vector of unknowns and $\bm b$ the right-hand side.
At this point we do not specify whether $A$ represents a dense/sparse matrix or a stencil, as long as all the operations employed within the subsequent methods are well-defined for a mathematical object of this type.
We can now rewrite Equation~\eqref{eq:general-system-of-linear-equations} to obtain
\begin{equation}
	\bm{x} = \bm{x} + \bm b - A \bm{x}.
	\label{eq:general-fixed-point}
\end{equation}
Which can be considered a fixed point of the form
\begin{equation}
	\bm x = f(\bm x).
\end{equation} 
Replacing $\bm x$ by $\bm{x}^{(i+1)}$ in the left and by $\bm{x}^{(i)}$ in the right part of Equation~\eqref{eq:general-fixed-point} yields the fixed-point iteration
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + \bm b - A \bm{x}^{(i)}.
	\label{eq:richardson-iteration}
\end{equation}
Equation~\eqref{eq:richardson-iteration} is usually called Richardson iteration and can be considered as the most basic form of an iterative method for solving a system of linear equations.
Here, the term $\bm{r}^{(i)} = \bm{b} - A \bm{x}^{(i)}$ represents the \emph{residual} or \emph{defect} in iteration $i$ of the method.
Next, we consider 
\begin{equation}
	M^{-1} A \bm{x} = M^{-1} \bm{b},
	\label{eq:general-preconditioned-system-of-linear-equations}
\end{equation}
which represents a modified version of Equation~\eqref{eq:general-system-of-linear-equations}, that is obtained by multiplying each side of the equation by the inverse of a matrix $M$.
By the rules of linear algebra, Equation~\eqref{eq:general-preconditioned-system-of-linear-equations} is equivalent to the original system and therefore has the same solution.
However, it can be also considered as a left-preconditioned version of Equation~\eqref{eq:general-system-of-linear-equations}, with $M$ as preconditioner.
By, again, considering the fixed point of Equation~\eqref{eq:general-preconditioned-system-of-linear-equations} we obtain the iteration
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + M^{-1}(\bm b - A \bm{x}^{(i)}),
	\label{eq:general-stationary-iterative-method}
\end{equation}
which represents the general form of a stationary iterative method. 
For instance, if we replace $M$ with the unit matrix $I$, we obtain a Richardson iteration.
Furthermore, setting $M = A$ allows us to obtain the solution in one step since then
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + A^{-1}(\bm b - A \bm{x}^{(i)}),
	\label{eq:one-step-iteration}
\end{equation}
which leads to
\begin{equation}
	\bm{x}^{(i+1)} = A^{-1}\bm b.
\end{equation}
The result of this iteration $x^{(i+1)}$ is hence independent of the choice of $x^{(i)}$ and if we insert $A^{-1}\bm b$ Equation~\eqref{eq:general-system-of-linear-equations} it becomes apparent that this term always represents the correct solution of the respective system of linear equations.
Since the computation of the inverse of a general matrix $A$, in general, is more expensive and numerically unstable than solving the system directly, the application of Equation~\eqref{eq:one-step-iteration} is impracticle.
However, with it we can already provide an intuition about the choice of $M$.
The closer $M^{-1}$ is to the actual inverse of $A$, the faster the convergence of the respective stationary iterative method will usually be.
In contrast, the choice of a matrix $M$ that is easy to invert, where the Richardson iteration represents a extreme case with $I^{-1} = I$, leads to an iterative method that is easy to compute, but potentially suffers from slow convergence.
Before we introduce some basic iterative methods that fall into this framework, note that we can reformulate Equation~\eqref{eq:general-stationary-iterative-method} to obtain
\begin{equation}
	M (\bm{x}^{(i+1)} - \bm{x}^{(i)}) = \bm{b} - A \bm{x}^{(i)}. 
\end{equation}
Moreover, by defining $\bm{c}^{(i+1)}$ as the correction term
\begin{equation}
	\bm{c}^{(i+1)} = \bm{x}^{(i+1)} - \bm{x}^{(i)},
\end{equation}
we obtain a new system of linear equations
\begin{equation}
	M \bm{c}^{(i+1)} = \bm{b} - A \bm{x}^{(i)}. 
	\label{eq:general-stationary-iterative-method-system-formulation}
\end{equation}
The solution of this system $\bm{c}^{(i+1)}$ then provides us with the approximate solution in step $i+1$ through the relation
\begin{equation}
	\bm{x}^{(i+1)} =  \bm{x}^{(i)} + \bm{c}^{(i+1)}.
\end{equation}
To obtain a new approximate solution in every iteration, we can, therefore, solve the system of linear equations denoted by Equation~\eqref{eq:general-stationary-iterative-method-system-formulation}, instead of computing the inverse of $M$.

Since, as we have already mentioned, it is desirable to choose $M \approx A$, we define the splitting
\begin{equation}
	A = D - L - U,
\end{equation} 
where $D$ is the lower diagonal, $-L$ the lower triangular and $-U$ the upper triangular part of $A$, respectively.
Both the Jacobi as well as the Gauss-Seidel method are defined based on this splitting.
First of all, setting $M = D$, yields the Jacobi method
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + D^{-1}(\bm b - A \bm{x}^{(i)}).
	\label{eq:jacobi-method}
\end{equation}
Note that since $D$ is a diagonal matrix, we can easily obtain its inverse by inverting all its diagonal entries.
Therefore, each iteration of the Jacobi method consists of a simple matrix-vector multiplication of the residual $\bm{r}^{(i)} = \bm{b} - A \bm{x}^{(i)}$ with the diagonal matrix $D^{-1}$.

To define the Jacobi method in the domain of stencil codes with respect to our derivation presented in Section~\ref{subsec:stencil-codes}, we need to obtain the diagonal of a given stencil $S$, which can be defined as
\begin{equation}
	\text{diag}(S) = \begin{cases}
		\{(\bm{0}, b) \} & \text{if} \; (\bm 0, b) \in S \\
		\emptyset & \text{else}.
	\end{cases}
	\label{eq:stencil-diag}
\end{equation}
Based on this definition, we can also specify the inverse diagonal of a given stencil $S$ as
\begin{equation}
	\text{inv}(S) = \begin{cases}
		\{(\bm{0}, \frac{1}{b}) \} & \text{if} \; (\bm 0, b) \in S \\
		\emptyset & \text{else}.
	\end{cases}
	\label{eq:stencil-diag-inv}
\end{equation}

\begin{equation}
	\text{lower}(S) = \begin{cases}
		\{(\bm{a}, b) \} \cup \text{lower}(\tilde{S}) & \text{if} \; (\bm a, b) \cup \tilde{S} \; \text{and} \; \bm a < \bm 0 \\
		\text{lower}(\tilde{S}) & \text{if} \; (\bm a, b) \cup \tilde{S} \; \text{and} \; \bm a \geq \bm 0 \\
		\emptyset & \text{else}.
	\end{cases}
	\label{eq:stencil-lower}
\end{equation}

\begin{equation}
	\text{upper}(S) = S - \text{lower}(S) - \text{diag}(S)
\end{equation}


