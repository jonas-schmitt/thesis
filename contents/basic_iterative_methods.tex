\section{Basic Iterative Methods}
In general, the methods for solving systems of linear equations can be assigned to two categories: Direct and iterative methods.
Direct methods are characterized by the fact that they are able to compute the exact solution of a linear system in a finite number of steps.
In contrast, iterative methods are based on computing a series of approximations for the solution of the linear system.
Even though this series often converges to the exact solution, there is usually no guarantee that the approximations will ever reach the accuracy of a solution computed by a direct method.
Unfortunately, for many problems applying direct methods infeasible, due to their high computational complexity and memory storage requirements.
For instance, Gaussian elimination, in general, requires $\mathcal O(n^3)$ operations for solving a system of linear equations with $n$ unknowns.
Moreover, since the goal of Gaussian elimination is to transform a given matrix into upper-triangular form, it based on the direct manipulation of the input matrix and, hence, usually requires storing it explicitly.
In contrast, many iterative methods are exclusively based on the computation of matrix-vector products and, hence, do not require to manipulate the input matrix directly.
As we have shown in Section~\ref{subsec:stencil-codes}, the discretization of many partial differential equations enables the representation of matrix-vector products as the application of a stencil code, whereas each stencil is directly derived from the discretization of a continuous differential operator.
If we assume that each stencil includes only a finite number of entries and that we need to store at most one stencil per grid point, we can reduce the storage requirements to $\mathcal{O}(n)$, where $n$ is the total number of grid points.
In case the same stencil applies to the whole domain, we even only need to store a single stencil, whose memory requirements are negligible compared to those for storing each grid point.
Furthermore, due to the approximation of real numbers by floating-point numbers on a computer, even the solution of a direct method is prone to numerical errors.
As a consequence, in practice, the exactness of direct methods is often undermined by these effects and, hence, the approximations computed by an iterative method can achieve the same degree of numerical accuracy.%TODO include ref
Furthermore, assuming that we can compute an acceptable approximation using only a finite number of $m$ matrix-vector multiplications, the application of an iterative method reduces the computational complexity for solving a system of linear equations to $O(mn)$.
In the following, we first introduce basic iterative methods, as the Jacobi and Gauss-Seidel method, and then derive fundamental statements about their convergence. 
%Finally, as multigrid methods represent the fundamental basis for this work, we discuss these methods in greater detail.   
\subsection{Jacobi and Gauss-Seidel} 
We begin our introduction of basic iterative methods by considering the general system of linear equations
\begin{equation}
	A \bm{x} = \bm{b},
	\label{eq:general-system-of-linear-equations}
\end{equation}
where $A$ is the coefficient matrix, $\bm x$ the vector of unknowns and $\bm b$ the right-hand side.
At this point we do not specify whether $A$ represents a dense/sparse matrix or a stencil, as long as all the operations employed within the subsequent methods are well-defined for a mathematical object of this type.
We can now rewrite Equation~\eqref{eq:general-system-of-linear-equations} to obtain
\begin{equation}
	\bm{x} = \bm{x} + \bm b - A \bm{x}.
	\label{eq:general-fixed-point}
\end{equation}
Which can be considered a fixed point of the form
\begin{equation}
	\bm x = f(\bm x).
\end{equation} 
Replacing $\bm x$ by $\bm{x}^{(i+1)}$ in the left and by $\bm{x}^{(i)}$ in the right part of Equation~\eqref{eq:general-fixed-point} yields the fixed-point iteration
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + \bm b - A \bm{x}^{(i)}.
	\label{eq:richardson-iteration}
\end{equation}
Equation~\eqref{eq:richardson-iteration} is usually called Richardson iteration and can be considered as the most basic form of an iterative method for solving a system of linear equations.
Here, the term $\bm{r}^{(i)} = \bm{b} - A \bm{x}^{(i)}$ represents the \emph{residual} or \emph{defect} in iteration $i$ of the method.
Next, we consider 
\begin{equation}
	M^{-1} A \bm{x} = M^{-1} \bm{b},
	\label{eq:general-preconditioned-system-of-linear-equations}
\end{equation}
which represents a modified version of Equation~\eqref{eq:general-system-of-linear-equations}, that is obtained by multiplying each side of the equation by the inverse of a matrix $M$.
By the rules of linear algebra, Equation~\eqref{eq:general-preconditioned-system-of-linear-equations} is equivalent to the original system and therefore has the same solution.
However, it can be also considered as a left-preconditioned version of Equation~\eqref{eq:general-system-of-linear-equations}, with $M$ as preconditioner.
By, again, considering the fixed point of Equation~\eqref{eq:general-preconditioned-system-of-linear-equations} we obtain the iteration
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + M^{-1}(\bm b - A \bm{x}^{(i)}),
	\label{eq:general-stationary-iterative-method}
\end{equation}
which represents the general form of a stationary iterative method. 
For instance, if we replace $M$ with the unit matrix $I$, we obtain a Richardson iteration.
Furthermore, setting $M = A$ allows us to obtain the solution in one step since then
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + A^{-1}(\bm b - A \bm{x}^{(i)}),
	\label{eq:one-step-iteration}
\end{equation}
which leads to
\begin{equation}
	\bm{x}^{(i+1)} = A^{-1}\bm b.
\end{equation}
The result of this iteration $x^{(i+1)}$ is hence independent of the choice of $x^{(i)}$ and if we insert $A^{-1}\bm b$ Equation~\eqref{eq:general-system-of-linear-equations} it becomes apparent that this term always represents the correct solution of the respective system of linear equations.
Since the computation of the inverse of a general matrix $A$, in general, is more expensive and numerically unstable than solving the system directly, the application of Equation~\eqref{eq:one-step-iteration} is impracticle.
However, with it we can already provide an intuition about the choice of $M$.
The closer $M^{-1}$ is to the actual inverse of $A$, the faster the convergence of the respective stationary iterative method will usually be.
In contrast, the choice of a matrix $M$ that is easy to invert, where the Richardson iteration represents a extreme case with $I^{-1} = I$, leads to an iterative method that is easy to compute, but potentially suffers from slow convergence.
Before we introduce some basic iterative methods that fall into this framework, note that we can reformulate Equation~\eqref{eq:general-stationary-iterative-method} to obtain
\begin{equation}
	M (\bm{x}^{(i+1)} - \bm{x}^{(i)}) = \bm{b} - A \bm{x}^{(i)}. 
\end{equation}
Moreover, by defining $\bm{c}^{(i+1)}$ as the correction term
\begin{equation}
	\bm{c}^{(i+1)} = \bm{x}^{(i+1)} - \bm{x}^{(i)},
\end{equation}
we obtain a new system of linear equations
\begin{equation}
	M \bm{c}^{(i+1)} = \bm{b} - A \bm{x}^{(i)}. 
	\label{eq:general-stationary-iterative-method-system-formulation}
\end{equation}
The solution of this system $\bm{c}^{(i+1)}$ then provides us with the approximate solution in step $i+1$ through the relation
\begin{equation}
	\bm{x}^{(i+1)} =  \bm{x}^{(i)} + \bm{c}^{(i+1)}.
\end{equation}
To obtain a new approximate solution in every iteration, we can, therefore, solve the system of linear equations denoted by Equation~\eqref{eq:general-stationary-iterative-method-system-formulation}, instead of computing the inverse of $M$.

Since, as we have already mentioned, it is desirable to choose $M \approx A$, we define the splitting
\begin{equation}
	A = D - L - U,
\end{equation} 
where $D$ is the lower diagonal, $-L$ the lower triangular and $-U$ the upper triangular part of $A$, respectively.
Both the Jacobi as well as the Gauss-Seidel method are defined based on this splitting.
First of all, setting $M = D$, yields the Jacobi method
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + D^{-1}(\bm b - A \bm{x}^{(i)}).
	\label{eq:jacobi-method}
\end{equation}
Note that since $D$ is a diagonal matrix, we can easily obtain its inverse by inverting all its diagonal entries.
Therefore, each iteration of the Jacobi method consists of a simple matrix-vector multiplication of the residual $\bm{r}^{(i)} = \bm{b} - A \bm{x}^{(i)}$ with the diagonal matrix $D^{-1}$.
To define the Jacobi method in the domain of stencil codes with respect to our derivation presented in Section~\ref{subsec:stencil-codes}, we need to obtain the diagonal of a given stencil $S$.
As an offset of zero in each dimension always refers to the grid point on which the stencil is applied and therefore to the diagonal entry in each line of the corresponding matrix.
We can therefore define the diagonal of $S$ as
\begin{equation}
	\text{diag}(S) = \begin{cases}
		\{(\bm{0}, b) \} & \text{if} \; (\bm 0, b) \in S \\
		\emptyset & \text{else}.
	\end{cases}
	\label{eq:stencil-diag}
\end{equation}
Based on this definition, we can also specify the inverse diagonal of a stencil as
\begin{equation}
	\text{diag-inv}(S) = \begin{cases}
		\{(\bm{0}, \frac{1}{b}) \} & \text{if} \; (\bm 0, b) \in S \\
		\emptyset & \text{else}.
	\end{cases}
	\label{eq:stencil-diag-inv}
\end{equation}
Because the Jacobi method, as defined in Equation~\eqref{eq:jacobi-method}, does often suffer from slow convergence, it is common to introduce a \emph{relaxation factor} or \emph{weight} $\omega$, which leads to the so-called weighted Jacobi method
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + \omega D^{-1}(\bm b - A \bm{x}^{(i)}),
	\label{eq:weighted-jacobi-method}
\end{equation}
where $\omega$ is chosen from the interval $\left(0, 2\right)$.
Here, a value of $\omega$ smaller than one is usually called underrelaxation, while for a value greater than one the term overrelaxation is used.
Note that $\omega = 1$ leads to the original Jacobi method without any relaxation.

Next, we define the Gauss-Seidel method with $M = D - U$ which results in an iteration of the form
\begin{equation}
	(D - L) \bm{c}^{(i+1)} = \bm{b} - A \bm{x}^{(i)}, 
	\label{eq:gauss-seidel-method}
\end{equation}
where, again, $\bm{c}^{(i+1)}$ is the correction term $\bm{c}^{(i+1)} = \bm{x}^{(i+1)} - \bm{x}^{(i)}$.
Note that since $M = D - L$ does not represent a diagonal matrix, we can not easily obtain its inverse, but instead solve Equation~\eqref{eq:gauss-seidel-method} in each iteration of the method.
As $D - L$ is a lower triangular matrix, the solution of this system can be computed with a single step of back substitution. 
To apply the Gauss-Seidel method within the domain of stencil codes, we also need to define a function that extracts the lower triangle of a given stencil.
As each diagonal entry corresponds to an offset of zero in each dimension, we can obtain the lower triangle by only including all entry with an offset lower than zero.
The resulting operation is defined in Equation~\eqref{eq:stencil-lower}.
\begin{equation}
	\text{lower}(S) = \begin{cases}
		\{(\bm{a}, b) \} \cup \text{lower}(\tilde{S}) & \text{if} \; \exists\, \bm a, b \; \text{with} \; \bm a < \bm 0 \; \text{and} \; S = (\bm a, b) \cup \tilde{S} \\
		\text{lower}(\tilde{S}) & \text{if} \; \exists\, \bm a, b \; \text{with} \; \bm a \geq \bm 0 \; \text{and} \; S = (\bm a, b) \cup \tilde{S} \\
		\emptyset & \text{else}.
	\end{cases}
	\label{eq:stencil-lower}
\end{equation}

Similarly, we can also provide a definition for obtaining the upper triangle of a given stencil, which is formulated in Equation~\eqref{eq:stencil-upper}.
\begin{equation}
	\text{upper}(S) = \begin{cases}
		\{(\bm{a}, b) \} \cup \text{upper}(\tilde{S}) & \text{if} \; \exists\, \bm a, b \; \text{with} \; \bm a > \bm 0 \; \text{and} \; S = (\bm a, b) \cup \tilde{S} \\
		\text{upper}(\tilde{S}) & \text{if} \; \exists\, \bm a, b \; \text{with} \; \bm a \leq \bm 0 \; \text{and} \; S = (\bm a, b) \cup \tilde{S} \\
		\emptyset & \text{else}.
	\end{cases}
	\label{eq:stencil-upper}
\end{equation}
While there are many more iterative methods that can be formulated in the form of Equation~\eqref{eq:general-stationary-iterative-method}, the goal of this section is to introduce only those concepts necessary for a basic understanding of these method's functioning.
Therefore, we postpone the treatment of other and more advanced variants of the methods presented here to later chapters of this thesis. 

\subsection{Convergence}
In contrast to direct methods, which compute the solution of a given system of linear equations in a fixed number of computational steps, iterative methods compute a series of approximations.
The method is then said to convergence when the difference between the actual solution and subsequent approximations approaches zero.
To quantify this behavior, we introduce a number of metrics used for evaluating the quality of a series of approximations obtained through a specific iterative method.
First of all, we can define the error in iteration $i$ as
\begin{equation*}
	\bm{e}^{(i)} = \bm{x}^{(i)} - \bm{x}^{*},
\end{equation*}
where $\bm{x}^{(i)}$ is the $i$th approximation and $\bm{x}^{*}$ the correct solution of the system.
While the error gives as an immediate way to quantify the accuracy of an approximation, we usually do not know the correct solution of a given system of linear equations and therefore are not able to compute this metric.
As a remedy, we can instead consider the residual
\begin{equation*}
	\bm{r}^{(i)} = \bm{b} - A \bm{x}^{(i)},
\end{equation*}
which can be always computed irrespective of whether the solution of the system is known.
Note that the residual will be always zero in case the error is zero.
We then reconsider that the iterative methods introduced in the last section can be all considered a fixed-point iteration of the form
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{(i)} + M^{-1}(\bm b - A \bm{x}^{(i)}),
\end{equation}
and assume that $\bm{e}^{(i)} = \bm{0}$.
Hence, we have $\bm{x}^{(i)} = \bm{x}^{*}$ and the above equation is reduced to
\begin{equation}
	\bm{x}^{(i+1)} = \bm{x}^{*}.
\end{equation}
The solution $\bm{x}^{*}$ of the system $A \bm{x} = b$ is therefore a fixed point of each stationary iterative method.
However, note that this represents a necessary and not a sufficient condition. 
We, therefore, consider an arbitrary fixed-point of Equation~\eqref{eq:general-stationary-iterative-method}
\begin{equation*}
	\bm{x} = \bm{x} + M^{-1}(\bm b - A \bm{x}).
\end{equation*}
Transforming this equation again into a system of linear equations yields
\begin{equation*}
	M^{-1} A \bm{x} = M^{-1}\bm b,
\end{equation*}
which means that each fixed-point $\bm{x}$ represents a solution of the preconditioned linear system~\eqref{eq:general-preconditioned-system-of-linear-equations} and, hence, also of the original system~\eqref{eq:general-system-of-linear-equations}.
If we assume that $A$ is a square, nonsingular matrix, the solution of each system of linear equations with $A$ as its coefficient matrix is unique and, therefore, it must be true that $\bm{x} = \bm{x}^{*}$.
After we have ensured that in case a stationary iterative method has converged to a fixed-point $\bm{x}$, it is equal to the correct solution of the linear system the method aims to solve.
However, the question remains to be answered under which conditions a sequence of the form of Equation~\eqref{eq:general-stationary-iterative-method} converges to a fixed point and how many iteration must be performed to achieve this goal.
For this purpose, we, again, reformulate Equation~\eqref{eq:general-stationary-iterative-method} to obtain
\begin{equation}
	\bm{x}^{(i+1)} = (I - M^{-1} A) \bm{x}^{(i)} + M^{-1}\bm b,
\end{equation}
as an alternative formulation.
Within this equation can now set $\bm{x}^{i} = \bm{x}^{0}$ 
\begin{equation}
	\bm{x}^{(1)} = (I - M^{-1} A) \bm{x}^{(0)} + M^{-1}\bm b,
\end{equation}
and expand this sequence to the two-iteration series
\begin{equation}
	\bm{x}^{(2)} = (I - M^{-1} A)^2 \bm{x}^{(0)} + (2I - M^{-1} A)M^{-1} \bm{b}.
\end{equation}
By continuing this recursively for an arbitrary number of $k$ times, we see that the resulting equation will always be of the form
\begin{equation}
	\bm{x}^{(k)} = (I - M^{-1} A)^k \bm{x}^{(0)} + N^{(k)}\bm{b},
	\label{eq:stationary-iterative-method-series}
\end{equation}
which means that we can always separate the term $N^{(k)}\bm{b}$ from the rest of the equation.
Note that here the superscript $G^k$ means that we have the $k$th power of $G$, while $N^{(k)}$ means that the matrix has been obtained through $k$ recursive substitutions of the respective iterate $\bm{x}^{(i)}$.
Since we have obtained Equation~\eqref{eq:stationary-iterative-method-series} from our original formulation it still holds true that $\bm{x}^{*}$ is a fixed point of this sequence and, thus, subtracting by $\bm{x}^{*}$ yields
\begin{equation}
	\bm{x}^{(k)} - \bm{x}^{*} = (\underbrace{I - M^{-1} A}_{G})^k (\bm{x}^{(0)} - \bm{x}^{*}).
	\label{eq:iteration-matrix-sequence}
\end{equation}
Here, $G = I - M^{-1} A$ is the so-called \emph{iteration matrix} of the given stationary iterative method.
Before we reason about convergence of this sequence, we introduce the spectral radius 
\begin{equation}
	\rho (A)=\max \limits_{1 \leq k \leq n} |\lambda _{k}|,
\end{equation}
where $A$ is a matrix of rank $n$ with the eigenvalues $\lambda_{1}, \dots, \lambda_{n}$.
Based on this definition we can then state the following theorem:
\begin{theorem}
$\lim \limits_{k \to  \infty} G^k = 0$ if and only if $\rho(G) < 1$.
\label{theorem:general-convergence-result}
\end{theorem}
%TODO potentially prove theorem

While Theorem~\ref{theorem:general-convergence-result} provides an answer whether the sequence~\eqref{eq:iteration-matrix-sequence} converges to zero, we still do not have any knowledge about the speed of this process.
For this purpose, we introduce the general \emph{convergence factor} $\rho$ of a sequence of the form of Equation~\eqref{eq:iteration-matrix-sequence} as
\begin{equation}
	\rho = \lim \limits_{k \to  \infty} \left( \max \limits_{x^{(0)} \in \mathbb{R}^{n}} \frac{\norm{\bm{x}^{(k)} - \bm{x}^{*}}}{\norm{\bm{x}^{(0)} - \bm{x}^{*}}} \right)^{\frac{1}{k}}.
\end{equation} 
Furthermore, the \emph{convergence rate} $\tau$ is defined as the natural logarithm of the inverse of the convergence factor
\begin{equation}
	\tau = -\ln \rho
\end{equation}
\begin{theorem}
	$\rho = \rho(G)$.
	\label{theorem:convergence-factor}
\end{theorem}
%TODO potentially prove theorem
\begin{proof}
	For a proof of Theorem~\ref{theorem:general-convergence-result} and~\ref{theorem:convergence-factor} the reader is referred to ~\cite{varga1962iterative,saad2003iterative}.
\end{proof}
As a result of Theorem~\ref{theorem:convergence-factor} we can state that the spectral radius $\rho(G)$ gives a lower limit for the speed of convergence of every stationary iterative method with an iteration matrix $G = I - M^{-1} A$ that is independent of the choice of the initial vector $\bm{x}^{(0)}$.
Since stationary iterative methods are all based on the simple formulation in Equation~\eqref{eq:general-stationary-iterative-method}, they are both easy to implement and analyze.
The application of these methods alone, however, leads to slow convergence in solving many PDE-based problems~\cite{briggs2000multigrid}.
While many other iterative methods for solving the systems of linear equations arising from the discretization of PDEs have been formulated~\cite{saad2003iterative}, the main focus of this thesis is on multigrid method, which will be discussed in the following section.


