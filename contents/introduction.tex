Many of the most fundamental laws of nature can be formulated as partial differential equations (PDEs).
Since the invention of modern computers, great efforts have been made to develop efficient frameworks and programming languages for solving these equations.
As a result of this effort, computer simulations nowadays represent an essential tool for researchers and engineers.
However, leveraging the power of simulation-based methods, in many cases, necessitates the use of PDE solvers that achieve the highest possible degree of efficiency.
This task often not only requires a great deal of expertise and domain knowledge that only a limited group of mathematical experts possesses, but often also a lot of effort needs to be invested for its accomplishment.
All this makes the manual design and implementation of efficient PDE solvers a difficult and labor-intensive endeavor.
The automation of manual labor has always been one of the greatest incentives of the technical progress since the first industrial revolution.
However, in contrast to previous technological advancements, which were mostly concerned with freeing people from physical labor, the development of computing devices with ever-increasing power and speed has led to a point where the automation even of challenging cognitive tasks has started to come into reach.
%In recent decades, this has enabled the automation of increasingly challenging cognitive tasks.
Artificial intelligence (AI) methods have demonstrated super-human performance in numerous applications, such as image processing~\cite{krizhevsky2017imagenet}, game playing~\cite{schrittwieser2020mastering,reed2022generalist}, and natural language processing~\cite{vaswani2017attention}.
Alongside the widespread success of AI methods, techniques for automated algorithm design have achieved breakthroughs in a number of cases, such as the design of SAT-solvers~\cite{khudabukhsh2016satenstein} and mixed-integer programming~\cite{hutter2010automated}.
In general, methods for automated algorithm design can be classified into \emph{top-down}, and \emph{bottom-up} approaches.
Top-down approaches, often also called algorithm configuration methods, aim to represent an algorithm design space as a finite list of global parameters.
Finding an optimal algorithm design thus corresponds to solving a combinatorial optimization problem, sometimes including continuous parameters, which can be tackled using classical black-box optimizers like evolutionary algorithms~\cite{back1996evolutionary} and bayesian optimization~\cite{frazier2018tutorial}.
However, a severe limitation of these approaches is that they do not allow modifying individual steps of an algorithm unless each of them is represented as a distinct global parameter.
Bottom-up design methods aim to overcome these limitations by considering the construction of an algorithm from its fundamental building blocks.
An algorithm is essentially a list of statements written in a formal language.
If we consider this language to be a programming language, the formulation of an algorithm is nothing else than writing a program in that language.
Therefore, the automated design of an optimal algorithm can be treated as a program synthesis task, which gives us the same degree of flexibility available in modern programming languages.
On the downside, bottom-up algorithm design requires the formulation of a programming language for expressing the individual steps of an algorithm as formal statements.
Furthermore, the greater flexibility of bottom-up algorithm design means that the number of different algorithms considered is significantly larger than in the case of top-down methods.
While all these difficulties impede the widespread application of bottom-up algorithm design, recent works in the area of machine learning~\cite{real2020automl,co2021evolving,zz_ne1,zz_ne2} and matrix multiplication~\cite{fawzi2022discovering} demonstrate that it has the potential to discover completely novel algorithms in different domains, a feat that is not possible with classical top-down approaches.
Until recently, the application of these methods to the design of PDE solvers has been a largely unexplored research field.
This thesis aims to change this situation by introducing a novel framework for the automated design of multigrid methods, a class of numerical methods that offer the potential to solve many PDEs in an asymptotically optimal way.
Since multigrid methods can only achieve this property through the correct choice and composition of their individual operations, for many PDEs, the design of an optimal or even functioning multigrid-based solver is an open problem~\cite{trottenberg2000multigrid,ernst2012difficult}.
As a remedy, the works by Oosterlee et al.~\cite{oosterlee2003genetic}, Thekale et al.~\cite{thekale2010optimizing}, and Brown et al.~\cite{brown2021tuning} represent a first step towards the automated design of these methods.
However, the authors of these papers only consider a limited configuration space, which makes the discovery of completely novel algorithmic patterns impossible.
To overcome the inherent limitations of this approach, this thesis considers the task of constructing an optimal multigrid method from its basic components as a program synthesis task.
By levering the power of evolutionary computation, it will be demonstrated that this approach enables the discovery of unique sequences of multigrid operations that can yield faster solvers than classical multigrid cycles.
In the following, a basic understanding of the fundamental theory of multigrid methods, formal languages, and evolutionary program synthesis will be established first to give the reader the necessary background for the main part of this thesis.