
@InProceedings{	  amdahl1967validity,
  location	= {Atlantic City, New Jersey},
  title		= {Validity of the single processor approach to achieving
		  large scale computing capabilities},
  url		= {http://portal.acm.org/citation.cfm?doid=1465482.1465560},
  doi		= {10.1145/1465482.1465560},
  eventtitle	= {the April 18-20, 1967, spring joint computer conference},
  pages		= {483},
  booktitle	= {Proceedings of the April 18-20, 1967, spring joint
		  computer conference on - {AFIPS} '67 (Spring)},
  publisher	= {{ACM} Press},
  author	= {Amdahl, Gene M.},
  urldate	= {2022-12-10},
  date		= {1967},
  langid	= {english}
}

@Book{		  ames2014numerical,
  title		= {Numerical Methods for Partial Differential Equations},
  author	= {Ames, William},
  date		= {1992}
}

@InCollection{	  antoine2016integral,
  title		= {Integral Equations and Iterative Schemes for Acoustic
		  Scattering Problems},
  url		= {https://hal.archives-ouvertes.fr/hal-00591456},
  booktitle	= {Numerical Methods for Acoustics Problems},
  publisher	= {Saxe-Coburg Editors},
  author	= {Antoine, Xavier and Darbas, Marion},
  date		= {2016}
}

@Article{	  avnat2022recursive,
  title		= {On the Recursive Structure of Multigrid Cycles},
  issn		= {1064-8275, 1095-7197},
  url		= {https://epubs.siam.org/doi/10.1137/21M1433502},
  doi		= {10.1137/21M1433502},
  pages		= {S103--S126},
  journaltitle	= {{SIAM} Journal on Scientific Computing},
  shortjournal	= {{SIAM} J. Sci. Comput.},
  author	= {Avnat, Or and Yavneh, Irad},
  urldate	= {2022-12-10},
  date		= {2022-07-13},
  langid	= {english}
}

@Book{		  back1996evolutionary,
  title		= {Evolutionary Algorithms in Theory and Practice: Evolution
		  Strategies, Evolutionary Programming, Genetic Algorithms},
  url		= {https://academic.oup.com/book/40791},
  shorttitle	= {Evolutionary Algorithms in Theory and Practice},
  abstract	= {This book presents a unified view of evolutionary
		  algorithms: the exciting new probabilistic search tools
		  inspired by biological models that have immense potential
		  as practical problem-solvers in a wide variety of settings,
		  academic, commercial, and industrial. In this work, the
		  author compares the three most prominent representatives of
		  evolutionary algorithms: genetic algorithms, evolution
		  strategies, and evolutionary programming. The algorithms
		  are presented within a unified framework, thereby
		  clarifying the similarities and differences of these
		  methods. The author also presents new results regarding the
		  role of mutation and selection in genetic algorithms,
		  showing how mutation seems to be much more important for
		  the performance of genetic algorithms than usually assumed.
		  The interaction of selection and mutation, and the impact
		  of the binary code are further topics of interest. Some of
		  the theoretical results are also confirmed by performing an
		  experiment in meta-evolution on a parallel computer. The
		  meta-algorithm used in this experiment combines components
		  from evolution strategies and genetic algorithms to yield a
		  hybrid capable of handling mixed integer optimization
		  problems. As a detailed description of the algorithms, with
		  practical guidelines for usage and implementation, this
		  work will interest a wide range of researchers in computer
		  science and engineering disciplines, as well as graduate
		  students in these fields.},
  publisher	= {Oxford University Press},
  author	= {Bäck, Thomas},
  urldate	= {2022-12-02},
  date		= {1996-02-15},
  langid	= {english},
  doi		= {10.1093/oso/9780195099713.001.0001}
}

@collection{back1997handbook,
	location = {Bristol ; Philadelphia : New York},
	title = {Handbook of evolutionary computation},
	pagetotal = {1},
	publisher = {Institute of Physics Pub. ; Oxford University Press},
	date = {1997},
}

@Book{		  brameier2007linear,
  location	= {Boston, {MA}},
  title		= {Linear Genetic Programming},
  volume	= {1},
  url		= {http://link.springer.com/10.1007/978-0-387-31030-5},
  series	= {Genetic and Evolutionary Computation},
  publisher	= {Springer {US}},
  author	= {Banzhaf, Wolfgang and Brameier, Markus},
  urldate	= {2022-12-02},
  date		= {2007},
  langid	= {english},
  doi		= {10.1007/978-0-387-31030-5}
}

@Article{	  benzi2002preconditioning,
  title		= {Preconditioning Techniques for Large Linear Systems: A
		  Survey},
  volume	= {182},
  issn		= {00219991},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0021999102971767},
  doi		= {10.1006/jcph.2002.7176},
  shorttitle	= {Preconditioning Techniques for Large Linear Systems},
  pages		= {418--477},
  number	= {2},
  journaltitle	= {Journal of Computational Physics},
  shortjournal	= {Journal of Computational Physics},
  author	= {Benzi, Michele},
  urldate	= {2022-12-10},
  date		= {2002-11},
  langid	= {english}
}

@Article{	  benzi2005numerical,
  title		= {Numerical solution of saddle point problems},
  volume	= {14},
  issn		= {0962-4929, 1474-0508},
  url		= {https://www.cambridge.org/core/product/identifier/S0962492904000212/type/journal_article},
  doi		= {10.1017/S0962492904000212},
  abstract	= {Large linear systems of saddle point type arise in a wide
		  variety of applications throughout computational science
		  and engineering. Due to their indefiniteness and often poor
		  spectral properties, such linear systems represent a
		  significant challenge for solver developers. In recent
		  years there has been a surge of interest in saddle point
		  problems, and numerous solution techniques have been
		  proposed for this type of system. The aim of this paper is
		  to present and discuss a large selection of solution
		  methods for linear systems in saddle point form, with an
		  emphasis on iterative methods for large and sparse
		  problems.},
  pages		= {1--137},
  journaltitle	= {Acta Numerica},
  shortjournal	= {Acta Numerica},
  author	= {Benzi, Michele and Golub, Gene H. and Liesen, Jörg},
  urldate	= {2022-12-10},
  date		= {2005-05},
  langid	= {english}
}

@Article{	  beyer2002evolution,
  title		= {Evolution strategies - A comprehensive introduction},
  volume	= {1},
  issn		= {15677818},
  url		= {http://link.springer.com/10.1023/A:1015059928466},
  doi		= {10.1023/A:1015059928466},
  pages		= {3--52},
  number	= {1},
  journaltitle	= {Natural Computing},
  author	= {Beyer, Hans-Georg and Schwefel, Hans-Paul},
  urldate	= {2022-12-02},
  date		= {2002}
}

@InProceedings{	  billette20052004,
  location	= {Madrid, Spain,},
  title		= {The 2004 {BP} Velocity Benchmark},
  url		= {https://www.earthdoc.org/content/papers/10.3997/2214-4609-pdb.1.B035},
  doi		= {10.3997/2214-4609-pdb.1.B035},
  eventtitle	= {67th {EAGE} Conference \& Exhibition},
  booktitle	= {67th {EAGE} Conference \& Exhibition},
  publisher	= {European Association of Geoscientists \& Engineers},
  author	= {Billette, F.J. and Brandsberg-Dahl, S.},
  urldate	= {2022-12-10},
  date		= {2005},
  langid	= {english}
}

@Article{	  bolten2018fourier,
  title		= {Fourier Analysis of Periodic Stencils in Multigrid
		  Methods},
  volume	= {40},
  issn		= {1064-8275, 1095-7197},
  url		= {https://epubs.siam.org/doi/10.1137/16M1073959},
  doi		= {10.1137/16M1073959},
  pages		= {A1642--A1668},
  number	= {3},
  journaltitle	= {{SIAM} Journal on Scientific Computing},
  shortjournal	= {{SIAM} J. Sci. Comput.},
  author	= {Bolten, M. and Rittich, H.},
  urldate	= {2022-12-02},
  date		= {2018-01},
  langid	= {english}
}

@InCollection{	  garcia2006initialization,
  location	= {London},
  title		= {Initialization Method for Grammar-Guided Genetic
		  Programming},
  url		= {http://link.springer.com/10.1007/978-1-84628-663-6_3},
  pages		= {32--44},
  booktitle	= {Research and Development in Intelligent Systems {XXIII}},
  publisher	= {Springer London},
  author	= {García-Arnau, M. and Manrique, D. and Ríos, J. and
		  Rodríguez-Patón, A.},
  urldate	= {2022-12-02},
  date		= {2007},
  langid	= {english},
  doi		= {10.1007/978-1-84628-663-6_3}
}

@Article{	  brandt1977multi,
  title		= {Multi-level adaptive solutions to boundary-value
		  problems},
  volume	= {31},
  issn		= {0025-5718, 1088-6842},
  url		= {https://www.ams.org/mcom/1977-31-138/S0025-5718-1977-0431719-X/},
  doi		= {10.1090/S0025-5718-1977-0431719-X},
  abstract	= {The boundary-value problem is discretized on several grids
		  (or finite-element spaces) of widely different mesh sizes.
		  Interactions between these levels enable us (i) to solve
		  the possibly nonlinear system of n discrete equations in
		  
		  O ( n )
		  
		  O(n)
		  
		  operations (40 n additions and shifts for Poisson
		  problems); (ii) to conveniently adapt the discretization
		  (the local mesh size, local order of approximation, etc.)
		  to the evolving solution in a nearly optimal way, obtaining
		  "
		  
		  ∞
		  
		  {\textbackslash}infty
		  
		  -order" approximations and low n , even when singularities
		  are present. General theoretical analysis of the numerical
		  process. Numerical experiments with linear and nonlinear,
		  elliptic and mixed-type (transonic flow) problems-confirm
		  theoretical predictions. Similar techniques for
		  initial-value problems are briefly discussed.},
  pages		= {333--390},
  number	= {138},
  journaltitle	= {Mathematics of Computation},
  shortjournal	= {Math. Comp.},
  author	= {Brandt, Achi},
  urldate	= {2022-12-10},
  date		= {1977},
  langid	= {english}
}

@Book{		  briggs2000multigrid,
  edition	= {Second},
  title		= {A Multigrid Tutorial, Second Edition},
  url		= {http://epubs.siam.org/doi/book/10.1137/1.9780898719505},
  publisher	= {Society for Industrial and Applied Mathematics},
  author	= {Briggs, William L. and Henson, Van Emden and {McCormick},
		  Steve F.},
  urldate	= {2022-12-01},
  date		= {2000-01},
  langid	= {english},
  doi		= {10.1137/1.9780898719505}
}

@InProceedings{	  brown2020language,
  title		= {Language models are few-shot learners},
  volume	= {33},
  url		= {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  pages		= {1877--1901},
  booktitle	= {Advances in Neural Information Processing Systems},
  author	= {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah,
		  Melanie and Kaplan, Jared D and Dhariwal, Prafulla and
		  Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish
		  and Askell, Amanda and {others}},
  date		= {2020}
}

@Article{	  brown2021tuning,
  title		= {Tuning Multigrid Methods with Robust Optimization and
		  Local Fourier Analysis},
  volume	= {43},
  issn		= {1064-8275, 1095-7197},
  url		= {https://epubs.siam.org/doi/10.1137/19M1308669},
  doi		= {10.1137/19M1308669},
  pages		= {A109--A138},
  number	= {1},
  journaltitle	= {{SIAM} Journal on Scientific Computing},
  shortjournal	= {{SIAM} J. Sci. Comput.},
  author	= {Brown, Jed and He, Yunhui and {MacLachlan}, Scott and
		  Menickelly, Matt and Wild, Stefan M.},
  urldate	= {2022-12-10},
  date		= {2021-01},
  langid	= {english}
}

@InCollection{	  schmitt2016systems,
  location	= {Cham},
  title		= {Systems of Partial Differential Equations in {ExaSlang}},
  volume	= {113},
  url		= {http://link.springer.com/10.1007/978-3-319-40528-5_3},
  pages		= {47--67},
  booktitle	= {Software for Exascale Computing - {SPPEXA} 2013-2015},
  publisher	= {Springer International Publishing},
  author	= {Schmitt, Christian and Kuckuk, Sebastian and Hannig, Frank
		  and Teich, Jürgen and Köstler, Harald and Rüde, Ulrich
		  and Lengauer, Christian},
		  Wolfgang E.},
  urldate	= {2022-12-10},
  date		= {2016},
  doi		= {10.1007/978-3-319-40528-5_3},
  note		= {Series Title: Lecture Notes in Computational Science and
		  Engineering}
}

@InCollection{	  lengauer2020exastencils,
  location	= {Cham},
  title		= {{ExaStencils}: Advanced Multigrid Solver Generation},
  volume	= {136},
  url		= {http://link.springer.com/10.1007/978-3-030-47956-5_14},
  shorttitle	= {{ExaStencils}},
  pages		= {405--452},
  booktitle	= {Software for Exascale Computing - {SPPEXA} 2016-2019},
  publisher	= {Springer International Publishing},
  author	= {Lengauer, Christian and Apel, Sven and Bolten, Matthias
		  and Chiba, Shigeru and Rüde, Ulrich and Teich, Jürgen and
		  Größlinger, Armin and Hannig, Frank and Köstler, Harald
		  and Claus, Lisa and Grebhahn, Alexander and Groth, Stefan
		  and Kronawitter, Stefan and Kuckuk, Sebastian and Rittich,
		  Hannah and Schmitt, Christian and Schmitt, Jonas},
		  Benjamin and Neumann, Philipp and Nagel, Wolfgang E.},
  urldate	= {2022-12-02},
  date		= {2020},
  langid	= {english},
  doi		= {10.1007/978-3-030-47956-5_14},
  note		= {Series Title: Lecture Notes in Computational Science and
		  Engineering}
}

@Book{		  chari2000numerical,
  location	= {San Diego},
  title		= {Numerical methods in electromagnetism},
  series	= {Academic Press series in electromagnetism},
  pagetotal	= {767},
  publisher	= {Academic Press},
  author	= {Chari, M. V. K. and Salon, S. J.},
  date		= {2000}
}

@Misc{		  co2021evolving,
  title		= {Evolving Reinforcement Learning Algorithms},
  url		= {http://arxiv.org/abs/2101.03958},
  abstract	= {We propose a method for meta-learning reinforcement
		  learning algorithms by searching over the space of
		  computational graphs which compute the loss function for a
		  value-based model-free {RL} agent to optimize. The learned
		  algorithms are domain-agnostic and can generalize to new
		  environments not seen during training. Our method can both
		  learn from scratch and bootstrap off known existing
		  algorithms, like {DQN}, enabling interpretable
		  modifications which improve performance. Learning from
		  scratch on simple classical control and gridworld tasks,
		  our method rediscovers the temporal-difference ({TD})
		  algorithm. Bootstrapped from {DQN}, we highlight two
		  learned algorithms which obtain good generalization
		  performance over other classical control tasks, gridworld
		  type tasks, and Atari games. The analysis of the learned
		  algorithm behavior shows resemblance to recently proposed
		  {RL} algorithms that address overestimation in value-based
		  methods.},
  number	= {{arXiv}:2101.03958},
  publisher	= {{arXiv}},
  author	= {Co-Reyes, John D. and Miao, Yingjie and Peng, Daiyi and
		  Real, Esteban and Levine, Sergey and Le, Quoc V. and Lee,
		  Honglak and Faust, Aleksandra},
  urldate	= {2022-12-02},
  date		= {2022-11-10},
  eprinttype	= {arxiv},
  eprint	= {2101.03958 [cs]}
}

@Article{	  cocquet2017shift,
  title		= {How Large a Shift is Needed in the Shifted Helmholtz
		  Preconditioner for its Effective Inversion by Multigrid?},
  volume	= {39},
  issn		= {1064-8275, 1095-7197},
  url		= {https://epubs.siam.org/doi/10.1137/15M102085X},
  doi		= {10.1137/15M102085X},
  pages		= {A438--A478},
  number	= {2},
  journaltitle	= {{SIAM} Journal on Scientific Computing},
  shortjournal	= {{SIAM} J. Sci. Comput.},
  author	= {Cocquet, Pierre-Henri and Gander, Martin J.},
  urldate	= {2022-12-02},
  date		= {2017-01},
  langid	= {english}
}

@Book{		  coello2007evolutionary,
  location	= {Boston, {MA}},
  title		= {Evolutionary Algorithms for Solving Multi-Objective
		  Problems},
  volume	= {5},
  url		= {http://link.springer.com/10.1007/978-0-387-36797-2},
  series	= {Genetic and Evolutionary Computation Series},
  publisher	= {Springer {US}},
  author	= {Coello, Carlos A. Coello and Lamont, Gary B. and Van
		  Veldhuizen, David A.},
  urldate	= {2022-12-02},
  date		= {2007},
  langid	= {english},
  doi		= {10.1007/978-0-387-36797-2}
}

@InCollection{	  coello2011sequential,
  location	= {Berlin, Heidelberg},
  title		= {Sequential Model-Based Optimization for General Algorithm
		  Configuration},
  volume	= {6683},
  url		= {http://link.springer.com/10.1007/978-3-642-25566-3_40},
  pages		= {507--523},
  booktitle	= {Learning and Intelligent Optimization},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Hutter, Frank and Hoos, Holger H. and Leyton-Brown,
		  Kevin},
  urldate	= {2022-12-10},
  date		= {2011},
  doi		= {10.1007/978-3-642-25566-3_40},
  note		= {Series Title: Lecture Notes in Computer Science}
}

@Article{	  cools2013analysis,
  title		= {Local Fourier analysis of the complex shifted Laplacian
		  preconditioner for Helmholtz problems},
  volume	= {20},
  issn		= {10705325},
  url		= {https://onlinelibrary.wiley.com/doi/10.1002/nla.1881},
  doi		= {10.1002/nla.1881},
  shorttitle	= {Local Fourier analysis of the complex shifted Laplacian
		  preconditioner for Helmholtz problems},
  pages		= {575--597},
  number	= {4},
  journaltitle	= {Numerical Linear Algebra with Applications},
  shortjournal	= {Numer. Linear Algebra Appl.},
  author	= {Cools, Siegfried and Vanroose, Wim},
  urldate	= {2022-12-10},
  date		= {2013-08},
  langid	= {english}
}

@Article{	  couchet2007crossover,
  title		= {Crossover and mutation operators for grammar-guided
		  genetic programming},
  volume	= {11},
  issn		= {1432-7643, 1433-7479},
  url		= {https://link.springer.com/10.1007/s00500-006-0144-9},
  doi		= {10.1007/s00500-006-0144-9},
  pages		= {943--955},
  number	= {10},
  journaltitle	= {Soft Computing},
  shortjournal	= {Soft Comput},
  author	= {Couchet, Jorge and Manrique, Daniel and Ríos, Juan and
		  Rodríguez-Patón, Alfonso},
  urldate	= {2022-12-01},
  date		= {2007-08},
  langid	= {english}
}

@Article{	  dalcin2021mpi4py,
  title		= {mpi4py: Status Update After 12 Years of Development},
  volume	= {23},
  issn		= {1521-9615, 1558-366X},
  url		= {https://ieeexplore.ieee.org/document/9439927/},
  doi		= {10.1109/MCSE.2021.3083216},
  shorttitle	= {mpi4py},
  pages		= {47--54},
  number	= {4},
  journaltitle	= {Computing in Science \& Engineering},
  shortjournal	= {Comput. Sci. Eng.},
  author	= {Dalcin, Lisandro and Fang, Yao-Lung L.},
  urldate	= {2022-12-10},
  date		= {2021-07-01}
}

@Article{	  deb2002fast,
  title		= {A fast and elitist multiobjective genetic algorithm:
		  {NSGA}-{II}},
  volume	= {6},
  issn		= {1089778X},
  url		= {http://ieeexplore.ieee.org/document/996017/},
  doi		= {10.1109/4235.996017},
  shorttitle	= {A fast and elitist multiobjective genetic algorithm},
  pages		= {182--197},
  number	= {2},
  journaltitle	= {{IEEE} Transactions on Evolutionary Computation},
  shortjournal	= {{IEEE} Trans. Evol. Computat.},
  author	= {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  urldate	= {2022-12-01},
  date		= {2002-04}
}

@Article{	  deb2013evolutionary,
  title		= {An Evolutionary Many-Objective Optimization Algorithm
		  Using Reference-Point-Based Nondominated Sorting Approach,
		  Part I: Solving Problems With Box Constraints},
  volume	= {18},
  issn		= {1089-778X, 1089-778X, 1941-0026},
  url		= {http://ieeexplore.ieee.org/document/6600851/},
  doi		= {10.1109/TEVC.2013.2281535},
  shorttitle	= {An Evolutionary Many-Objective Optimization Algorithm
		  Using Reference-Point-Based Nondominated Sorting Approach,
		  Part I},
  pages		= {577--601},
  number	= {4},
  journaltitle	= {{IEEE} Transactions on Evolutionary Computation},
  shortjournal	= {{IEEE} Trans. Evol. Computat.},
  author	= {Deb, Kalyanmoy and Jain, Himanshu},
  urldate	= {2022-12-01},
  date		= {2014-08}
}

@Article{	  demmel2007fast,
  title		= {Fast linear algebra is stable},
  volume	= {108},
  issn		= {0029-599X, 0945-3245},
  url		= {http://link.springer.com/10.1007/s00211-007-0114-x},
  doi		= {10.1007/s00211-007-0114-x},
  pages		= {59--91},
  number	= {1},
  journaltitle	= {Numerische Mathematik},
  shortjournal	= {Numer. Math.},
  author	= {Demmel, James and Dumitriu, Ioana and Holtz, Olga},
  urldate	= {2022-12-02},
  date		= {2007-10-23},
  langid	= {english}
}

@Article{	  dendy1982black,
  title		= {Black box multigrid},
  volume	= {48},
  issn		= {00219991},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/0021999182900572},
  doi		= {10.1016/0021-9991(82)90057-2},
  pages		= {366--386},
  number	= {3},
  journaltitle	= {Journal of Computational Physics},
  shortjournal	= {Journal of Computational Physics},
  author	= {Dendy, J.E},
  urldate	= {2022-12-01},
  date		= {1982-12},
  langid	= {english}
}

@Article{	  elsken2019neural,
  title		= {Neural architecture search: A survey},
  volume	= {20},
  url		= {http://jmlr.org/papers/v20/18-598.html},
  pages		= {1997--2017},
  number	= {1},
  journaltitle	= {The Journal of Machine Learning Research},
  author	= {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  date		= {2019},
  note		= {Publisher: {JMLR}. org}
}

@Article{	  erlangga2004preconditioner,
  title		= {On a class of preconditioners for solving the Helmholtz
		  equation},
  volume	= {50},
  issn		= {01689274},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0168927404000091},
  doi		= {10.1016/j.apnum.2004.01.009},
  pages		= {409--425},
  number	= {3},
  journaltitle	= {Applied Numerical Mathematics},
  shortjournal	= {Applied Numerical Mathematics},
  author	= {Erlangga, Y.A and Vuik, C and Oosterlee, C.W},
  urldate	= {2022-12-10},
  date		= {2004-09},
  langid	= {english}
}

@Article{	  erlangga2006comparison,
  title		= {Comparison of multigrid and incomplete {LU}
		  shifted-Laplace preconditioners for the inhomogeneous
		  Helmholtz equation},
  volume	= {56},
  issn		= {01689274},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S016892740500108X},
  doi		= {10.1016/j.apnum.2005.04.039},
  pages		= {648--666},
  number	= {5},
  journaltitle	= {Applied Numerical Mathematics},
  shortjournal	= {Applied Numerical Mathematics},
  author	= {Erlangga, Y.A. and Vuik, C. and Oosterlee, C.W.},
  urldate	= {2022-12-01},
  date		= {2006-05},
  langid	= {english}
}

@Article{	  erlangga2006multigrid,
  title		= {A Novel Multigrid Based Preconditioner For Heterogeneous
		  Helmholtz Problems},
  volume	= {27},
  issn		= {1064-8275, 1095-7197},
  url		= {http://epubs.siam.org/doi/10.1137/040615195},
  doi		= {10.1137/040615195},
  pages		= {1471--1492},
  number	= {4},
  journaltitle	= {{SIAM} Journal on Scientific Computing},
  shortjournal	= {{SIAM} J. Sci. Comput.},
  author	= {Erlangga, Y. A. and Oosterlee, C. W. and Vuik, C.},
  urldate	= {2022-12-01},
  date		= {2006-01},
  langid	= {english}
}

@Article{	  erlangga2008advances,
  title		= {Advances in Iterative Methods and Preconditioners for the
		  Helmholtz Equation},
  volume	= {15},
  issn		= {1134-3060, 1886-1784},
  url		= {http://link.springer.com/10.1007/s11831-007-9013-7},
  doi		= {10.1007/s11831-007-9013-7},
  pages		= {37--66},
  number	= {1},
  journaltitle	= {Archives of Computational Methods in Engineering},
  shortjournal	= {Arch Computat Methods Eng},
  author	= {Erlangga, Yogi A.},
  urldate	= {2022-12-02},
  date		= {2008-03},
  langid	= {english}
}

@Article{	  erlangga2008multilevel,
  title		= {On a multilevel Krylov method for the Helmholtz equation
		  preconditioned by shifted Laplacian},
  volume	= {31},
  pages		= {3},
  number	= {403},
  journaltitle	= {Electronic Transactions on Numerical Analysis},
  author	= {Erlangga, Yogi A. and Nabben, Reinhard},
  date		= {2008},
  note		= {Publisher: Institute of Computational Mathematics}
}

@Book{		  evans2010partial,
  location	= {Providence, R.I},
  edition	= {2nd ed},
  title		= {Partial differential equations},
  series	= {Graduate studies in mathematics},
  abstract	= {"This is the second edition of the now definitive text on
		  partial differential equations ({PDE}). It offers a
		  comprehensive survey of modern techniques in the
		  theoretical study of {PDE} with particular emphasis on
		  nonlinear equations. Its wide scope and clear exposition
		  make it a great text for a graduate course in {PDE}. For
		  this edition, the author has made numerous changes,
		  including: a new chapter on nonlinear wave equations, more
		  than 80 new exercises, several new sections, and a
		  significantly expanded bibliography."--Publisher's
		  description},
  pagetotal	= {749},
  number	= {v. 19},
  publisher	= {American Mathematical Society},
  author	= {Evans, Lawrence C.},
  date		= {2010},
  note		= {{OCLC}: ocn465190110}
}

@InCollection{	  eymard2000finite,
  title		= {Finite volume methods},
  volume	= {7},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S1570865900070058},
  pages		= {713--1018},
  booktitle	= {Handbook of Numerical Analysis},
  publisher	= {Elsevier},
  author	= {Eymard, Robert and Gallouët, Thierry and Herbin,
		  Raphaèle},
  urldate	= {2022-12-02},
  date		= {2000},
  langid	= {english},
  doi		= {10.1016/S1570-8659(00)07005-8}
}

@InProceedings{	  fanaskov2021neural,
  location	= {Shenzhen, China},
  title		= {Neural Multigrid Architectures},
  url		= {https://ieeexplore.ieee.org/document/9533736/},
  doi		= {10.1109/IJCNN52387.2021.9533736},
  eventtitle	= {2021 International Joint Conference on Neural Networks
		  ({IJCNN})},
  pages		= {1--8},
  booktitle	= {2021 International Joint Conference on Neural Networks
		  ({IJCNN})},
  publisher	= {{IEEE}},
  author	= {Fanaskov, Vladimir},
  urldate	= {2022-12-10},
  date		= {2021-07-18}
}

@Article{	  fawzi2022discovering,
  title		= {Discovering faster matrix multiplication algorithms with
		  reinforcement learning},
  volume	= {610},
  issn		= {0028-0836, 1476-4687},
  url		= {https://www.nature.com/articles/s41586-022-05172-4},
  doi		= {10.1038/s41586-022-05172-4},
  abstract	= {Abstract
		  
		  Improving the efficiency of algorithms for fundamental
		  computations can have a widespread impact, as it can affect
		  the overall speed of a large amount of computations. Matrix
		  multiplication is one such primitive task, occurring in
		  many systems—from neural networks to scientific computing
		  routines. The automatic discovery of algorithms using
		  machine learning offers the prospect of reaching beyond
		  human intuition and outperforming the current best
		  human-designed algorithms. However, automating the
		  algorithm discovery procedure is intricate, as the space of
		  possible algorithms is enormous. Here we report a deep
		  reinforcement learning approach based on {AlphaZero} 1 for
		  discovering efficient and provably correct algorithms for
		  the multiplication of arbitrary matrices. Our agent,
		  {AlphaTensor}, is trained to play a single-player game
		  where the objective is finding tensor decompositions within
		  a finite factor space. {AlphaTensor} discovered algorithms
		  that outperform the state-of-the-art complexity for many
		  matrix sizes. Particularly relevant is the case of
		  4 × 4 matrices in a finite field, where
		  {AlphaTensor}’s algorithm improves on Strassen’s
		  two-level algorithm for the first time, to our knowledge,
		  since its discovery 50 years ago 2 . We further showcase
		  the flexibility of {AlphaTensor} through different
		  use-cases: algorithms with state-of-the-art complexity for
		  structured matrix multiplication and improved practical
		  efficiency by optimizing matrix multiplication for runtime
		  on specific hardware. Our results highlight
		  {AlphaTensor}’s ability to accelerate the process of
		  algorithmic discovery on a range of problems, and to
		  optimize for different criteria.},
  pages		= {47--53},
  number	= {7930},
  journaltitle	= {Nature},
  shortjournal	= {Nature},
  author	= {Fawzi, Alhussein and Balog, Matej and Huang, Aja and
		  Hubert, Thomas and Romera-Paredes, Bernardino and
		  Barekatain, Mohammadamin and Novikov, Alexander and R.
		  Ruiz, Francisco J. and Schrittwieser, Julian and Swirszcz,
		  Grzegorz and Silver, David and Hassabis, Demis and Kohli,
		  Pushmeet},
  urldate	= {2022-12-02},
  date		= {2022-10-06},
  langid	= {english}
}

@Article{	  fedorenko1962relaxation,
  title		= {A relaxation method for solving elliptic difference
		  equations},
  volume	= {1},
  issn		= {00415553},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/0041555362900319},
  doi		= {10.1016/0041-5553(62)90031-9},
  pages		= {1092--1096},
  number	= {4},
  journaltitle	= {{USSR} Computational Mathematics and Mathematical
		  Physics},
  shortjournal	= {{USSR} Computational Mathematics and Mathematical
		  Physics},
  author	= {Fedorenko, R.P.},
  urldate	= {2022-12-01},
  date		= {1962-01},
  langid	= {english}
}

@Book{		  folland2020introduction,
  title		= {Introduction to Partial Differential Equations: Second
		  Edition},
  url		= {https://www.degruyter.com/document/doi/10.1515/9780691213033/html},
  shorttitle	= {Introduction to Partial Differential Equations},
  publisher	= {Princeton University Press},
  author	= {Folland, Gerald B.},
  urldate	= {2022-12-02},
  date		= {1976-12-31},
  doi		= {10.1515/9780691213033}
}

@Article{	  rainville2012deap,
  title		= {{DEAP}: Evolutionary Algorithms Made Easy},
  volume	= {13},
  url		= {http://jmlr.org/papers/v13/fortin12a.html},
  pages		= {2171--2175},
  journaltitle	= {Journal of Machine Learning Research},
  author	= {Fortin, Félix-Antoine and Rainville, François-Michel De
		  and Gardner, Marc-André and Parizeau, Marc and Gagné,
		  Christian},
  date		= {2012-07}
}

@InProceedings{	  fortin2013generalizing,
  location	= {Amsterdam, The Netherlands},
  title		= {Generalizing the improved run-time complexity algorithm
		  for non-dominated sorting},
  url		= {http://dl.acm.org/citation.cfm?doid=2463372.2463454},
  doi		= {10.1145/2463372.2463454},
  eventtitle	= {Proceeding of the fifteenth annual conference},
  pages		= {615},
  booktitle	= {Proceeding of the fifteenth annual conference on Genetic
		  and evolutionary computation conference - {GECCO} '13},
  publisher	= {{ACM} Press},
  author	= {Fortin, Félix-Antoine and Grenier, Simon and Parizeau,
		  Marc},
  urldate	= {2022-12-02},
  date		= {2013},
  langid	= {english}
}

@Misc{		  frazier2018tutorial,
  title		= {A Tutorial on Bayesian Optimization},
  url		= {http://arxiv.org/abs/1807.02811},
  abstract	= {Bayesian optimization is an approach to optimizing
		  objective functions that take a long time (minutes or
		  hours) to evaluate. It is best-suited for optimization over
		  continuous domains of less than 20 dimensions, and
		  tolerates stochastic noise in function evaluations. It
		  builds a surrogate for the objective and quantifies the
		  uncertainty in that surrogate using a Bayesian machine
		  learning technique, Gaussian process regression, and then
		  uses an acquisition function defined from this surrogate to
		  decide where to sample. In this tutorial, we describe how
		  Bayesian optimization works, including Gaussian process
		  regression and three common acquisition functions: expected
		  improvement, entropy search, and knowledge gradient. We
		  then discuss more advanced techniques, including running
		  multiple function evaluations in parallel, multi-fidelity
		  and multi-information source optimization,
		  expensive-to-evaluate constraints, random environmental
		  conditions, multi-task Bayesian optimization, and the
		  inclusion of derivative information. We conclude with a
		  discussion of Bayesian optimization software and future
		  research directions in the field. Within our tutorial
		  material we provide a generalization of expected
		  improvement to noisy evaluations, beyond the noise-free
		  setting where it is more commonly applied. This
		  generalization is justified by a formal decision-theoretic
		  argument, standing in contrast to previous ad hoc
		  modifications.},
  number	= {{arXiv}:1807.02811},
  publisher	= {{arXiv}},
  author	= {Frazier, Peter I.},
  urldate	= {2022-12-01},
  date		= {2018-07-08},
  eprinttype	= {arxiv},
  eprint	= {1807.02811 [cs, math, stat]}
}

@Article{	  gander2019class,
  title		= {A Class of Iterative Solvers for the Helmholtz Equation:
		  Factorizations, Sweeping Preconditioners, Source Transfer,
		  Single Layer Potentials, Polarized Traces, and Optimized
		  Schwarz Methods},
  volume	= {61},
  issn		= {0036-1445, 1095-7200},
  url		= {https://epubs.siam.org/doi/10.1137/16M109781X},
  doi		= {10.1137/16M109781X},
  shorttitle	= {A Class of Iterative Solvers for the Helmholtz Equation},
  pages		= {3--76},
  number	= {1},
  journaltitle	= {{SIAM} Review},
  shortjournal	= {{SIAM} Rev.},
  author	= {Gander, Martin J. and Zhang, Hui},
  urldate	= {2022-12-01},
  date		= {2019-01},
  langid	= {english}
}

@InCollection{	  gendreau2019automated,
  location	= {Cham},
  title		= {Automated Design of Metaheuristic Algorithms},
  volume	= {272},
  url		= {http://link.springer.com/10.1007/978-3-319-91086-4_17},
  pages		= {541--579},
  booktitle	= {Handbook of Metaheuristics},
  publisher	= {Springer International Publishing},
  author	= {Stützle, Thomas and López-Ibáñez, Manuel},
  urldate	= {2022-12-01},
  date		= {2019},
  langid	= {english},
  doi		= {10.1007/978-3-319-91086-4_17},
  note		= {Series Title: International Series in Operations Research
		  \& Management Science}
}

@InCollection{	  goldberg1991comparative,
  title		= {A Comparative Analysis of Selection Schemes Used in
		  Genetic Algorithms},
  volume	= {1},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/B9780080506845500082},
  pages		= {69--93},
  booktitle	= {Foundations of Genetic Algorithms},
  publisher	= {Elsevier},
  author	= {Goldberg, David E. and Deb, Kalyanmoy},
  urldate	= {2022-12-01},
  date		= {1991},
  langid	= {english},
  doi		= {10.1016/B978-0-08-050684-5.50008-2}
}

@Article{	  gong2015distributed,
  title		= {Distributed evolutionary algorithms and their models: A
		  survey of the state-of-the-art},
  volume	= {34},
  issn		= {15684946},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S1568494615002987},
  doi		= {10.1016/j.asoc.2015.04.061},
  shorttitle	= {Distributed evolutionary algorithms and their models},
  pages		= {286--300},
  journaltitle	= {Applied Soft Computing},
  shortjournal	= {Applied Soft Computing},
  author	= {Gong, Yue-Jiao and Chen, Wei-Neng and Zhan, Zhi-Hui and
		  Zhang, Jun and Li, Yun and Zhang, Qingfu and Li,
		  Jing-Jing},
  urldate	= {2022-12-02},
  date		= {2015-09},
  langid	= {english}
}

@InCollection{	  ernst2012difficult,
  location	= {Berlin, Heidelberg},
  title		= {Why it is Difficult to Solve Helmholtz Problems with
		  Classical Iterative Methods},
  volume	= {83},
  url		= {http://link.springer.com/10.1007/978-3-642-22061-6_10},
  pages		= {325--363},
  booktitle	= {Numerical Analysis of Multiscale Problems},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Ernst, O. G. and Gander, M. J.},
		  Scheichl, Robert},
  urldate	= {2022-12-10},
  date		= {2012},
  doi		= {10.1007/978-3-642-22061-6_10},
  note		= {Series Title: Lecture Notes in Computational Science and
		  Engineering}
}

@Article{	  gray1995migration,
  title		= {Migration from topography: Improving the near-surface
		  image},
  volume	= {31},
  pages		= {18--24},
  number	= {1},
  journaltitle	= {Canadian Journal of Exploration Geophysics},
  author	= {Gray, Samuel H. and Marfurt, Kurt J.},
  date		= {1995}
}

@InProceedings{	  greenfeld2019learning,
  title		= {Learning to Optimize Multigrid {PDE} Solvers},
  volume	= {97},
  url		= {https://proceedings.mlr.press/v97/greenfeld19a.html},
  series	= {Proceedings of Machine Learning Research},
  abstract	= {Constructing fast numerical solvers for partial
		  differential equations ({PDEs}) is crucial for many
		  scientific disciplines. A leading technique for solving
		  large-scale {PDEs} is using multigrid methods. At the core
		  of a multigrid solver is the prolongation matrix, which
		  relates between different scales of the problem. This
		  matrix is strongly problem-dependent, and its optimal
		  construction is critical to the efficiency of the solver.
		  In practice, however, devising multigrid algorithms for new
		  problems often poses formidable challenges. In this paper
		  we propose a framework for learning multigrid solvers. Our
		  method learns a (single) mapping from discretized {PDEs} to
		  prolongation operators for a broad class of 2D diffusion
		  problems. We train a neural network once for the entire
		  class of {PDEs}, using an efficient and unsupervised loss
		  function. Our tests demonstrate improved convergence rates
		  compared to the widely used Black-Box multigrid scheme,
		  suggesting that our method successfully learned rules for
		  constructing prolongation matrices.},
  pages		= {2415--2423},
  booktitle	= {Proceedings of the 36th International Conference on
		  Machine Learning},
  publisher	= {{PMLR}},
  author	= {Greenfeld, Daniel and Galun, Meirav and Basri, Ronen and
		  Yavneh, Irad and Kimmel, Ron},
  date		= {2019-06-09}
}

@InProceedings{	  guibas2021efficient,
  title		= {Efficient Token Mixing for Transformers via Adaptive
		  Fourier Neural Operators},
  url		= {https://arxiv.org/abs/2111.13587},
  booktitle	= {International Conference on Learning Representations},
  author	= {Guibas, John and Mardani, Morteza and Li, Zongyi and Tao,
		  Andrew and Anandkumar, Anima and Catanzaro, Bryan},
  date		= {2021}
}

@Book{		  hackbusch2013multi,
  location	= {Berlin, Heidelberg},
  title		= {Multi-Grid Methods and Applications},
  volume	= {4},
  url		= {http://link.springer.com/10.1007/978-3-662-02427-0},
  series	= {Springer Series in Computational Mathematics},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Hackbusch, Wolfgang},
  urldate	= {2022-12-10},
  date		= {1985},
  doi		= {10.1007/978-3-662-02427-0}
}

@Book{		  hager2010introduction,
  location	= {Boca Raton, {FL}},
  title		= {Introduction to high performance computing for scientists
		  and engineers},
  series	= {Chapman \& Hall/{CRC} computational science series ; 7},
  pagetotal	= {330},
  publisher	= {{CRC} Press},
  author	= {Hager, Georg and Wellein, Gerhard},
  date		= {2011}
}

@Article{	  hager2016exploring,
  title		= {Exploring performance and power properties of modern
		  multi-core chips via simple machine models},
  volume	= {28},
  issn		= {15320626},
  url		= {https://onlinelibrary.wiley.com/doi/10.1002/cpe.3180},
  doi		= {10.1002/cpe.3180},
  shorttitle	= {Exploring performance and power properties of modern
		  multi-core chips via simple machine models},
  pages		= {189--210},
  number	= {2},
  journaltitle	= {Concurrency and Computation: Practice and Experience},
  shortjournal	= {Concurrency Computat.: Pract. Exper.},
  author	= {Hager, Georg and Treibig, Jan and Habich, Johannes and
		  Wellein, Gerhard},
  urldate	= {2022-12-02},
  date		= {2016-02},
  langid	= {english}
}

@Article{	  hansen2001completely,
  title		= {Completely Derandomized Self-Adaptation in Evolution
		  Strategies},
  volume	= {9},
  issn		= {1063-6560, 1530-9304},
  url		= {https://direct.mit.edu/evco/article/9/2/159-195/892},
  doi		= {10.1162/106365601750190398},
  abstract	= {This paper puts forward two useful methods for
		  self-adaptation of the mutation distribution - the concepts
		  of derandomization and cumulation. Principle shortcomings
		  of the concept of mutative strategy parameter control and
		  two levels of derandomization are reviewed. Basic demands
		  on the self-adaptation of arbitrary (normal) mutation
		  distributions are developed. Applying arbitrary, normal
		  mutation distributions is equiv-alent to applying a
		  general, linear problem encoding. The underlying objective
		  of mutative strategy parameter control is roughly to favor
		  previously selected mutation steps in the future. If this
		  objective is pursued rigor-ously, a completely derandomized
		  self-adaptation scheme results, which adapts arbitrary
		  normal mutation distributions. This scheme, called
		  covariance matrix adaptation ({CMA}), meets the previously
		  stated demands. It can still be considerably improved by
		  cumulation - utilizing an evolution path rather than single
		  search steps. Simulations on various test functions reveal
		  local and global search properties of the evolution
		  strategy with and without covariance matrix adaptation.
		  Their performances are comparable only on perfectly scaled
		  functions. On badly scaled, non-separable functions usually
		  a speed up factor of several orders of magnitude is
		  ob-served. On moderately mis-scaled functions a speed up
		  factor of three to ten can be expected.},
  pages		= {159--195},
  number	= {2},
  journaltitle	= {Evolutionary Computation},
  shortjournal	= {Evolutionary Computation},
  author	= {Hansen, Nikolaus and Ostermeier, Andreas},
  urldate	= {2022-12-01},
  date		= {2001-06},
  langid	= {english}
}

@Article{	  he2021automl,
  title		= {{AutoML}: A survey of the state-of-the-art},
  volume	= {212},
  issn		= {09507051},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0950705120307516},
  doi		= {10.1016/j.knosys.2020.106622},
  shorttitle	= {{AutoML}},
  pages		= {106622},
  journaltitle	= {Knowledge-Based Systems},
  shortjournal	= {Knowledge-Based Systems},
  author	= {He, Xin and Zhao, Kaiyong and Chu, Xiaowen},
  urldate	= {2022-12-01},
  date		= {2021-01},
  langid	= {english}
}

@Article{	  helmuth2014solving,
  title		= {Solving Uncompromising Problems With Lexicase Selection},
  volume	= {19},
  issn		= {1089-778X, 1089-778X, 1941-0026},
  url		= {http://ieeexplore.ieee.org/document/6920034/},
  doi		= {10.1109/TEVC.2014.2362729},
  pages		= {630--643},
  number	= {5},
  journaltitle	= {{IEEE} Transactions on Evolutionary Computation},
  shortjournal	= {{IEEE} Trans. Evol. Computat.},
  author	= {Helmuth, Thomas and Spector, Lee and Matheson, James},
  urldate	= {2022-12-10},
  date		= {2015-10}
}

@InProceedings{	  henson2003multigrid,
  location	= {Santa Clara, {CA}},
  title		= {Multigrid methods nonlinear problems: an overview},
  url		= {http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.499473},
  doi		= {10.1117/12.499473},
  shorttitle	= {Multigrid methods nonlinear problems},
  eventtitle	= {Electronic Imaging 2003},
  pages		= {36},
  author	= {Henson, Van E.},
  urldate	= {2022-12-10},
  date		= {2003-06-27}
}

@Book{		  higham2002accuracy,
  edition	= {Second},
  title		= {Accuracy and Stability of Numerical Algorithms},
  url		= {http://epubs.siam.org/doi/book/10.1137/1.9780898718027},
  publisher	= {Society for Industrial and Applied Mathematics},
  author	= {Higham, Nicholas J.},
  urldate	= {2022-12-02},
  date		= {2002-01},
  langid	= {english},
  doi		= {10.1137/1.9780898718027}
}

@Book{		  holzapfel2001nonlinear,
  location	= {Chichester ; New York},
  title		= {Nonlinear solid mechanics: a continuum approach for
		  engineering},
  shorttitle	= {Nonlinear solid mechanics},
  pagetotal	= {455},
  publisher	= {Wiley},
  author	= {Holzapfel, Gerhard A.},
  date		= {2000}
}

@Article{	  hornik1989multilayer,
  title		= {Multilayer feedforward networks are universal
		  approximators},
  volume	= {2},
  issn		= {08936080},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/0893608089900208},
  doi		= {10.1016/0893-6080(89)90020-8},
  pages		= {359--366},
  number	= {5},
  journaltitle	= {Neural Networks},
  shortjournal	= {Neural Networks},
  author	= {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  urldate	= {2022-12-10},
  date		= {1989-01},
  langid	= {english}
}

@thesis{hoefer2020comparing,
	location = {Germany},
	title = {Comparing {MCTS} with Genetic Algorithms for Optimizing Multigrid Methods},
	url = {https://www10.cs.fau.de/publications/theses/2020/Hoefer_MT_2020.pdf},
	institution = {Friedrich-Alexander-Universität Erlangen-Nürnberg},
	type = {Master's Thesis},
	author = {Höfer, Daniel},
	date = {2020},
}


@Article{	  hsieh2019learning,
  title		= {Learning Neural {PDE} Solvers with Convergence
		  Guarantees},
  rights	= {{arXiv}.org perpetual, non-exclusive license},
  url		= {https://arxiv.org/abs/1906.01200},
  doi		= {10.48550/ARXIV.1906.01200},
  abstract	= {Partial differential equations ({PDEs}) are widely used
		  across the physical and computational sciences. Decades of
		  research and engineering went into designing fast iterative
		  solution methods. Existing solvers are general purpose, but
		  may be sub-optimal for specific classes of problems. In
		  contrast to existing hand-crafted solutions, we propose an
		  approach to learn a fast iterative solver tailored to a
		  specific domain. We achieve this goal by learning to modify
		  the updates of an existing solver using a deep neural
		  network. Crucially, our approach is proven to preserve
		  strong correctness and convergence guarantees. After
		  training on a single geometry, our model generalizes to a
		  wide variety of geometries and boundary conditions, and
		  achieves 2-3 times speedup compared to state-of-the-art
		  solvers.},
  author	= {Hsieh, Jun-Ting and Zhao, Shengjia and Eismann, Stephan
		  and Mirabella, Lucia and Ermon, Stefano},
  urldate	= {2022-12-02},
  date		= {2019},
  note		= {Publisher: {arXiv} Version Number: 1}
}

@Article{	  huang2021learning,
  title		= {Learning Optimal Multigrid Smoothers via Neural Networks},
  issn		= {1064-8275, 1095-7197},
  url		= {https://epubs.siam.org/doi/10.1137/21M1430030},
  doi		= {10.1137/21M1430030},
  pages		= {S199--S225},
  journaltitle	= {{SIAM} Journal on Scientific Computing},
  shortjournal	= {{SIAM} J. Sci. Comput.},
  author	= {Huang, Ru and Li, Ruipeng and Xi, Yuanzhe},
  urldate	= {2022-12-02},
  date		= {2022-08-24},
  langid	= {english}
}

@InCollection{	  hutter2010automated,
  location	= {Berlin, Heidelberg},
  title		= {Automated Configuration of Mixed Integer Programming
		  Solvers},
  volume	= {6140},
  url		= {http://link.springer.com/10.1007/978-3-642-13520-0_23},
  pages		= {186--202},
  booktitle	= {Integration of {AI} and {OR} Techniques in Constraint
		  Programming for Combinatorial Optimization Problems},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Hutter, Frank and Hoos, Holger H. and Leyton-Brown,
		  Kevin},
		  Kleinberg, Jon M. and Mattern, Friedemann and Mitchell,
		  John C. and Naor, Moni and Nierstrasz, Oscar and Pandu
		  Rangan, C. and Steffen, Bernhard and Sudan, Madhu and
		  Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y.
		  and Weikum, Gerhard},
  urldate	= {2022-12-01},
  date		= {2010},
  doi		= {10.1007/978-3-642-13520-0_23},
  note		= {Series Title: Lecture Notes in Computer Science}
}

@InCollection{	  fang2010review,
  location	= {Berlin, Heidelberg},
  title		= {A Review of Tournament Selection in Genetic Programming},
  volume	= {6382},
  url		= {http://link.springer.com/10.1007/978-3-642-16493-4_19},
  pages		= {181--192},
  booktitle	= {Advances in Computation and Intelligence},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Fang, Yongsheng and Li, Jun},
		  Kleinberg, Jon M. and Mattern, Friedemann and Mitchell,
		  John C. and Naor, Moni and Nierstrasz, Oscar and Pandu
		  Rangan, C. and Steffen, Bernhard and Sudan, Madhu and
		  Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y.
		  and Weikum, Gerhard},
  urldate	= {2022-12-01},
  date		= {2010},
  doi		= {10.1007/978-3-642-16493-4_19},
  note		= {Series Title: Lecture Notes in Computer Science}
}

@InProceedings{	  hutter2007automatic,
  title		= {Automatic Algorithm Configuration based on Local Search},
  url		= {https://www.aaai.org/Papers/AAAI/2007/AAAI07-183.pdf},
  booktitle	= {Proceedings of the Twenty-second National Conference on
		  Artificial Intelligence ({AAAI}'07)},
  author	= {Hutter, F and Hoos, H and Stützle, T},
  date		= {2007}
}

@collection{hutter2019automated,
	location = {Cham, Switzerland},
	title = {Automated machine learning: methods, systems, challenges},
	series = {The Springer series on challenges in machine learning},
	shorttitle = {Automated machine learning},
	abstract = {This open access book presents the first comprehensive overview of general methods in Automated Machine Learning ({AutoML}), collects descriptions of existing systems based on these methods, and discusses the first series of international challenges of {AutoML} systems. The recent success of commercial {ML} applications and the rapid growth of the field has created a high demand for off-the-shelf {ML} methods that can be used easily and without expert knowledge. However, many of the recent machine learning successes crucially rely on human experts, who manually select appropriate {ML} architectures (deep learning architectures or more traditional {ML} workflows) and their hyperparameters. To overcome this problem, the field of {AutoML} targets a progressive automation of machine learning, based on principles from optimization and machine learning itself. This book serves as a point of entry into this quickly-developing field for researchers and advanced students alike, as well as providing a reference for practitioners aiming to use {AutoML} in their work},
	pagetotal = {219},
	publisher = {Springer},
	date = {2019},
}

@InCollection{	  pitzer2012comprehensive,
  location	= {Berlin, Heidelberg},
  title		= {A Comprehensive Survey on Fitness Landscape Analysis},
  volume	= {378},
  url		= {http://link.springer.com/10.1007/978-3-642-23229-9_8},
  pages		= {161--191},
  booktitle	= {Recent Advances in Intelligent Engineering Systems},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Pitzer, Erik and Affenzeller, Michael},
		  Carmen Paz},
  urldate	= {2022-12-01},
  date		= {2012},
  doi		= {10.1007/978-3-642-23229-9_8},
  note		= {Series Title: Studies in Computational Intelligence}
}

@InCollection{	  deb2015multi,
  location	= {Berlin, Heidelberg},
  title		= {Multi-Objective Evolutionary Algorithms},
  url		= {http://link.springer.com/10.1007/978-3-662-43505-2_49},
  pages		= {995--1015},
  booktitle	= {Springer Handbook of Computational Intelligence},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Deb, Kalyanmoy},
  urldate	= {2022-12-10},
  date		= {2015},
  langid	= {english},
  doi		= {10.1007/978-3-662-43505-2_49}
}

@InCollection{	  sudholt2015parallel,
  location	= {Berlin, Heidelberg},
  title		= {Parallel Evolutionary Algorithms},
  url		= {http://link.springer.com/10.1007/978-3-662-43505-2_46},
  pages		= {929--959},
  booktitle	= {Springer Handbook of Computational Intelligence},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Sudholt, Dirk},
  urldate	= {2022-12-10},
  date		= {2015},
  langid	= {english},
  doi		= {10.1007/978-3-662-43505-2_46}
}

@Article{	  kahl2020automated,
  title		= {Automated local Fourier analysis ({aLFA})},
  volume	= {60},
  issn		= {0006-3835, 1572-9125},
  url		= {http://link.springer.com/10.1007/s10543-019-00797-w},
  doi		= {10.1007/s10543-019-00797-w},
  abstract	= {Abstract Local Fourier analysis is a commonly used tool to
		  assess the quality and aid in the construction of geometric
		  multigrid methods for translationally invariant operators.
		  In this paper we automate the process of local Fourier
		  analysis and present a framework that can be applied to
		  arbitrary, including non-orthogonal, repetitive structures.
		  To this end we introduce the notion of crystal structures
		  and a suitable definition of corresponding wave functions,
		  which allow for a natural representation of almost all
		  translationally invariant operators that are encountered in
		  applications, e.g., discretizations of systems of {PDEs},
		  tight-binding Hamiltonians of crystalline structures,
		  colored domain decomposition approaches and last but not
		  least two- or multigrid hierarchies. Based on this
		  definition we are able to automate the process of local
		  Fourier analysis both with respect to spatial manipulations
		  of operators as well as the Fourier analysis back-end. This
		  automation most notably simplifies the user input by
		  removing the necessity for compatible representations of
		  the involved operators. Each individual operator and its
		  corresponding structure can be provided in any
		  representation chosen by the user.},
  pages		= {651--686},
  number	= {3},
  journaltitle	= {{BIT} Numerical Mathematics},
  shortjournal	= {Bit Numer Math},
  author	= {Kahl, Karsten and Kintscher, Nils},
  urldate	= {2022-12-01},
  date		= {2020-09},
  langid	= {english}
}

@InCollection{	  koestler2004extrapolation,
  location	= {Berlin, Heidelberg},
  title		= {Extrapolation Techniques for Computing Accurate Solutions
		  of Elliptic Problems with Singular Solutions},
  volume	= {3039},
  url		= {http://link.springer.com/10.1007/978-3-540-25944-2_54},
  pages		= {410--417},
  booktitle	= {Computational Science - {ICCS} 2004},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Koestler, H. and Ruede, U.},
		  M. A. and Dongarra, Jack},
		  Mattern, Friedemann and Mitchell, John C. and Naor, Moni
		  and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen,
		  Bernhard and Sudan, Madhu and Terzopoulos, Demetri and
		  Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard},
  urldate	= {2022-12-02},
  date		= {2004},
  langid	= {english},
  doi		= {10.1007/978-3-540-25944-2_54},
  note		= {Series Title: Lecture Notes in Computer Science}
}

@Article{	  karniadakis2021physics,
  title		= {Physics-informed machine learning},
  volume	= {3},
  issn		= {2522-5820},
  url		= {https://www.nature.com/articles/s42254-021-00314-5},
  doi		= {10.1038/s42254-021-00314-5},
  pages		= {422--440},
  number	= {6},
  journaltitle	= {Nature Reviews Physics},
  shortjournal	= {Nat Rev Phys},
  author	= {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu,
		  Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
  urldate	= {2022-12-10},
  date		= {2021-05-24},
  langid	= {english}
}

@Article{	  katrutsa2020black,
  title		= {Black-box learning of multigrid parameters},
  volume	= {368},
  issn		= {03770427},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0377042719305291},
  doi		= {10.1016/j.cam.2019.112524},
  pages		= {112524},
  journaltitle	= {Journal of Computational and Applied Mathematics},
  shortjournal	= {Journal of Computational and Applied Mathematics},
  author	= {Katrutsa, Alexandr and Daulbaev, Talgat and Oseledets,
		  Ivan},
  urldate	= {2022-12-01},
  date		= {2020-04},
  langid	= {english}
}

@Article{	  kharazmi2019variational,
  title		= {Variational Physics-Informed Neural Networks For Solving
		  Partial Differential Equations},
  rights	= {{arXiv}.org perpetual, non-exclusive license},
  url		= {https://arxiv.org/abs/1912.00873},
  doi		= {10.48550/ARXIV.1912.00873},
  abstract	= {Physics-informed neural networks ({PINNs}) [31] use
		  automatic differentiation to solve partial differential
		  equations ({PDEs}) by penalizing the {PDE} in the loss
		  function at a random set of points in the domain of
		  interest. Here, we develop a Petrov-Galerkin version of
		  {PINNs} based on the nonlinear approximation of deep neural
		  networks ({DNNs}) by selecting the \{{\textbackslash}em
		  trial space\} to be the space of neural networks and the
		  \{{\textbackslash}em test space\} to be the space of
		  Legendre polynomials. We formulate the
		  {\textbackslash}textit\{variational residual\} of the {PDE}
		  using the {DNN} approximation by incorporating the
		  variational form of the problem into the loss function of
		  the network and construct a
		  {\textbackslash}textit\{variational physics-informed neural
		  network\} ({VPINN}). By integrating by parts the integrand
		  in the variational form, we lower the order of the
		  differential operators represented by the neural networks,
		  hence effectively reducing the training cost in {VPINNs}
		  while increasing their accuracy compared to {PINNs} that
		  essentially employ delta test functions. For shallow
		  networks with one hidden layer, we analytically obtain
		  explicit forms of the {\textbackslash}textit\{variational
		  residual\}. We demonstrate the performance of the new
		  formulation for several examples that show clear advantages
		  of {VPINNs} over {PINNs} in terms of both accuracy and speed.},
  author	= {Kharazmi, E. and Zhang, Z. and Karniadakis, G. E.},
  urldate	= {2022-12-10},
  date		= {2019},
  note		= {Publisher: {arXiv} Version Number: 1}
}

@Article{	  kharazmi2021hp,
  title		= {hp-{VPINNs}: Variational physics-informed neural networks
		  with domain decomposition},
  volume	= {374},
  issn		= {00457825},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0045782520307325},
  doi		= {10.1016/j.cma.2020.113547},
  shorttitle	= {hp-{VPINNs}},
  pages		= {113547},
  journaltitle	= {Computer Methods in Applied Mechanics and Engineering},
  shortjournal	= {Computer Methods in Applied Mechanics and Engineering},
  author	= {Kharazmi, Ehsan and Zhang, Zhongqiang and Karniadakis,
		  George E.M.},
  urldate	= {2022-12-02},
  date		= {2021-02},
  langid	= {english}
}

@Article{	  khudabukhsh2016satenstein,
  title		= {{SATenstein}: Automatically building local search {SAT}
		  solvers from components},
  volume	= {232},
  issn		= {00043702},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0004370215001678},
  doi		= {10.1016/j.artint.2015.11.002},
  shorttitle	= {{SATenstein}},
  pages		= {20--42},
  journaltitle	= {Artificial Intelligence},
  shortjournal	= {Artificial Intelligence},
  author	= {{KhudaBukhsh}, Ashiqur R. and Xu, Lin and Hoos, Holger H.
		  and Leyton-Brown, Kevin},
  urldate	= {2022-12-10},
  date		= {2016-03},
  langid	= {english}
}

@Book{		  knupp2020fundamentals,
  edition	= {1},
  title		= {Fundamentals of Grid Generation},
  url		= {https://www.taylorfrancis.com/books/9780429610820},
  publisher	= {{CRC} Press},
  author	= {Knupp, Patrick and Steinberg, Stanly},
  urldate	= {2022-12-02},
  date		= {2020-12-17},
  langid	= {english},
  doi		= {10.1201/9780138755287}
}

@Article{	  kostler2020code,
  title		= {Code generation approaches for parallel geometric
		  multigrid solvers},
  volume	= {28},
  issn		= {1844-0835},
  url		= {https://www.sciendo.com/article/10.2478/auom-2020-0038},
  doi		= {10.2478/auom-2020-0038},
  abstract	= {Abstract Software development for applications in
		  computational science and engineering has become complex in
		  recent years. This is mainly due to the increasing
		  parallelism and heterogeneity in modern computer
		  architectures and to the more realistic physical and
		  mathematical models that have to be processed. One idea to
		  address this issue is to use code generation techniques. In
		  contrast to manual implementations in a general-purpose
		  computing language, they allow to integrate automatic code
		  transforms to produce efficient code for different models
		  and platforms. As an example the numerical solution of an
		  elliptic partial differential equation via generated
		  geometric multigrid solvers is considered. We present three
		  code generation approaches for it and discuss their
		  advantages and disadvantages with respect to performance,
		  portability, and productivity.},
  pages		= {123--152},
  number	= {3},
  journaltitle	= {Analele Universitatii "Ovidius" Constanta - Seria
		  Matematica},
  author	= {Köstler, Harald and Heisig, Marco and Kohl, Nils and
		  Kuckuk, Sebastian and Bauer, Martin and Rüde, Ulrich},
  urldate	= {2022-12-01},
  date		= {2020-12-01},
  langid	= {english}
}

@Article{	  kovachki2021neural,
  title		= {Neural Operator: Learning Maps Between Function Spaces},
  rights	= {{arXiv}.org perpetual, non-exclusive license},
  url		= {https://arxiv.org/abs/2108.08481},
  doi		= {10.48550/ARXIV.2108.08481},
  shorttitle	= {Neural Operator},
  abstract	= {The classical development of neural networks has primarily
		  focused on learning mappings between finite dimensional
		  Euclidean spaces or finite sets. We propose a
		  generalization of neural networks to learn operators,
		  termed neural operators, that map between infinite
		  dimensional function spaces. We formulate the neural
		  operator as a composition of linear integral operators and
		  nonlinear activation functions. We prove a universal
		  approximation theorem for our proposed neural operator,
		  showing that it can approximate any given nonlinear
		  continuous operator. The proposed neural operators are also
		  discretization-invariant, i.e., they share the same model
		  parameters among different discretization of the underlying
		  function spaces. Furthermore, we introduce four classes of
		  efficient parameterization, viz., graph neural operators,
		  multi-pole graph neural operators, low-rank neural
		  operators, and Fourier neural operators. An important
		  application for neural operators is learning surrogate maps
		  for the solution operators of partial differential
		  equations ({PDEs}). We consider standard {PDEs} such as the
		  Burgers, Darcy subsurface flow, and the Navier-Stokes
		  equations, and show that the proposed neural operators have
		  superior performance compared to existing machine learning
		  based methodologies, while being several orders of
		  magnitude faster than conventional {PDE} solvers.},
  author	= {Kovachki, Nikola and Li, Zongyi and Liu, Burigede and
		  Azizzadenesheli, Kamyar and Bhattacharya, Kaushik and
		  Stuart, Andrew and Anandkumar, Anima},
  urldate	= {2022-12-10},
  date		= {2021},
  note		= {Publisher: {arXiv} Version Number: 4}
}

@Article{	  koza1994genetic,
  title		= {Genetic programming as a means for programming computers
		  by natural selection},
  volume	= {4},
  issn		= {0960-3174, 1573-1375},
  url		= {http://link.springer.com/10.1007/BF00175355},
  doi		= {10.1007/BF00175355},
  number	= {2},
  journaltitle	= {Statistics and Computing},
  shortjournal	= {Stat Comput},
  author	= {Koza, {JohnR}.},
  urldate	= {2022-12-02},
  date		= {1994-06},
  langid	= {english}
}

@Article{	  koza2010human,
  title		= {Human-competitive results produced by genetic
		  programming},
  volume	= {11},
  issn		= {1389-2576, 1573-7632},
  url		= {http://link.springer.com/10.1007/s10710-010-9112-3},
  doi		= {10.1007/s10710-010-9112-3},
  pages		= {251--284},
  number	= {3},
  journaltitle	= {Genetic Programming and Evolvable Machines},
  shortjournal	= {Genet Program Evolvable Mach},
  author	= {Koza, John R.},
  urldate	= {2022-12-02},
  date		= {2010-09},
  langid	= {english}
}

@Article{	  krizhevsky2017imagenet,
  title		= {{ImageNet} classification with deep convolutional neural
		  networks},
  volume	= {60},
  issn		= {0001-0782, 1557-7317},
  url		= {https://dl.acm.org/doi/10.1145/3065386},
  doi		= {10.1145/3065386},
  abstract	= {We trained a large, deep convolutional neural network to
		  classify the 1.2 million high-resolution images in the
		  {ImageNet} {LSVRC}-2010 contest into the 1000 different
		  classes. On the test data, we achieved top-1 and top-5
		  error rates of 37.5\% and 17.0\%, respectively, which is
		  considerably better than the previous state-of-the-art. The
		  neural network, which has 60 million parameters and 650,000
		  neurons, consists of five convolutional layers, some of
		  which are followed by max-pooling layers, and three fully
		  connected layers with a final 1000-way softmax. To make
		  training faster, we used non-saturating neurons and a very
		  efficient {GPU} implementation of the convolution
		  operation. To reduce overfitting in the fully connected
		  layers we employed a recently developed regularization
		  method called "dropout" that proved to be very effective.
		  We also entered a variant of this model in the
		  {ILSVRC}-2012 competition and achieved a winning top-5 test
		  error rate of 15.3\%, compared to 26.2\% achieved by the
		  second-best entry.},
  pages		= {84--90},
  number	= {6},
  journaltitle	= {Communications of the {ACM}},
  shortjournal	= {Commun. {ACM}},
  author	= {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey
		  E.},
  urldate	= {2022-12-02},
  date		= {2017-05-24},
  langid	= {english}
}

@Article{	  kuckuk2016automatic,
  title		= {Automatic Generation of Massively Parallel Codes from
		  {ExaSlang}},
  volume	= {4},
  issn		= {2079-3197},
  url		= {http://www.mdpi.com/2079-3197/4/3/27},
  doi		= {10.3390/computation4030027},
  pages		= {27},
  number	= {3},
  journaltitle	= {Computation},
  shortjournal	= {Computation},
  author	= {Kuckuk, Sebastian and Köstler, Harald},
  urldate	= {2022-12-01},
  date		= {2016-08-04},
  langid	= {english}
}

@InProceedings{	  la2016epsilon,
  location	= {Denver Colorado {USA}},
  title		= {Epsilon-Lexicase Selection for Regression},
  url		= {https://dl.acm.org/doi/10.1145/2908812.2908898},
  doi		= {10.1145/2908812.2908898},
  eventtitle	= {{GECCO} '16: Genetic and Evolutionary Computation
		  Conference},
  pages		= {741--748},
  booktitle	= {Proceedings of the Genetic and Evolutionary Computation
		  Conference 2016},
  publisher	= {{ACM}},
  author	= {La Cava, William and Spector, Lee and Danai, Kourosh},
  urldate	= {2022-12-02},
  date		= {2016-07-20},
  langid	= {english}
}

@Article{	  lagaris1998artificial,
  title		= {Artificial neural networks for solving ordinary and
		  partial differential equations},
  volume	= {9},
  issn		= {10459227},
  url		= {http://ieeexplore.ieee.org/document/712178/},
  doi		= {10.1109/72.712178},
  pages		= {987--1000},
  number	= {5},
  journaltitle	= {{IEEE} Transactions on Neural Networks},
  shortjournal	= {{IEEE} Trans. Neural Netw.},
  author	= {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
  urldate	= {2022-12-01},
  date		= {1998-09}
}

@Article{	  li2020fourier,
  title		= {Fourier Neural Operator for Parametric Partial
		  Differential Equations},
  rights	= {{arXiv}.org perpetual, non-exclusive license},
  url		= {https://arxiv.org/abs/2010.08895},
  doi		= {10.48550/ARXIV.2010.08895},
  abstract	= {The classical development of neural networks has primarily
		  focused on learning mappings between finite-dimensional
		  Euclidean spaces. Recently, this has been generalized to
		  neural operators that learn mappings between function
		  spaces. For partial differential equations ({PDEs}), neural
		  operators directly learn the mapping from any functional
		  parametric dependence to the solution. Thus, they learn an
		  entire family of {PDEs}, in contrast to classical methods
		  which solve one instance of the equation. In this work, we
		  formulate a new neural operator by parameterizing the
		  integral kernel directly in Fourier space, allowing for an
		  expressive and efficient architecture. We perform
		  experiments on Burgers' equation, Darcy flow, and
		  Navier-Stokes equation. The Fourier neural operator is the
		  first {ML}-based method to successfully model turbulent
		  flows with zero-shot super-resolution. It is up to three
		  orders of magnitude faster compared to traditional {PDE}
		  solvers. Additionally, it achieves superior accuracy
		  compared to previous learning-based solvers under fixed
		  resolution.},
  author	= {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli,
		  Kamyar and Liu, Burigede and Bhattacharya, Kaushik and
		  Stuart, Andrew and Anandkumar, Anima},
  urldate	= {2022-12-02},
  date		= {2020},
  note		= {Publisher: {arXiv} Version Number: 3}
}

@Article{	  li2021physics,
  title		= {Physics-Informed Neural Operator for Learning Partial
		  Differential Equations},
  rights	= {{arXiv}.org perpetual, non-exclusive license},
  url		= {https://arxiv.org/abs/2111.03794},
  doi		= {10.48550/ARXIV.2111.03794},
  abstract	= {In this paper, we propose physics-informed neural
		  operators ({PINO}) that uses available data and/or physics
		  constraints to learn the solution operator of a family of
		  parametric Partial Differential Equation ({PDE}). This
		  hybrid approach allows {PINO} to overcome the limitations
		  of purely data-driven and physics-based methods. For
		  instance, data-driven methods fail to learn when data is of
		  limited quantity and/or quality, and physics-based
		  approaches fail to optimize on challenging {PDE}
		  constraints. By combining both data and {PDE} constraints,
		  {PINO} overcomes all these challenges. Additionally, a
		  unique property that {PINO} enjoys over other hybrid
		  learning methods is its ability to incorporate data and
		  {PDE} constraints at different resolutions. This allows us
		  to combine coarse-resolution data, which is inexpensive to
		  obtain from numerical solvers, with higher resolution {PDE}
		  constraints, and the resulting {PINO} has no degradation in
		  accuracy even on high-resolution test instances. This
		  discretization-invariance property in {PINO} is due to
		  neural-operator framework which learns mappings between
		  function spaces and allows evaluation at different
		  resolutions without the need for re-training. Moreover,
		  {PINO} succeeds in the purely physics setting, where no
		  data is available, while other approaches such as the
		  Physics-Informed Neural Network ({PINN}) fail due to
		  optimization challenges, e.g. in multi-scale dynamic
		  systems such as Kolmogorov flows. This is because {PINO}
		  learns the solution operator by optimizing {PDE}
		  constraints on multiple instances while {PINN} optimizes
		  {PDE} constraints of a single {PDE} instance. Further, in
		  {PINO}, we incorporate the Fourier neural operator ({FNO})
		  architecture which achieves orders-of-magnitude speedup
		  over numerical solvers and also allows us to compute
		  explicit gradients on function spaces efficiently.},
  author	= {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and
		  Jin, David and Chen, Haoxuan and Liu, Burigede and
		  Azizzadenesheli, Kamyar and Anandkumar, Anima},
  urldate	= {2022-12-10},
  date		= {2021},
  note		= {Publisher: {arXiv} Version Number: 2}
}

@Book{		  linz2006introduction,
  location	= {Burlington, Massachusetts},
  edition	= {Seventh edition},
  title		= {An introduction to formal languages and automata},
  abstract	= {"This book is designed for an introductory course on
		  formal languages, automata, computability, and related
		  matters"--},
  pagetotal	= {584},
  publisher	= {Jones \& Bartlett Learning},
  author	= {Linz, Peter and Rodger, Susan H.},
  date		= {2023}
}

@Article{	  lipowski2012roulette,
  title		= {Roulette-wheel selection via stochastic acceptance},
  volume	= {391},
  issn		= {03784371},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0378437111009010},
  doi		= {10.1016/j.physa.2011.12.004},
  pages		= {2193--2196},
  number	= {6},
  journaltitle	= {Physica A: Statistical Mechanics and its Applications},
  shortjournal	= {Physica A: Statistical Mechanics and its Applications},
  author	= {Lipowski, Adam and Lipowska, Dorota},
  urldate	= {2022-12-10},
  date		= {2012-03},
  langid	= {english}
}

@InProceedings{	  liu2022evolvability,
  location	= {Boston Massachusetts},
  title		= {Evolvability degeneration in multi-objective genetic
		  programming for symbolic regression},
  url		= {https://dl.acm.org/doi/10.1145/3512290.3528787},
  doi		= {10.1145/3512290.3528787},
  eventtitle	= {{GECCO} '22: Genetic and Evolutionary Computation
		  Conference},
  pages		= {973--981},
  booktitle	= {Proceedings of the Genetic and Evolutionary Computation
		  Conference},
  publisher	= {{ACM}},
  author	= {Liu, Dazhuang and Virgolin, Marco and Alderliesten, Tanja
		  and Bosman, Peter A. N.},
  urldate	= {2022-12-02},
  date		= {2022-07-08},
  langid	= {english}
}

@InCollection{	  lengauer2014exastencils,
  location	= {Cham},
  title		= {{ExaStencils}: Advanced Stencil-Code Engineering},
  volume	= {8806},
  url		= {http://link.springer.com/10.1007/978-3-319-14313-2_47},
  shorttitle	= {{ExaStencils}},
  pages		= {553--564},
  booktitle	= {Euro-Par 2014: Parallel Processing Workshops},
  publisher	= {Springer International Publishing},
  author	= {Lengauer, Christian and Apel, Sven and Bolten, Matthias
		  and Größlinger, Armin and Hannig, Frank and Köstler,
		  Harald and Rüde, Ulrich and Teich, Jürgen and Grebhahn,
		  Alexander and Kronawitter, Stefan and Kuckuk, Sebastian and
		  Rittich, Hannah and Schmitt, Christian},
		  and Cascella, Roberto G. and Kecskemeti, Gabor and Jeannot,
		  Emmanuel and Cannataro, Mario and Ricci, Laura and Benkner,
		  Siegfried and Petit, Salvador and Scarano, Vittorio and
		  Gracia, José and Hunold, Sascha and Scott, Stephen L. and
		  Lankes, Stefan and Lengauer, Christian and Carretero,
		  Jesús and Breitbart, Jens and Alexander, Michael},
  urldate	= {2022-12-02},
  date		= {2014},
  langid	= {english},
  doi		= {10.1007/978-3-319-14313-2_47},
  note		= {Series Title: Lecture Notes in Computer Science}
}

@Article{	  lopez-ibanez2016irace,
  title		= {The irace package: Iterated racing for automatic algorithm
		  configuration},
  volume	= {3},
  issn		= {22147160},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S2214716015300270},
  doi		= {10.1016/j.orp.2016.09.002},
  shorttitle	= {The irace package},
  pages		= {43--58},
  journaltitle	= {Operations Research Perspectives},
  shortjournal	= {Operations Research Perspectives},
  author	= {López-Ibáñez, Manuel and Dubois-Lacoste, Jérémie and
		  Pérez Cáceres, Leslie and Birattari, Mauro and Stützle,
		  Thomas},
  urldate	= {2022-12-10},
  date		= {2016},
  langid	= {english}
}

@Article{	  lu2021deepxde,
  title		= {{DeepXDE}: A Deep Learning Library for Solving
		  Differential Equations},
  volume	= {63},
  issn		= {0036-1445, 1095-7200},
  url		= {https://epubs.siam.org/doi/10.1137/19M1274067},
  doi		= {10.1137/19M1274067},
  shorttitle	= {{DeepXDE}},
  pages		= {208--228},
  number	= {1},
  journaltitle	= {{SIAM} Review},
  shortjournal	= {{SIAM} Rev.},
  author	= {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis,
		  George Em},
  urldate	= {2022-12-01},
  date		= {2021-01},
  langid	= {english}
}

@Article{	  lu2021learning,
  title		= {Learning nonlinear operators via {DeepONet} based on the
		  universal approximation theorem of operators},
  volume	= {3},
  issn		= {2522-5839},
  url		= {http://www.nature.com/articles/s42256-021-00302-5},
  doi		= {10.1038/s42256-021-00302-5},
  pages		= {218--229},
  number	= {3},
  journaltitle	= {Nature Machine Intelligence},
  shortjournal	= {Nat Mach Intell},
  author	= {Lu, Lu and Jin, Pengzhan and Pang, Guofei and Zhang,
		  Zhongqiang and Karniadakis, George Em},
  urldate	= {2022-12-02},
  date		= {2021-03},
  langid	= {english}
}

@InProceedings{	  luz2020learning,
  title		= {Learning Algebraic Multigrid Using Graph Neural Networks},
  volume	= {119},
  url		= {https://proceedings.mlr.press/v119/luz20a.html},
  series	= {Proceedings of Machine Learning Research},
  abstract	= {Efficient numerical solvers for sparse linear systems are
		  crucial in science and engineering. One of the fastest
		  methods for solving large-scale sparse linear systems is
		  algebraic multigrid ({AMG}). The main challenge in the
		  construction of {AMG} algorithms is the selection of the
		  prolongation operator—a problem-dependent sparse matrix
		  which governs the multiscale hierarchy of the solver and is
		  critical to its efficiency. Over many years, numerous
		  methods have been developed for this task, and yet there is
		  no known single right answer except in very special cases.
		  Here we propose a framework for learning {AMG} prolongation
		  operators for linear systems with sparse symmetric positive
		  (semi-) definite matrices. We train a single graph neural
		  network to learn a mapping from an entire class of such
		  matrices to prolongation operators, using an efficient
		  unsupervised loss function. Experiments on a broad class of
		  problems demonstrate improved convergence rates compared to
		  classical {AMG}, demonstrating the potential utility of
		  neural networks for developing sparse system solvers.},
  pages		= {6489--6499},
  booktitle	= {Proceedings of the 37th International Conference on
		  Machine Learning},
  publisher	= {{PMLR}},
  author	= {Luz, Ilay and Galun, Meirav and Maron, Haggai and Basri,
		  Ronen and Yavneh, Irad},
  date		= {2020-07-13}
}

@Article{	  markidis2021old,
  title		= {The Old and the New: Can Physics-Informed Deep-Learning
		  Replace Traditional Linear Solvers?},
  volume	= {4},
  issn		= {2624-909X},
  url		= {https://www.frontiersin.org/articles/10.3389/fdata.2021.669097/full},
  doi		= {10.3389/fdata.2021.669097},
  shorttitle	= {The Old and the New},
  abstract	= {Physics-Informed Neural Networks ({PINN}) are neural
		  networks encoding the problem governing equations, such as
		  Partial Differential Equations ({PDE}), as a part of the
		  neural network. {PINNs} have emerged as a new essential
		  tool to solve various challenging problems, including
		  computing linear systems arising from {PDEs}, a task for
		  which several traditional methods exist. In this work, we
		  focus first on evaluating the potential of {PINNs} as
		  linear solvers in the case of the Poisson equation, an
		  omnipresent equation in scientific computing. We
		  characterize {PINN} linear solvers in terms of accuracy and
		  performance under different network configurations (depth,
		  activation functions, input data set distribution). We
		  highlight the critical role of transfer learning. Our
		  results show that low-frequency components of the solution
		  converge quickly as an effect of the F-principle. In
		  contrast, an accurate solution of the high frequencies
		  requires an exceedingly long time. To address this
		  limitation, we propose integrating {PINNs} into traditional
		  linear solvers. We show that this integration leads to the
		  development of new solvers whose performance is on par with
		  other high-performance solvers, such as {PETSc} conjugate
		  gradient linear solvers, in terms of performance and
		  accuracy. Overall, while the accuracy and computational
		  performance are still a limiting factor for the direct use
		  of {PINN} linear solvers, hybrid strategies combining old
		  traditional linear solver approaches with new emerging
		  deep-learning techniques are among the most promising
		  methods for developing a new class of linear solvers.},
  pages		= {669097},
  journaltitle	= {Frontiers in Big Data},
  shortjournal	= {Front. Big Data},
  author	= {Markidis, Stefano},
  urldate	= {2022-12-10},
  date		= {2021-11-19}
}

@Article{	  martin2006marmousi2,
  title		= {Marmousi2: An elastic upgrade for Marmousi},
  volume	= {25},
  issn		= {1070-485X, 1938-3789},
  url		= {http://library.seg.org/doi/10.1190/1.2172306},
  doi		= {10.1190/1.2172306},
  shorttitle	= {Marmousi2},
  pages		= {156--166},
  number	= {2},
  journaltitle	= {The Leading Edge},
  shortjournal	= {The Leading Edge},
  author	= {Martin, Gary S. and Wiley, Robert and Marfurt, Kurt J.},
  urldate	= {2022-12-10},
  date		= {2006-02},
  langid	= {english}
}

@Article{	  mascia2014grammar,
  title		= {Grammar-based generation of stochastic local search
		  heuristics through automatic algorithm configuration
		  tools},
  volume	= {51},
  issn		= {03050548},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0305054814001555},
  doi		= {10.1016/j.cor.2014.05.020},
  pages		= {190--199},
  journaltitle	= {Computers \& Operations Research},
  shortjournal	= {Computers \& Operations Research},
  author	= {Mascia, Franco and López-Ibáñez, Manuel and
		  Dubois-Lacoste, Jérémie and Stützle, Thomas},
  urldate	= {2022-12-02},
  date		= {2014-11},
  langid	= {english}
}

@InCollection{	  ruge1987algebraic,
  title		= {Algebraic Multigrid},
  url		= {http://epubs.siam.org/doi/10.1137/1.9781611971057.ch4},
  pages		= {73--130},
  booktitle	= {Multigrid Methods},
  publisher	= {Society for Industrial and Applied Mathematics},
  author	= {Ruge, J. W. and Stüben, K.},
  urldate	= {2022-12-02},
  date		= {1987-01},
  langid	= {english},
  doi		= {10.1137/1.9781611971057.ch4}
}

@Article{	  mckay2010grammar,
  title		= {Grammar-based Genetic Programming: a survey},
  volume	= {11},
  issn		= {1389-2576, 1573-7632},
  url		= {http://link.springer.com/10.1007/s10710-010-9109-y},
  doi		= {10.1007/s10710-010-9109-y},
  shorttitle	= {Grammar-based Genetic Programming},
  pages		= {365--396},
  number	= {3},
  journaltitle	= {Genetic Programming and Evolvable Machines},
  shortjournal	= {Genet Program Evolvable Mach},
  author	= {{McKay}, Robert I. and Hoai, Nguyen Xuan and Whigham,
		  Peter Alexander and Shan, Yin and O’Neill, Michael},
  urldate	= {2022-12-02},
  date		= {2010-09},
  langid	= {english}
}

@InProceedings{	  miller2008cartesian,
  location	= {Atlanta, {GA}, {USA}},
  title		= {Cartesian genetic programming},
  url		= {http://portal.acm.org/citation.cfm?doid=1388969.1389075},
  doi		= {10.1145/1388969.1389075},
  eventtitle	= {the 2008 {GECCO} conference companion},
  pages		= {2701},
  booktitle	= {Proceedings of the 2008 {GECCO} conference companion on
		  Genetic and evolutionary computation - {GECCO} '08},
  publisher	= {{ACM} Press},
  author	= {Miller, Julian Francis and Harding, Simon L.},
  urldate	= {2022-12-01},
  date		= {2008},
  langid	= {english}
}

@Article{	  montana1995strongly,
  title		= {Strongly Typed Genetic Programming},
  volume	= {3},
  issn		= {1063-6560, 1530-9304},
  url		= {https://direct.mit.edu/evco/article/3/2/199-230/731},
  doi		= {10.1162/evco.1995.3.2.199},
  abstract	= {Genetic programming is a powerful method for automatically
		  generating computer programs via the process of natural
		  selection (Koza, 1992). However, in its standard form,
		  there is no way to restrict the programs it generates to
		  those where the functions operate on appropriate data
		  types. In the case when the programs manipulate multiple
		  data types and contain functions designed to operate on
		  particular data types, this can lead to unnecessarily large
		  search times and/or unnecessarily poor generalization
		  performance. Strongly typed genetic programming ({STGP}) is
		  an enhanced version of genetic programming that enforces
		  data-type constraints and whose use of generic functions
		  and generic data types makes it more powerful than other
		  approaches to type-constraint enforcement. After describing
		  its operation, we illustrate its use on problems in two
		  domains, matrix/vector manipulation and list manipulation,
		  which require its generality. The examples are (1) the
		  multidimensional least-squares regression problem, (2) the
		  multidimensional Kalman filter, (3) the list manipulation
		  function {NTH}, and (4) the list manipulation function
		  {MAPCAR}.},
  pages		= {199--230},
  number	= {2},
  journaltitle	= {Evolutionary Computation},
  shortjournal	= {Evolutionary Computation},
  author	= {Montana, David J.},
  urldate	= {2022-12-10},
  date		= {1995-06},
  langid	= {english}
}

@Article{	  oneill2001grammatical,
  title		= {Grammatical evolution},
  volume	= {5},
  issn		= {1089778X},
  url		= {http://ieeexplore.ieee.org/document/942529/},
  doi		= {10.1109/4235.942529},
  pages		= {349--358},
  number	= {4},
  journaltitle	= {{IEEE} Transactions on Evolutionary Computation},
  shortjournal	= {{IEEE} Trans. Evol. Computat.},
  author	= {O'Neill, M. and Ryan, C.},
  urldate	= {2022-12-02},
  date		= {2001-08}
}

@Article{	  oosterlee2003genetic,
  title		= {A Genetic Search for Optimal Multigrid Components Within a
		  Fourier Analysis Setting},
  volume	= {24},
  issn		= {1064-8275, 1095-7197},
  url		= {http://epubs.siam.org/doi/10.1137/S1064827501397950},
  doi		= {10.1137/S1064827501397950},
  pages		= {924--944},
  number	= {3},
  journaltitle	= {{SIAM} Journal on Scientific Computing},
  shortjournal	= {{SIAM} J. Sci. Comput.},
  author	= {Oosterlee, C. W. and Wienands, R.},
  urldate	= {2022-12-01},
  date		= {2003-01},
  langid	= {english}
}

@Article{	  osei2010preconditioning,
  title		= {Preconditioning Helmholtz linear systems},
  volume	= {60},
  issn		= {01689274},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0168927409001603},
  doi		= {10.1016/j.apnum.2009.09.003},
  pages		= {420--431},
  number	= {4},
  journaltitle	= {Applied Numerical Mathematics},
  shortjournal	= {Applied Numerical Mathematics},
  author	= {Osei-Kuffuor, Daniel and Saad, Yousef},
  urldate	= {2022-12-10},
  date		= {2010-04},
  langid	= {english}
}

@Article{	  pagnozzi2022evaluating,
  title		= {Evaluating the impact of grammar complexity in automatic
		  algorithm design},
  volume	= {29},
  issn		= {0969-6016, 1475-3995},
  url		= {https://onlinelibrary.wiley.com/doi/10.1111/itor.12902},
  doi		= {10.1111/itor.12902},
  pages		= {2789--2814},
  number	= {5},
  journaltitle	= {International Transactions in Operational Research},
  shortjournal	= {Int Trans Operational Res},
  author	= {Pagnozzi, Federico and Stützle, Thomas},
  urldate	= {2022-12-02},
  date		= {2022-09},
  langid	= {english}
}

@InCollection{	  hennigh2021nvidia,
  location	= {Cham},
  title		= {{NVIDIA} {SimNet}™: An {AI}-Accelerated Multi-Physics
		  Simulation Framework},
  volume	= {12746},
  url		= {https://link.springer.com/10.1007/978-3-030-77977-1_36},
  shorttitle	= {{NVIDIA} {SimNet}™},
  pages		= {447--461},
  booktitle	= {Computational Science – {ICCS} 2021},
  publisher	= {Springer International Publishing},
  author	= {Hennigh, Oliver and Narasimhan, Susheela and Nabian,
		  Mohammad Amin and Subramaniam, Akshay and Tangsali,
		  Kaustubh and Fang, Zhiwei and Rietmann, Max and Byeon,
		  Wonmin and Choudhry, Sanjay},
		  Krzhizhanovskaya, Valeria V. and Dongarra, Jack J. and
		  Sloot, Peter M.A.},
  urldate	= {2022-12-10},
  date		= {2021},
  langid	= {english},
  doi		= {10.1007/978-3-030-77977-1_36},
  note		= {Series Title: Lecture Notes in Computer Science}
}

@InProceedings{	  peng2020pyglove,
  title		= {{PyGlove}: Symbolic programming for automated machine
		  learning},
  volume	= {33},
  url		= {https://proceedings.neurips.cc/paper/2020/file/012a91467f210472fab4e11359bbfef6-Paper.pdf},
  pages		= {96--108},
  booktitle	= {Advances in Neural Information Processing Systems},
  author	= {Peng, Daiyi and Dong, Xuanyi and Real, Esteban and Tan,
		  Mingxing and Lu, Yifeng and Bender, Gabriel and Liu,
		  Hanxiao and Kraft, Adam and Liang, Chen and Le, Quoc},
  date		= {2020}
}

@Misc{		  pfaff2020learning,
  title		= {Learning Mesh-Based Simulation with Graph Networks},
  url		= {http://arxiv.org/abs/2010.03409},
  abstract	= {Mesh-based simulations are central to modeling complex
		  physical systems in many disciplines across science and
		  engineering. Mesh representations support powerful
		  numerical integration methods and their resolution can be
		  adapted to strike favorable trade-offs between accuracy and
		  efficiency. However, high-dimensional scientific
		  simulations are very expensive to run, and solvers and
		  parameters must often be tuned individually to each system
		  studied. Here we introduce {MeshGraphNets}, a framework for
		  learning mesh-based simulations using graph neural
		  networks. Our model can be trained to pass messages on a
		  mesh graph and to adapt the mesh discretization during
		  forward simulation. Our results show it can accurately
		  predict the dynamics of a wide range of physical systems,
		  including aerodynamics, structural mechanics, and cloth.
		  The model's adaptivity supports learning
		  resolution-independent dynamics and can scale to more
		  complex state spaces at test time. Our method is also
		  highly efficient, running 1-2 orders of magnitude faster
		  than the simulation on which it is trained. Our approach
		  broadens the range of problems on which neural network
		  simulators can operate and promises to improve the
		  efficiency of complex, scientific modeling tasks.},
  number	= {{arXiv}:2010.03409},
  publisher	= {{arXiv}},
  author	= {Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez,
		  Alvaro and Battaglia, Peter W.},
  urldate	= {2022-12-02},
  date		= {2021-06-18},
  eprinttype	= {arxiv},
  eprint	= {2010.03409 [cs]}
}

@Book{		  pierce2002types,
  location	= {Cambridge, Mass},
  title		= {Types and programming languages},
  pagetotal	= {623},
  publisher	= {{MIT} Press},
  author	= {Pierce, Benjamin C.},
  date		= {2002}
}

@Book{		  pierce2019acoustics,
  location	= {Cham},
  title		= {Acoustics: An Introduction to Its Physical Principles and
		  Applications},
  url		= {http://link.springer.com/10.1007/978-3-030-11214-1},
  shorttitle	= {Acoustics},
  publisher	= {Springer International Publishing},
  author	= {Pierce, Allan D.},
  urldate	= {2022-12-01},
  date		= {2019},
  langid	= {english},
  doi		= {10.1007/978-3-030-11214-1}
}

@Article{	  poli1998schema,
  title		= {Schema Theory for Genetic Programming with One-Point
		  Crossover and Point Mutation},
  volume	= {6},
  issn		= {1063-6560, 1530-9304},
  url		= {https://direct.mit.edu/evco/article/6/3/231-252/825},
  doi		= {10.1162/evco.1998.6.3.231},
  abstract	= {We review the main results obtained in the theory of
		  schemata in genetic programming ({GP}), emphasizing their
		  strengths and weaknesses. Then we propose a new, simpler
		  definition of the concept of schema for {GP}, which is
		  closer to the original concept of schema in genetic
		  algorithms ({GAs}). Along with a new form of crossover,
		  one-point crossover, and point mutation, this concept of
		  schema has been used to derive an improved schema theorem
		  for {GP} that describes the propagation of schemata from
		  one generation to the next. We discuss this result and show
		  that our schema theorem is the natural counterpart for {GP}
		  of the schema theorem for {GAs}, to which it asymptotically
		  converges.},
  pages		= {231--252},
  number	= {3},
  journaltitle	= {Evolutionary Computation},
  shortjournal	= {Evolutionary Computation},
  author	= {Poli, Riccardo and Langdon, William B.},
  urldate	= {2022-12-10},
  date		= {1998-09},
  langid	= {english}
}

@Book{		  poli2008field,
  title		= {A Field Guide to Genetic Programming},
  abstract	= {Genetic programming ({GP}) is a systematic,
		  domain-independent method for getting computers to solve
		  problems automatically starting from a high-level statement
		  of what needs to be done. Using ideas from natural
		  evolution, {GP} starts from an ooze of random computer
		  programs, and progressively refines them through processes
		  of mutation and sexual recombination, until high-fitness
		  solutions emerge. All this without the user having to know
		  or specify the form or structure of solutions in advance.
		  {GP} has generated a plethora of human-competitive results
		  and applications, including novel scientific discoveries
		  and patentable inventions. This unique overview of this
		  exciting technique is written by three of the most active
		  scientists in {GP}. See www.gp-field-guide.org.uk for more
		  information on the book.},
  publisher	= {Lulu Enterprises, {UK} Ltd},
  author	= {Poli, Riccardo and Langdon, William B. and {McPhee},
		  Nicholas Freitag},
  date		= {2008}
}

@Article{	  raissi2019physics,
  title		= {Physics-informed neural networks: A deep learning
		  framework for solving forward and inverse problems
		  involving nonlinear partial differential equations},
  volume	= {378},
  issn		= {00219991},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0021999118307125},
  doi		= {10.1016/j.jcp.2018.10.045},
  shorttitle	= {Physics-informed neural networks},
  pages		= {686--707},
  journaltitle	= {Journal of Computational Physics},
  shortjournal	= {Journal of Computational Physics},
  author	= {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
  urldate	= {2022-12-10},
  date		= {2019-02},
  langid	= {english}
}

@Article{	  criado2020grammatically,
  title		= {Grammatically uniform population initialization for
		  grammar-guided genetic programming},
  volume	= {24},
  issn		= {1432-7643, 1433-7479},
  url		= {https://link.springer.com/10.1007/s00500-020-05061-w},
  doi		= {10.1007/s00500-020-05061-w},
  pages		= {11265--11282},
  number	= {15},
  journaltitle	= {Soft Computing},
  shortjournal	= {Soft Comput},
  author	= {Ramos Criado, Pablo and Barrios Rolanía, D. and Manrique,
		  Daniel and Serrano, Emilio},
  urldate	= {2022-12-02},
  date		= {2020-08},
  langid	= {english}
}

@InProceedings{	  real2020automl,
  title		= {{AutoML}-Zero: Evolving Machine Learning Algorithms From
		  Scratch},
  volume	= {119},
  url		= {https://proceedings.mlr.press/v119/real20a.html},
  series	= {Proceedings of Machine Learning Research},
  abstract	= {Machine learning research has advanced in multiple
		  aspects, including model structures and learning methods.
		  The effort to automate such research, known as {AutoML},
		  has also made significant progress. However, this progress
		  has largely focused on the architecture of neural networks,
		  where it has relied on sophisticated expert-designed layers
		  as building blocks—or similarly restrictive search
		  spaces. Our goal is to show that {AutoML} can go further:
		  it is possible today to automatically discover complete
		  machine learning algorithms just using basic mathematical
		  operations as building blocks. We demonstrate this by
		  introducing a novel framework that significantly reduces
		  human bias through a generic search space. Despite the
		  vastness of this space, evolutionary search can still
		  discover two-layer neural networks trained by
		  backpropagation. These simple neural networks can then be
		  surpassed by evolving directly on tasks of interest, e.g.
		  {CIFAR}-10 variants, where modern techniques emerge in the
		  top algorithms, such as bilinear interactions, normalized
		  gradients, and weight averaging. Moreover, evolution adapts
		  algorithms to different task types: e.g., dropout-like
		  techniques appear when little data is available. We believe
		  these preliminary successes in discovering machine learning
		  algorithms from scratch indicate a promising new direction
		  for the field.},
  pages		= {8007--8019},
  booktitle	= {Proceedings of the 37th International Conference on
		  Machine Learning},
  publisher	= {{PMLR}},
  author	= {Real, Esteban and Liang, Chen and So, David and Le, Quoc},
  date		= {2020-07-13}
}

@Misc{		  reed2022generalist,
  title		= {A Generalist Agent},
  url		= {http://arxiv.org/abs/2205.06175},
  abstract	= {Inspired by progress in large-scale language modeling, we
		  apply a similar approach towards building a single
		  generalist agent beyond the realm of text outputs. The
		  agent, which we refer to as Gato, works as a multi-modal,
		  multi-task, multi-embodiment generalist policy. The same
		  network with the same weights can play Atari, caption
		  images, chat, stack blocks with a real robot arm and much
		  more, deciding based on its context whether to output text,
		  joint torques, button presses, or other tokens. In this
		  report we describe the model and the data, and document the
		  current capabilities of Gato.},
  number	= {{arXiv}:2205.06175},
  publisher	= {{arXiv}},
  author	= {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and
		  Colmenarejo, Sergio Gomez and Novikov, Alexander and
		  Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and
		  Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom
		  and Bruce, Jake and Razavi, Ali and Edwards, Ashley and
		  Heess, Nicolas and Chen, Yutian and Hadsell, Raia and
		  Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
  urldate	= {2022-12-01},
  date		= {2022-11-11},
  eprinttype	= {arxiv},
  eprint	= {2205.06175 [cs]}
}

@Article{	  ren2021comprehensive,
  title		= {A Comprehensive Survey of Neural Architecture Search:
		  Challenges and Solutions},
  volume	= {54},
  issn		= {0360-0300, 1557-7341},
  url		= {https://dl.acm.org/doi/10.1145/3447582},
  doi		= {10.1145/3447582},
  shorttitle	= {A Comprehensive Survey of Neural Architecture Search},
  abstract	= {Deep learning has made substantial breakthroughs in many
		  fields due to its powerful automatic representation
		  capabilities. It has been proven that neural architecture
		  design is crucial to the feature representation of data and
		  the final performance. However, the design of the neural
		  architecture heavily relies on the researchers’ prior
		  knowledge and experience. And due to the limitations of
		  humans’ inherent knowledge, it is difficult for people to
		  jump out of their original thinking paradigm and design an
		  optimal model. Therefore, an intuitive idea would be to
		  reduce human intervention as much as possible and let the
		  algorithm automatically design the neural architecture.
		  
		  Neural Architecture Search
		  
		  ( {NAS} ) is just such a revolutionary algorithm, and the
		  related research work is complicated and rich. Therefore, a
		  comprehensive and systematic survey on the {NAS} is
		  essential. Previously related surveys have begun to
		  classify existing work mainly based on the key components
		  of {NAS}: search space, search strategy, and evaluation
		  strategy. While this classification method is more
		  intuitive, it is difficult for readers to grasp the
		  challenges and the landmark work involved. Therefore, in
		  this survey, we provide a new perspective: beginning with
		  an overview of the characteristics of the earliest {NAS}
		  algorithms, summarizing the problems in these early {NAS}
		  algorithms, and then providing solutions for subsequent
		  related research work. In addition, we conduct a detailed
		  and comprehensive analysis, comparison, and summary of
		  these works. Finally, we provide some possible future
		  research directions.},
  pages		= {1--34},
  number	= {4},
  journaltitle	= {{ACM} Computing Surveys},
  shortjournal	= {{ACM} Comput. Surv.},
  author	= {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang,
		  Po-yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
  urldate	= {2022-12-01},
  date		= {2022-05-31},
  langid	= {english}
}

@PhDThesis{	  rittich2018extending,
	title		= {Extending and automating Fourier analysis for multigrid
	methods},
	author	= {Rittich, Hannah},
	year		= {2018},
	school	= {Universit{\"a}t Wuppertal, Fakult{\"a}t f{\"u}r Mathematik
	und Naturwissenschaften~…}
}

@Article{	  rodrigo2017validity,
  title		= {On the validity of the local Fourier analysis},
  rights	= {{arXiv}.org perpetual, non-exclusive license},
  url		= {https://arxiv.org/abs/1710.00408},
  doi		= {10.48550/ARXIV.1710.00408},
  abstract	= {Local Fourier analysis ({LFA}) is a useful tool in
		  predicting the convergence factors of geometric multigrid
		  methods ({GMG}). As is well known, on rectangular domains
		  with periodic boundary conditions this analysis gives the
		  exact convergence factors of such methods. In this work,
		  using the Fourier method, we extend these results by
		  proving that such analysis yields the exact convergence
		  factors for a wider class of problems.},
  author	= {Rodrigo, Carmen and Gaspar, Francisco J. and Zikatanov,
		  Ludmil T.},
  urldate	= {2022-12-10},
  date		= {2017},
  note		= {Publisher: {arXiv} Version Number: 2}
}

@collection{ryan2018handbook,
	location = {Cham},
	title = {Handbook of Grammatical Evolution},
	url = {http://link.springer.com/10.1007/978-3-319-78717-6},
	publisher = {Springer International Publishing},
	urldate = {2022-12-02},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-78717-6},
}

@Book{		  saad2003iterative,
  edition	= {Second},
  title		= {Iterative Methods for Sparse Linear Systems},
  url		= {http://epubs.siam.org/doi/book/10.1137/1.9780898718003},
  publisher	= {Society for Industrial and Applied Mathematics},
  author	= {Saad, Yousef},
  urldate	= {2022-12-02},
  date		= {2003-01},
  langid	= {english},
  doi		= {10.1137/1.9780898718003}
}

@InProceedings{	  schmitt2014exaslang,
  location	= {New Orleans, {LA}, {USA}},
  title		= {{ExaSlang}: A Domain-Specific Language for Highly Scalable
		  Multigrid Solvers},
  url		= {http://ieeexplore.ieee.org/document/7101662/},
  doi		= {10.1109/WOLFHPC.2014.11},
  shorttitle	= {{ExaSlang}},
  eventtitle	= {2014 Fourth International Workshop on Domain-Specific
		  Languages and High-Level Frameworks for High Performance
		  Computing ({WOLFHPC})},
  pages		= {42--51},
  booktitle	= {2014 Fourth International Workshop on Domain-Specific
		  Languages and High-Level Frameworks for High Performance
		  Computing},
  publisher	= {{IEEE}},
  author	= {Schmitt, Christian and Kuckuk, Sebastian and Hannig, Frank
		  and Kostler, Harald and Teich, Jurgen},
  urldate	= {2022-12-02},
  date		= {2014-11}
}

@Article{	  schmitt2018automating,
  title		= {Automating the Development of High-Performance Multigrid
		  Solvers},
  volume	= {106},
  issn		= {0018-9219, 1558-2256},
  url		= {https://ieeexplore.ieee.org/document/8436430/},
  doi		= {10.1109/JPROC.2018.2854229},
  pages		= {1969--1984},
  number	= {11},
  journaltitle	= {Proceedings of the {IEEE}},
  shortjournal	= {Proc. {IEEE}},
  author	= {Schmitt, Christian and Kronawitter, Stefan and Hannig,
		  Frank and Teich, Jurgen and Lengauer, Christian},
  urldate	= {2022-12-01},
  date		= {2018-11}
}

@InProceedings{	  schmitt2020constructing,
  location	= {Cancún Mexico},
  title		= {Constructing efficient multigrid solvers with genetic
		  programming},
  url		= {https://dl.acm.org/doi/10.1145/3377930.3389811},
  doi		= {10.1145/3377930.3389811},
  eventtitle	= {{GECCO} '20: Genetic and Evolutionary Computation
		  Conference},
  pages		= {1012--1020},
  booktitle	= {Proceedings of the 2020 Genetic and Evolutionary
		  Computation Conference},
  publisher	= {{ACM}},
  author	= {Schmitt, Jonas and Kuckuk, Sebastian and Köstler,
		  Harald},
  urldate	= {2022-12-01},
  date		= {2020-06-25},
  langid	= {english}
}

@Article{	  schmitt2021evostencils,
  title		= {{EvoStencils}: a grammar-based genetic programming
		  approach for constructing efficient geometric multigrid
		  methods},
  volume	= {22},
  issn		= {1389-2576, 1573-7632},
  url		= {https://link.springer.com/10.1007/s10710-021-09412-w},
  doi		= {10.1007/s10710-021-09412-w},
  shorttitle	= {{EvoStencils}},
  abstract	= {Abstract For many systems of linear equations that arise
		  from the discretization of partial differential equations,
		  the construction of an efficient multigrid solver is
		  challenging. Here we present {EvoStencils}, a novel
		  approach for optimizing geometric multigrid methods with
		  grammar-guided genetic programming, a stochastic program
		  optimization technique inspired by the principle of natural
		  evolution. A multigrid solver is represented as a tree of
		  mathematical expressions that we generate based on a formal
		  grammar. The quality of each solver is evaluated in terms
		  of convergence and compute performance by automatically
		  generating an optimized implementation using code
		  generation that is then executed on the target platform to
		  measure all relevant performance metrics. Based on this, a
		  multi-objective optimization is performed using a
		  non-dominated sorting-based selection. To evaluate a large
		  number of solvers in parallel, they are distributed to
		  multiple compute nodes. We demonstrate the effectiveness of
		  our implementation by constructing geometric multigrid
		  solvers that are able to outperform hand-crafted methods
		  for Poisson’s equation and a linear elastic boundary
		  value problem with up to 16 million unknowns on multi-core
		  processors with Ivy Bridge and Broadwell
		  microarchitecture.},
  pages		= {511--537},
  number	= {4},
  journaltitle	= {Genetic Programming and Evolvable Machines},
  shortjournal	= {Genet Program Evolvable Mach},
  author	= {Schmitt, Jonas and Kuckuk, Sebastian and Köstler,
		  Harald},
  urldate	= {2022-12-02},
  date		= {2021-12},
  langid	= {english}
}

@InProceedings{	  schmitt2022evolving,
  location	= {Boston Massachusetts},
  title		= {Evolving generalizable multigrid-based helmholtz
		  preconditioners with grammar-guided genetic programming},
  url		= {https://dl.acm.org/doi/10.1145/3512290.3528688},
  doi		= {10.1145/3512290.3528688},
  eventtitle	= {{GECCO} '22: Genetic and Evolutionary Computation
		  Conference},
  pages		= {1009--1018},
  booktitle	= {Proceedings of the Genetic and Evolutionary Computation
		  Conference},
  publisher	= {{ACM}},
  author	= {Schmitt, Jonas and Köstler, Harald},
  urldate	= {2022-12-02},
  date		= {2022-07-08},
  langid	= {english}
}

@Article{	  schrittwieser2020mastering,
  title		= {Mastering Atari, Go, chess and shogi by planning with a
		  learned model},
  volume	= {588},
  issn		= {0028-0836, 1476-4687},
  url		= {http://www.nature.com/articles/s41586-020-03051-4},
  doi		= {10.1038/s41586-020-03051-4},
  pages		= {604--609},
  number	= {7839},
  journaltitle	= {Nature},
  shortjournal	= {Nature},
  author	= {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert,
		  Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt,
		  Simon and Guez, Arthur and Lockhart, Edward and Hassabis,
		  Demis and Graepel, Thore and Lillicrap, Timothy and Silver,
		  David},
  urldate	= {2022-12-10},
  date		= {2020-12-24},
  langid	= {english}
}

@Misc{		  schrodi2022towards,
  title		= {Towards Discovering Neural Architectures from Scratch},
  url		= {http://arxiv.org/abs/2211.01842},
  abstract	= {The discovery of neural architectures from scratch is the
		  long-standing goal of Neural Architecture Search ({NAS}).
		  Searching over a wide spectrum of neural architectures can
		  facilitate the discovery of previously unconsidered but
		  well-performing architectures. In this work, we take a
		  large step towards discovering neural architectures from
		  scratch by expressing architectures algebraically. This
		  algebraic view leads to a more general method for designing
		  search spaces, which allows us to compactly represent
		  search spaces that are 100s of orders of magnitude larger
		  than common spaces from the literature. Further, we propose
		  a Bayesian Optimization strategy to efficiently search over
		  such huge spaces, and demonstrate empirically that both our
		  search space design and our search strategy can be superior
		  to existing baselines. We open source our algebraic {NAS}
		  approach and provide {APIs} for {PyTorch} and
		  {TensorFlow}.},
  number	= {{arXiv}:2211.01842},
  publisher	= {{arXiv}},
  author	= {Schrodi, Simon and Stoll, Danny and Ru, Binxin and
		  Sukthanker, Rhea and Brox, Thomas and Hutter, Frank},
  urldate	= {2022-12-10},
  date		= {2022-11-03},
  eprinttype	= {arxiv},
  eprint	= {2211.01842 [cs, stat]}
}

@Article{	  sheikh2013convergence,
  title		= {On the convergence of shifted Laplace preconditioner
		  combined with multilevel deflation},
  volume	= {20},
  issn		= {10705325},
  url		= {https://onlinelibrary.wiley.com/doi/10.1002/nla.1882},
  doi		= {10.1002/nla.1882},
  shorttitle	= {On the convergence of shifted Laplace preconditioner
		  combined with multilevel deflation},
  pages		= {645--662},
  number	= {4},
  journaltitle	= {Numerical Linear Algebra with Applications},
  shortjournal	= {Numer. Linear Algebra Appl.},
  author	= {Sheikh, A. H. and Lahaye, D. and Vuik, C.},
  urldate	= {2022-12-10},
  date		= {2013-08},
  langid	= {english}
}

@Article{	  sirignano2018dgm,
  title		= {{DGM}: A deep learning algorithm for solving partial
		  differential equations},
  volume	= {375},
  issn		= {00219991},
  url		= {https://linkinghub.elsevier.com/retrieve/pii/S0021999118305527},
  doi		= {10.1016/j.jcp.2018.08.029},
  shorttitle	= {{DGM}},
  pages		= {1339--1364},
  journaltitle	= {Journal of Computational Physics},
  shortjournal	= {Journal of Computational Physics},
  author	= {Sirignano, Justin and Spiliopoulos, Konstantinos},
  urldate	= {2022-12-02},
  date		= {2018-12},
  langid	= {english}
}

@InProceedings{	  snoek2012practical,
  title		= {Practical Bayesian Optimization of Machine Learning
		  Algorithms},
  volume	= {25},
  url		= {https://proceedings.neurips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf},
  booktitle	= {Advances in Neural Information Processing Systems},
  publisher	= {Curran Associates, Inc.},
  author	= {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
		  Weinberger, K. Q.},
  date		= {2012}
}

@Book{		  sterling2017high,
  location	= {Cambridge, {MA}},
  title		= {High performance computing: modern systems and practices},
  shorttitle	= {High performance computing},
  abstract	= {High Performance Computing: Modern Systems and Practices
		  is a fully comprehensive and easily accessible treatment of
		  high performance computing, covering fundamental concepts
		  and essential knowledge while also providing key skills
		  training. With this book, domain scientists will learn how
		  to use supercomputers as a key tool in their quest for new
		  knowledge. In addition, practicing engineers will discover
		  how supercomputers can employ {HPC} systems and methods to
		  the design and simulation of innovative products, and
		  students will begin their careers with an understanding of
		  possible directions for future research and development in
		  {HPC}. Those who maintain and administer commodity clusters
		  will find this textbook provides essential coverage of not
		  only what {HPC} systems do, but how they are used. --
		  Provided by publisher},
  pagetotal	= {689},
  publisher	= {Morgan Kaufmann, an imprint of Elsevier},
  author	= {Sterling, Thomas and Anderson, Matthew and Brodowicz,
		  Maciej},
  date		= {2018},
  note		= {{OCLC}: on1023863095}
}

@Book{		  strikwerda2004finite,
  title		= {Finite Difference Schemes and Partial Differential
		  Equations, Second Edition},
  url		= {http://epubs.siam.org/doi/book/10.1137/1.9780898717938},
  publisher	= {Society for Industrial and Applied Mathematics},
  author	= {Strikwerda, John C.},
  urldate	= {2022-12-02},
  date		= {2004-01},
  langid	= {english},
  doi		= {10.1137/1.9780898717938}
}

@InCollection{	  stuben2001introduction,
  title		= {An Introduction to Algebraic Multigrid},
  pages		= {413--532},
  booktitle	= {Multigrid},
  author	= {Stüben, Klaus},
  date		= {2001}
}

@InProceedings{	  taghibakhshi2021optimization,
  title		= {Optimization-Based Algebraic Multigrid Coarsening Using
		  Reinforcement Learning},
  volume	= {34},
  url		= {https://proceedings.neurips.cc/paper/2021/file/6531b32f8d02fece98ff36a64a7c8260-Paper.pdf},
  pages		= {12129--12140},
  booktitle	= {Advances in Neural Information Processing Systems},
  author	= {Taghibakhshi, Ali and {MacLachlan}, Scott and Olson, Luke
		  and West, Matthew},
		  P. S. and Vaughan, J. Wortman},
  date		= {2021},
  note		= {Publisher: Curran Associates, Inc.}
}

@Article{	  thekale2010optimizing,
  title		= {Optimizing the number of multigrid cycles in the full
		  multigrid algorithm},
  volume	= {17},
  issn		= {10705325},
  url		= {https://onlinelibrary.wiley.com/doi/10.1002/nla.697},
  doi		= {10.1002/nla.697},
  shorttitle	= {Optimizing the number of multigrid cycles in the full
		  multigrid algorithm},
  pages		= {199--210},
  number	= {2},
  journaltitle	= {Numerical Linear Algebra with Applications},
  shortjournal	= {Numer. Linear Algebra Appl.},
  author	= {Thekale, A. and Gradl, T. and Klamroth, K. and Rüde, U.},
  urldate	= {2022-12-10},
  date		= {2010-04},
  langid	= {english}
}

@InProceedings{	  kotthoff2019auto,
  location	= {Chicago Illinois {USA}},
  title		= {Auto-{WEKA}: combined selection and hyperparameter
		  optimization of classification algorithms},
  url		= {https://dl.acm.org/doi/10.1145/2487575.2487629},
  doi		= {10.1145/2487575.2487629},
  shorttitle	= {Auto-{WEKA}},
  eventtitle	= {{KDD}' 13: The 19th {ACM} {SIGKDD} International
		  Conference on Knowledge Discovery and Data Mining},
  pages		= {847--855},
  booktitle	= {Proceedings of the 19th {ACM} {SIGKDD} international
		  conference on Knowledge discovery and data mining},
  publisher	= {{ACM}},
  author	= {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and
		  Leyton-Brown, Kevin},
  urldate	= {2022-12-01},
  date		= {2013-08-11},
  langid	= {english}
}

@Article{	  thuerey2020deep,
  title		= {Deep Learning Methods for Reynolds-Averaged
		  Navier–Stokes Simulations of Airfoil Flows},
  volume	= {58},
  issn		= {0001-1452, 1533-385X},
  url		= {https://arc.aiaa.org/doi/10.2514/1.J058291},
  doi		= {10.2514/1.J058291},
  pages		= {25--36},
  number	= {1},
  journaltitle	= {{AIAA} Journal},
  shortjournal	= {{AIAA} Journal},
  author	= {Thuerey, Nils and Weißenow, Konstantin and Prantl, Lukas
		  and Hu, Xiangyu},
  urldate	= {2022-12-01},
  date		= {2020-01},
  langid	= {english}
}

@Book{		  trottenberg2000multigrid,
  location	= {San Diego},
  title		= {Multigrid},
  pagetotal	= {631},
  publisher	= {Academic Press},
  author	= {Trottenberg, U. and Oosterlee, C. W. and Schüller,
		  Anton},
  date		= {2001}
}

@Article{	  umetami2009multigrid,
  title		= {A multigrid-based shifted Laplacian preconditioner for a fourth-order Helmholtz discretization},
  volume	= {16},
  issn		= {10705325, 10991506},
  url		= {https://onlinelibrary.wiley.com/doi/10.1002/nla.634},
  doi		= {10.1002/nla.634},
  pages		= {603--626},
  number	= {8},
  journaltitle	= {Numerical Linear Algebra with Applications},
  shortjournal	= {Numer. Linear Algebra Appl.},
  author	= {Umetani, N. and {MacLachlan}, S. P. and Oosterlee, C. W.},
  urldate	= {2022-12-01},
  date		= {2009-08},
  langid	= {english}
}

@Article{	  vanspectralgijzen,
  title		= {Spectral Analysis of the Discrete Helmholtz Operator
		  Preconditioned with a Shifted Laplacian},
  volume	= {29},
  issn		= {1064-8275, 1095-7197},
  url		= {http://epubs.siam.org/doi/10.1137/060661491},
  doi		= {10.1137/060661491},
  pages		= {1942--1958},
  number	= {5},
  journaltitle	= {{SIAM} Journal on Scientific Computing},
  shortjournal	= {{SIAM} J. Sci. Comput.},
  author	= {van Gijzen, M. B. and Erlangga, Y. A. and Vuik, C.},
  urldate	= {2022-12-10},
  date		= {2007-01},
  langid	= {english}
}

@Book{		  varga1962iterative,
  location	= {Berlin, Heidelberg},
  title		= {Matrix Iterative Analysis},
  volume	= {27},
  url		= {http://link.springer.com/10.1007/978-3-642-05156-2},
  series	= {Springer Series in Computational Mathematics},
  publisher	= {Springer Berlin Heidelberg},
  author	= {Varga, Richard S.},
  urldate	= {2022-12-02},
  date		= {2000},
  doi		= {10.1007/978-3-642-05156-2}
}

@Article{	  versteeg1994marmousi,
  title		= {The Marmousi experience: Velocity model determination on a
		  synthetic complex data set},
  volume	= {13},
  issn		= {1070-485X, 1938-3789},
  url		= {http://library.seg.org/doi/10.1190/1.1437051},
  doi		= {10.1190/1.1437051},
  shorttitle	= {The Marmousi experience},
  pages		= {927--936},
  number	= {9},
  journaltitle	= {The Leading Edge},
  shortjournal	= {The Leading Edge},
  author	= {Versteeg, Roelof},
  urldate	= {2022-12-10},
  date		= {1994-09},
  langid	= {english}
}

@Book{		  versteeg2007introduction,
  location	= {Harlow, England ; New York},
  edition	= {2nd ed},
  title		= {An introduction to computational fluid dynamics: the
		  finite volume method},
  shorttitle	= {An introduction to computational fluid dynamics},
  pagetotal	= {503},
  publisher	= {Pearson Education Ltd},
  author	= {Versteeg, H. K. and Malalasekera, W.},
  date		= {2007},
  note		= {{OCLC}: ocm76821177}
}

@InCollection{	  wah2008finite,
  location	= {Hoboken, {NJ}, {USA}},
  title		= {Finite Element Method},
  url		= {https://onlinelibrary.wiley.com/doi/10.1002/9780470050118.ecse159},
  pages		= {ecse159},
  booktitle	= {Wiley Encyclopedia of Computer Science and Engineering},
  publisher	= {John Wiley \& Sons, Inc.},
  author	= {Bathe, Klaus-Jürgen},
  urldate	= {2022-12-02},
  date		= {2008-06-13},
  langid	= {english},
  doi		= {10.1002/9780470050118.ecse159}
}

@Article{	  walker1996mpi,
  title		= {{MPI}: a standard message passing interface},
  volume	= {12},
  pages		= {56--68},
  journaltitle	= {Supercomputer},
  author	= {Walker, David W and Dongarra, Jack J},
  date		= {1996},
  note		= {Publisher: {ASFRA} {BV}}
}

@InCollection{	  deb2011multi,
  location	= {London},
  title		= {Multi-objective Optimisation Using Evolutionary
		  Algorithms: An Introduction},
  url		= {http://link.springer.com/10.1007/978-0-85729-652-8_1},
  shorttitle	= {Multi-objective Optimisation Using Evolutionary
		  Algorithms},
  pages		= {3--34},
  booktitle	= {Multi-objective Evolutionary Optimisation for Product
		  Design and Manufacturing},
  publisher	= {Springer London},
  author	= {Deb, Kalyanmoy},
  urldate	= {2022-12-10},
  date		= {2011},
  langid	= {english},
  doi		= {10.1007/978-0-85729-652-8_1}
}

@InProceedings{	  whigham1995grammatically,
  title		= {Grammatically-based genetic programming},
  volume	= {16},
  pages		= {33--41},
  booktitle	= {Proceedings of the workshop on genetic programming: from
		  theory to real-world applications},
  author	= {Whigham, Peter A. and {others}},
  date		= {1995},
  note		= {Issue: 3}
}

@Book{		  wienands2004practical,
  location	= {Boca Raton, {FL}},
  title		= {Practical Fourier analysis for multigrid methods},
  series	= {Numerical insights},
  pagetotal	= {217},
  number	= {v. 4},
  publisher	= {Chapman \& Hall/{CRC}},
  author	= {Wienands, R. and Joppich, W.},
  date		= {2005}
}

@Article{	  williams2009roofline,
  title		= {Roofline: an insightful visual performance model for
		  multicore architectures},
  volume	= {52},
  issn		= {0001-0782, 1557-7317},
  url		= {https://dl.acm.org/doi/10.1145/1498765.1498785},
  doi		= {10.1145/1498765.1498785},
  shorttitle	= {Roofline},
  abstract	= {The Roofline model offers insight on how to improve the
		  performance of software and hardware.},
  pages		= {65--76},
  number	= {4},
  journaltitle	= {Communications of the {ACM}},
  shortjournal	= {Commun. {ACM}},
  author	= {Williams, Samuel and Waterman, Andrew and Patterson,
		  David},
  urldate	= {2022-12-10},
  date		= {2009-04},
  langid	= {english}
}

@Article{	  xu2017algebraic,
  title		= {Algebraic multigrid methods},
  volume	= {26},
  issn		= {0962-4929, 1474-0508},
  url		= {https://www.cambridge.org/core/product/identifier/S0962492917000083/type/journal_article},
  doi		= {10.1017/S0962492917000083},
  abstract	= {This paper provides an overview of {AMG} methods for
		  solving large-scale systems of equations, such as those
		  from discretizations of partial differential equations.
		  {AMG} is often understood as the acronym of ‘algebraic
		  multigrid’, but it can also be understood as ‘abstract
		  multigrid’. Indeed, we demonstrate in this paper how and
		  why an algebraic multigrid method can be better understood
		  at a more abstract level. In the literature, there are many
		  different algebraic multigrid methods that have been
		  developed from different perspectives. In this paper we try
		  to develop a unified framework and theory that can be used
		  to derive and analyse different algebraic multigrid methods
		  in a coherent manner. Given a smoother
		  
		  \$R\$
		  
		  for a matrix
		  
		  \$A\$
		  
		  , such as Gauss–Seidel or Jacobi, we prove that the
		  optimal coarse space of dimension
		  
		  \$n\_\{c\}\$
		  
		  is the span of the eigenvectors corresponding to the first
		  
		  \$n\_\{c\}\$
		  
		  eigenvectors
		  
		  \${\textbackslash}bar\{R\}A\$
		  
		  (with
		  
		  \${\textbackslash}bar\{R\}=R+R{\textasciicircum}\{T\}-R{\textasciicircum}\{T\}{AR}\$
		  
		  ).
		  We also prove that this optimal coarse space can be
		  obtained via a constrained trace-minimization problem for a
		  matrix associated with
		  
		  \${\textbackslash}bar\{R\}A\$
		  
		  , and demonstrate that coarse spaces of most existing {AMG}
		  methods can be viewed as approximate solutions of this
		  trace-minimization problem. Furthermore, we provide a
		  general approach to the construction of quasi-optimal
		  coarse spaces, and we prove that under appropriate
		  assumptions the resulting two-level {AMG} method for the
		  underlying linear system converges uniformly with respect
		  to the size of the problem, the coefficient variation and
		  the anisotropy. Our theory applies to most existing
		  multigrid methods, including the standard geometric
		  multigrid method, classical {AMG}, energy-minimization
		  {AMG}, unsmoothed and smoothed aggregation {AMG} and spectral {AMGe}.},
  pages		= {591--721},
  journaltitle	= {Acta Numerica},
  shortjournal	= {Acta Numerica},
  author	= {Xu, Jinchao and Zikatanov, Ludmil},
  urldate	= {2022-12-01},
  date		= {2017-05-01},
  langid	= {english}
}

@Book{		  zienkiewicz2005finite,
  location	= {Amsterdam},
  edition	= {Seventh edition},
  title		= {The finite element method: its basis and fundamentals},
  shorttitle	= {The finite element method},
  pagetotal	= {714},
  publisher	= {Elsevier, Butterworth-Heinemann},
  author	= {Zienkiewicz, O. C. and Taylor, Robert L. and Zhu, J. Z.},
  date		= {2013},
  note		= {{OCLC}: ocn852808496}
}
